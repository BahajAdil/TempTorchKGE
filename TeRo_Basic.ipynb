{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d2a7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "# encoding: utf-8\n",
    "import os\n",
    "from collections import defaultdict as ddict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from numpy.random import RandomState\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617d143",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc42133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraph:\n",
    "    def __init__(self, data_dir, gran=1,rev_set=0):\n",
    "        self.data_dir = data_dir\n",
    "        self.entity_dict = {}\n",
    "        self.gran = gran\n",
    "        self.entities = []\n",
    "        self.relation_dict = {}\n",
    "        self.n_entity = 0\n",
    "        self.n_relation = 0\n",
    "        self.training_triples = []  # list of triples in the form of (h, t, r)\n",
    "        self.validation_triples = []\n",
    "        self.test_triples = []\n",
    "        self.training_facts = []\n",
    "        self.validation_facts = []\n",
    "        self.test_facts = []\n",
    "        self.n_training_triple = 0\n",
    "        self.n_validation_triple = 0\n",
    "        self.n_test_triple = 0\n",
    "        self.rev_set = rev_set\n",
    "        self.start_date = '2014-01-01' if self.data_dir == 'icews14' else '2005-01-01'\n",
    "        self.start_sec = time.mktime(time.strptime(self.start_date,'%Y-%m-%d'))\n",
    "        self.n_time=365 if self.data_dir == 'icews14' else 4017\n",
    "        self.to_skip_final = {'lhs': {}, 'rhs': {}}\n",
    "        '''load dicts and triples'''\n",
    "        self.load_dicts()\n",
    "        self.load_triples()\n",
    "        self.load_filters()\n",
    "        '''construct pools after loading'''\n",
    "        # self.training_triple_pool = set(self.training_triples)\n",
    "        # self.golden_triple_pool = set(self.training_triples) | set(self.validation_triples) | set(self.test_triples)\n",
    "\n",
    "    def load_dicts(self):\n",
    "        entity_dict_file = 'entity2id.txt'\n",
    "        relation_dict_file = 'relation2id.txt'\n",
    "        print('-----Loading entity dict-----')\n",
    "        entity_df = pd.read_table(os.path.join(self.data_dir, entity_dict_file), header=None)\n",
    "        self.entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
    "        self.n_entity = len(self.entity_dict)\n",
    "        self.entities = list(self.entity_dict.values())\n",
    "        print('#entity: {}'.format(self.n_entity))\n",
    "        print('-----Loading relation dict-----')\n",
    "        relation_df = pd.read_table(os.path.join(self.data_dir, relation_dict_file), header=None)\n",
    "        self.relation_dict = dict(zip(relation_df[0], relation_df[1]))\n",
    "        self.n_relation = len(self.relation_dict)\n",
    "        if self.rev_set>0: self.n_relation *= 2\n",
    "        print('#relation: {}'.format(self.n_relation))\n",
    "\n",
    "    def load_triples(self):\n",
    "        training_file = 'train.txt'\n",
    "        validation_file = 'valid.txt'\n",
    "        test_file = 'test.txt'\n",
    "        print('-----Loading training triples-----')\n",
    "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None)\n",
    "        training_df = np.array(training_df).tolist()\n",
    "        for triple in training_df:\n",
    "            end_sec = time.mktime(time.strptime(triple[3], '%Y-%m-%d'))\n",
    "            day = int((end_sec - self.start_sec) / (self.gran*24 * 60 * 60))\n",
    "            self.training_triples.append([self.entity_dict[triple[0]],self.entity_dict[triple[2]],self.relation_dict[triple[1]],day])\n",
    "            self.training_facts.append([self.entity_dict[triple[0]],self.entity_dict[triple[2]],self.relation_dict[triple[1]],triple[3],0])\n",
    "            if self.rev_set>0: self.training_triples.append([self.entity_dict[triple[2]],self.entity_dict[triple[0]],self.relation_dict[triple[1]]+self.n_relation//2,day])\n",
    "\n",
    "        self.n_training_triple = len(self.training_triples)\n",
    "        print('#training triple: {}'.format(self.n_training_triple))\n",
    "        print('-----Loading validation triples-----')\n",
    "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None)\n",
    "        validation_df = np.array(validation_df).tolist()\n",
    "        for triple in validation_df:\n",
    "            end_sec = time.mktime(time.strptime(triple[3], '%Y-%m-%d'))\n",
    "            day = int((end_sec - self.start_sec) / (self.gran*24 * 60 * 60))\n",
    "            self.validation_triples.append([self.entity_dict[triple[0]],self.entity_dict[triple[2]],self.relation_dict[triple[1]],day])\n",
    "            self.validation_facts.append([self.entity_dict[triple[0]],self.entity_dict[triple[2]],self.relation_dict[triple[1]],triple[3],0])\n",
    "\n",
    "        self.n_validation_triple = len(self.validation_triples)\n",
    "        print('#validation triple: {}'.format(self.n_validation_triple))\n",
    "        print('-----Loading test triples------')\n",
    "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None)\n",
    "        test_df = np.array(test_df).tolist()\n",
    "        for triple in test_df:\n",
    "            end_sec = time.mktime(time.strptime(triple[3], '%Y-%m-%d'))\n",
    "            day = int((end_sec - self.start_sec) / (self.gran*24 * 60 * 60))\n",
    "            self.test_triples.append(\n",
    "                    [self.entity_dict[triple[0]], self.entity_dict[triple[2]], self.relation_dict[triple[1]], day])\n",
    "            self.test_facts.append([self.entity_dict[triple[0]],self.entity_dict[triple[2]],self.relation_dict[triple[1]],triple[3],0])\n",
    "\n",
    "        self.n_test_triple = len(self.test_triples)\n",
    "        print('#test triple: {}'.format(self.n_test_triple))\n",
    "\n",
    "\n",
    "    def load_filters(self):\n",
    "        print(\"creating filtering lists\")\n",
    "        to_skip = {'lhs': defaultdict(set), 'rhs': defaultdict(set)}\n",
    "        facts_pool = [self.training_facts,self.validation_facts,self.test_facts]\n",
    "        for facts in facts_pool:\n",
    "            for fact in facts:\n",
    "                to_skip['lhs'][(fact[1], fact[2],fact[3], fact[4])].add(fact[0])  # left prediction\n",
    "                to_skip['rhs'][(fact[0], fact[2],fact[3], fact[4])].add(fact[1])  # right prediction\n",
    "                \n",
    "        for kk, skip in to_skip.items():\n",
    "            for k, v in skip.items():\n",
    "                self.to_skip_final[kk][k] = sorted(list(v))\n",
    "        print(\"data preprocess completed\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a18ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphYG:\n",
    "    def __init__(self, data_dir, count=300, rev_set=0):\n",
    "        self.data_dir = data_dir\n",
    "        self.entity_dict = {}\n",
    "        self.entities = []\n",
    "        self.relation_dict = {}\n",
    "        self.n_entity = 0\n",
    "        self.n_relation = 0\n",
    "        self.training_triples = []  # list of triples in the form of (h, t, r)\n",
    "        self.validation_triples = []\n",
    "        self.test_triples = []\n",
    "        self.training_facts = []\n",
    "        self.validation_facts = []\n",
    "        self.test_facts = []\n",
    "        self.n_training_triple = 0\n",
    "        self.n_validation_triple = 0\n",
    "        self.n_test_triple = 0\n",
    "        self.n_time = 0\n",
    "        self.start_year= -500\n",
    "        self.end_year = 3000\n",
    "        self.year_class=[]\n",
    "        self.year2id = dict()\n",
    "        self.rev_set = rev_set\n",
    "        self.fact_count = count\n",
    "        self.to_skip_final = {'lhs': {}, 'rhs': {}}\n",
    "        '''load dicts and triples'''\n",
    "        self.time_list()\n",
    "        self.load_dicts()\n",
    "        self.load_triples()\n",
    "        self.load_filters()\n",
    "        '''construct pools after loading'''\n",
    "        # self.training_triple_pool = set(self.training_triples)\n",
    "        # self.golden_triple_pool = set(self.training_triples) | set(self.validation_triples) | set(self.test_triples)\n",
    "\n",
    "    def load_dicts(self):\n",
    "        entity_dict_file = 'entity2id.txt'\n",
    "        relation_dict_file = 'relation2id.txt'\n",
    "        print('-----Loading entity dict-----')\n",
    "        entity_df = pd.read_table(os.path.join(self.data_dir, entity_dict_file), header=None)\n",
    "        self.entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
    "        self.n_entity = len(self.entity_dict)\n",
    "        self.entities = list(self.entity_dict.values())\n",
    "        print('#entity: {}'.format(self.n_entity))\n",
    "        print('-----Loading relation dict-----')\n",
    "        relation_df = pd.read_table(os.path.join(self.data_dir, relation_dict_file), header=None)\n",
    "        self.relation_dict = dict(zip(relation_df[0], relation_df[1]))\n",
    "        self.n_relation = len(self.relation_dict)\n",
    "        if self.rev_set>0: self.n_relation *= 2\n",
    "        print('#relation: {}'.format(self.n_relation))\n",
    "\n",
    "    def time_list(self):\n",
    "        training_file = 'train.txt'\n",
    "        validation_file = 'valid.txt'\n",
    "        test_file = 'test.txt'\n",
    "        triple_file = 'triple2id.txt'\n",
    "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None)\n",
    "        training_df = np.array(training_df).tolist()\n",
    "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None)\n",
    "        validation_df = np.array(validation_df).tolist()\n",
    "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None)\n",
    "        test_df = np.array(test_df).tolist()\n",
    " #       triple_df = pd.read_table(os.path.join(self.data_dir, triple_file), header=None)\n",
    " #       triple_df = np.array(triple_df).tolist()\n",
    "        triple_df = np.concatenate([training_df,validation_df,test_df],axis=0)\n",
    "        n=0\n",
    "        \n",
    "        year_list=[]\n",
    "        for triple in triple_df:\n",
    "            n+=1\n",
    "            if triple[3][0]=='-':\n",
    "                start = -int(triple[3].split('-')[1])\n",
    "                year_list.append(start)\n",
    "            else:\n",
    "                start = triple[3].split('-')[0]\n",
    "                if start =='####':\n",
    "                    start = self.start_year\n",
    "                else:\n",
    "                    start = start.replace('#', '0')\n",
    "                    start = int(start)\n",
    "                    year_list.append(start)\n",
    "\n",
    "\n",
    "            if triple[4][0]=='-':\n",
    "                end = -int(triple[4].split('-')[1])\n",
    "                year_list.append(end)\n",
    "            else:\n",
    "                end = triple[4].split('-')[0]\n",
    "                if end =='####':\n",
    "                    end = self.end_year\n",
    "                else:\n",
    "                    end = end.replace('#', '0')\n",
    "                    end = int(end)\n",
    "                    year_list.append(end)\n",
    "\n",
    "#            for i in range(start,end):\n",
    "#                 year_list.append(i)\n",
    "            \n",
    "\n",
    "\n",
    "        year_list.sort()\n",
    "\n",
    "        freq=ddict(int)\n",
    "        for year in year_list:\n",
    "            freq[year]=freq[year]+1\n",
    "\n",
    "        year_class=[]\n",
    "        count=0\n",
    "        for key in sorted(freq.keys()):\n",
    "            count += freq[key]\n",
    "            if count>=self.fact_count:\n",
    "                year_class.append(key)\n",
    "                count=0\n",
    "        year_class[-1]=year_list[-1]\n",
    "\n",
    "        year2id = dict()\n",
    "        prev_year = year_list[0]\n",
    "        i = 0\n",
    "        for i, yr in enumerate(year_class): \n",
    "            year2id[(prev_year, yr)] = i\n",
    " #           if i>2: \n",
    "            prev_year = yr + 1\n",
    "\n",
    "        self.year2id=year2id\n",
    "        self.year_class = year_class\n",
    "        self.n_time = len(self.year2id.keys())\n",
    "\n",
    "\n",
    "    def load_triples(self):\n",
    "        training_file = 'train.txt'\n",
    "        validation_file = 'valid.txt'\n",
    "        test_file = 'test.txt'\n",
    "        print('-----Loading training triples-----')\n",
    "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None)\n",
    "        training_df = np.array(training_df).tolist()\n",
    "        for triple in training_df:\n",
    "            if triple[3].split('-')[0] == '####':\n",
    "                start=self.start_year\n",
    "                start_idx = 0\n",
    "            elif triple[3][0] == '-':\n",
    "                start=-int(triple[3].split('-')[1].replace('#', '0'))\n",
    "            elif triple[3][0] != '-':\n",
    "                start = int(triple[3].split('-')[0].replace('#','0'))\n",
    "            \n",
    "            if triple[4].split('-')[0] == '####':\n",
    "                end = self.end_year\n",
    "                end_idx = self.n_time-1\n",
    "            elif triple[4][0] == '-':\n",
    "                end =-int(triple[4].split('-')[1].replace('#', '0'))\n",
    "            elif triple[4][0] != '-':\n",
    "                end = int(triple[4].split('-')[0].replace('#','0'))\n",
    "        \n",
    "            for key, time_idx in sorted(self.year2id.items(), key=lambda x:x[1]):\n",
    "                if start>=key[0] and start<=key[1]:\n",
    "                    start_idx = time_idx\n",
    "                if end>=key[0] and end<=key[1]:\n",
    "                    end_idx = time_idx\n",
    "\n",
    "\n",
    "            self.training_triples.append([triple[0],triple[2],triple[1],start_idx,end_idx])\n",
    "            self.training_facts.append([triple[0],triple[2],triple[1],triple[3],triple[4]])\n",
    "            if self.rev_set>0: self.training_triples.append([triple[2],triple[0],triple[1]+self.n_relation//2,start_idx,end_idx])\n",
    "            # for day_idx in range(start_idx,end_idx+1):\n",
    "            #     try:\n",
    "            #         self.training_triples.append([triple[0],triple[2],triple[1],day_idx])\n",
    "            #     except KeyError:\n",
    "            #         continue\n",
    "        self.n_training_triple = len(self.training_triples)\n",
    "        print('#training triple: {}'.format(self.n_training_triple))\n",
    "        print('-----Loading validation triples-----')\n",
    "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None)\n",
    "        validation_df = np.array(validation_df).tolist()\n",
    "        for triple in validation_df:\n",
    "            if triple[3].split('-')[0] == '####':\n",
    "                start=self.start_year\n",
    "                start_idx = 0\n",
    "            elif triple[3][0] == '-':\n",
    "                start=-int(triple[3].split('-')[1].replace('#', '0'))\n",
    "            elif triple[3][0] != '-':\n",
    "                start = int(triple[3].split('-')[0].replace('#','0'))\n",
    "            \n",
    "            if triple[4].split('-')[0] == '####':\n",
    "                end = self.end_year\n",
    "                end_idx = self.n_time-1\n",
    "            elif triple[4][0] == '-':\n",
    "                end =-int(triple[4].split('-')[1].replace('#', '0'))\n",
    "            elif triple[4][0] != '-':\n",
    "                end = int(triple[4].split('-')[0].replace('#','0'))\n",
    "        \n",
    "            for key, time_idx in sorted(self.year2id.items(), key=lambda x:x[1]):\n",
    "                if start>=key[0] and start<=key[1]:\n",
    "                    start_idx = time_idx\n",
    "                if end>=key[0] and end<=key[1]:\n",
    "                    end_idx = time_idx\n",
    "            \n",
    "                    \n",
    "            self.validation_triples.append([triple[0],triple[2],triple[1],start_idx,end_idx])\n",
    "            self.validation_facts.append([triple[0],triple[2],triple[1],triple[3],triple[4]])\n",
    "            # for day_idx in range(start_idx,end_idx+1):\n",
    "            #     try:\n",
    "            #         self.validation_triples.append([triple[0],triple[2],triple[1],day_idx])\n",
    "            #     except KeyError:\n",
    "            #         continue\n",
    "        self.n_validation_triple = len(self.validation_triples)\n",
    "        print('#validation triple: {}'.format(self.n_validation_triple))\n",
    "        print('-----Loading test triples------')\n",
    "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None)\n",
    "        test_df = np.array(test_df).tolist()\n",
    "        for triple in test_df:\n",
    "            if triple[3].split('-')[0] == '####':\n",
    "                start=self.start_year\n",
    "                start_idx = 0\n",
    "            elif triple[3][0] == '-':\n",
    "                start=-int(triple[3].split('-')[1].replace('#', '0'))\n",
    "            elif triple[3][0] != '-':\n",
    "                start = int(triple[3].split('-')[0].replace('#','0'))\n",
    "            \n",
    "            if triple[4].split('-')[0] == '####':\n",
    "                end = self.end_year\n",
    "                end_idx = self.n_time-1\n",
    "            elif triple[4][0] == '-':\n",
    "                end =-int(triple[4].split('-')[1].replace('#', '0'))\n",
    "            elif triple[4][0] != '-':\n",
    "                end = int(triple[4].split('-')[0].replace('#','0'))\n",
    "        \n",
    "            for key, time_idx in sorted(self.year2id.items(), key=lambda x:x[1]):\n",
    "                if start>=key[0] and start<=key[1]:\n",
    "                    start_idx = time_idx\n",
    "                if end>=key[0] and end<=key[1]:\n",
    "                    end_idx = time_idx\n",
    "                    \n",
    "\n",
    "            self.test_triples.append([triple[0],triple[2],triple[1],start_idx,end_idx])\n",
    "            self.test_facts.append([triple[0],triple[2],triple[1],triple[3],triple[4]])\n",
    "            # for day_idx in range(start_idx,end_idx+1):\n",
    "            #     try:\n",
    "            #         self.test_triples.append([triple[0],triple[2],triple[1],day_idx])\n",
    "            #     except KeyError:\n",
    "            #         continue\n",
    "        self.n_test_triple = len(self.test_triples)\n",
    "        print('#test triple: {}'.format(self.n_test_triple))\n",
    "\n",
    "    def load_filters(self):\n",
    "        print(\"creating filtering lists\")\n",
    "        to_skip = {'lhs': defaultdict(set), 'rhs': defaultdict(set)}\n",
    "        facts_pool = [self.training_facts,self.validation_facts,self.test_facts]\n",
    "        for facts in facts_pool:\n",
    "            for fact in facts:\n",
    "                to_skip['lhs'][(fact[1], fact[2],fact[3],fact[4])].add(fact[0])  # left prediction\n",
    "                to_skip['rhs'][(fact[0], fact[2],fact[3],fact[4])].add(fact[1])  # right prediction\n",
    "                \n",
    "        for kk, skip in to_skip.items():\n",
    "            for k, v in skip.items():\n",
    "                self.to_skip_final[kk][k] = sorted(list(v))\n",
    "        print(\"data preprocess completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823310ae",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3076cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeRo(nn.Module):\n",
    "    def __init__(self, kg, embedding_dim, batch_size, learning_rate, L, gran, gamma, n_day, gpu=True):\n",
    "        super(TeRo, self).__init__()\n",
    "        self.gpu = gpu\n",
    "        self.kg = kg\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.n_day = n_day\n",
    "        self.gran = gran\n",
    "\n",
    "        self.L = L\n",
    "        # Nets\n",
    "        self.emb_E_real = torch.nn.Embedding(self.kg.n_entity, self.embedding_dim, padding_idx=0)\n",
    "        self.emb_E_img = torch.nn.Embedding(self.kg.n_entity, self.embedding_dim, padding_idx=0)\n",
    "        self.emb_R_real = torch.nn.Embedding(self.kg.n_relation*2, self.embedding_dim, padding_idx=0)\n",
    "        self.emb_R_img = torch.nn.Embedding(self.kg.n_relation*2, self.embedding_dim, padding_idx=0)\n",
    "        self.emb_Time = torch.nn.Embedding(n_day, self.embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Initialization\n",
    "        r = 6 / np.sqrt(self.embedding_dim)\n",
    "        self.emb_E_real.weight.data.uniform_(-r, r)\n",
    "        self.emb_E_img.weight.data.uniform_(-r, r)\n",
    "        self.emb_R_real.weight.data.uniform_(-r, r)\n",
    "        self.emb_R_img.weight.data.uniform_(-r, r)\n",
    "        self.emb_Time.weight.data.uniform_(-r, r)\n",
    "        # self.emb_T_img.weight.data.uniform_(-r, r)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        h_i, t_i, r_i, d_i = X[:, 0].astype(np.int64), X[:, 1].astype(np.int64), X[:, 2].astype(np.int64), X[:, 3].astype(np.int64)//self.gran\n",
    "\n",
    "        if self.gpu:\n",
    "            h_i = Variable(torch.from_numpy(h_i).cuda())\n",
    "            t_i = Variable(torch.from_numpy(t_i).cuda())\n",
    "            r_i = Variable(torch.from_numpy(r_i).cuda())\n",
    "            d_i = Variable(torch.from_numpy(d_i).cuda())\n",
    "        else:\n",
    "            h_i = Variable(torch.from_numpy(h_i))\n",
    "            t_i = Variable(torch.from_numpy(t_i))\n",
    "            r_i = Variable(torch.from_numpy(r_i))\n",
    "            d_i = Variable(torch.from_numpy(d_i))\n",
    "\n",
    "        pi = 3.14159265358979323846\n",
    "        d_img = torch.sin(self.emb_Time(d_i).view(-1, self.embedding_dim))#/(6 / np.sqrt(self.embedding_dim)/pi))\n",
    "\n",
    "        d_real = torch.cos(\n",
    "            self.emb_Time(d_i).view(-1, self.embedding_dim))#/(6 / np.sqrt(self.embedding_dim)/pi))\n",
    "\n",
    "        h_real = self.emb_E_real(h_i).view(-1, self.embedding_dim) *d_real-\\\n",
    "                 self.emb_E_img(h_i).view(-1,self.embedding_dim) *d_img\n",
    "\n",
    "        t_real = self.emb_E_real(t_i).view(-1, self.embedding_dim) *d_real-\\\n",
    "                 self.emb_E_img(t_i).view(-1,self.embedding_dim)*d_img\n",
    "\n",
    "\n",
    "        r_real = self.emb_R_real(r_i).view(-1, self.embedding_dim)\n",
    "\n",
    "        h_img = self.emb_E_real(h_i).view(-1, self.embedding_dim) *d_img+\\\n",
    "                 self.emb_E_img(h_i).view(-1,self.embedding_dim) *d_real\n",
    "\n",
    "\n",
    "        t_img = self.emb_E_real(t_i).view(-1, self.embedding_dim) *d_img+\\\n",
    "                self.emb_E_img(t_i).view(-1,self.embedding_dim) *d_real\n",
    "\n",
    "        r_img = self.emb_R_img(r_i).view(-1, self.embedding_dim)\n",
    "\n",
    "\n",
    "\n",
    "        if self.L == 'L1':\n",
    "            out_real = torch.sum(torch.abs(h_real + r_real - t_real), 1)\n",
    "            out_img = torch.sum(torch.abs(h_img + r_img + t_img), 1)\n",
    "            out = out_real + out_img\n",
    "\n",
    "        else:\n",
    "            out_real = torch.sum((h_real + r_real + d_i - t_real) ** 2, 1)\n",
    "            out_img = torch.sum((h_img + r_img + d_i + t_real) ** 2, 1)\n",
    "            out = torch.sqrt(out_img + out_real)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def normalize_embeddings(self):\n",
    "        self.emb_E_real.weight.data.renorm_(p=2, dim=0, maxnorm=1)\n",
    "        self.emb_E_img.weight.data.renorm_(p=2, dim=0, maxnorm=1)\n",
    "\n",
    "    def log_rank_loss(self, y_pos, y_neg, temp=0):\n",
    "        print('y_pos.shape: ',y_pos.shape, '| y_neg.shape: ',y_neg.shape)\n",
    "        M = y_pos.size(0)\n",
    "        N = y_neg.size(0)\n",
    "        y_pos = self.gamma-y_pos\n",
    "        y_neg = self.gamma-y_neg\n",
    "        C = int(N / M)\n",
    "        y_neg = y_neg.view(C, -1).transpose(0, 1)\n",
    "        p = F.softmax(temp * y_neg)\n",
    "        loss_pos = torch.sum(F.softplus(-1 * y_pos))\n",
    "        loss_neg = torch.sum(p * F.softplus(y_neg))\n",
    "        loss = (loss_pos + loss_neg) / 2 / M\n",
    "        if self.gpu:\n",
    "            loss = loss.cuda()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def rank_loss(self, y_pos, y_neg):\n",
    "        M = y_pos.size(0)\n",
    "        N = y_neg.size(0)\n",
    "        C = int(N / M)\n",
    "        y_pos = y_pos.repeat(C)\n",
    "        if self.gpu:\n",
    "            target = Variable(torch.from_numpy(-np.ones(N, dtype=np.float32))).cuda()\n",
    "        else:\n",
    "            target = Variable(torch.from_numpy(-np.ones(N, dtype=np.float32))).cpu()\n",
    "        loss = nn.MarginRankingLoss(margin=self.gamma)\n",
    "        loss = loss(y_pos, y_neg, target)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def rank_left(self, X, facts, kg, timedisc, rev_set=0):\n",
    "        rank = []\n",
    "        with torch.no_grad():\n",
    "            if timedisc:\n",
    "                for triple, fact in zip(X, facts):\n",
    "                    X_i = np.ones([self.kg.n_entity, 4])\n",
    "                    Xe_i = np.ones([self.kg.n_entity, 4])\n",
    "                    for i in range(0, self.kg.n_entity):\n",
    "                        X_i[i, 0] = i\n",
    "                        X_i[i, 1] = triple[1]\n",
    "                        X_i[i, 2] = triple[2] if triple[3]>=0 else triple[2]+self.kg.n_relation\n",
    "                        X_i[i, 3] = triple[3] if triple[3]>=0 else triple[4]\n",
    "                        Xe_i[i, 0] = i\n",
    "                        Xe_i[i, 1] = triple[1]\n",
    "                        Xe_i[i, 2] = triple[2]+self.kg.n_relation if triple[4]>=0 else triple[2]\n",
    "                        Xe_i[i, 3] = triple[4] if triple[4]>=0 else triple[3]\n",
    "                    i_score = self.forward(X_i)+self.forward(Xe_i)\n",
    "                    if rev_set>0:\n",
    "                        X_rev = np.ones([self.kg.n_entity,4])\n",
    "                        Xe_rev = np.ones([self.kg.n_entity,4])\n",
    "                        for i in range(0, self.kg.n_entity):\n",
    "                            X_rev[i, 0] = triple[1]\n",
    "                            X_rev[i, 1] = i\n",
    "                            X_rev[i, 2] = triple[2]+self.kg.n_relation//2 if triple[3]>=0 else triple[2]+self.kg.n_relation+self.kg.n_relation//2\n",
    "                            X_rev[i, 3] = triple[3] if triple[3]>=0 else triple[4]\n",
    "                            Xe_rev[i, 0] = triple[1]\n",
    "                            Xe_rev[i, 1] = i\n",
    "                            Xe_rev[i, 2] = triple[2]+self.kg.n_relation//2+self.kg.n_relation if triple[4]>=0 else triple[2]+self.kg.n_relation//2\n",
    "                            Xe_rev[i, 3] = triple[4] if triple[4]>=0 else triple[3]\n",
    "                        i_score = i_score + self.forward(X_rev).view(-1)+self.forward(Xe_rev).view(-1)\n",
    "                    if self.gpu:\n",
    "                        i_score = i_score.cuda()\n",
    "        \n",
    "                    filter_out = kg.to_skip_final['lhs'][(fact[1], fact[2],fact[3], fact[4])]                            \n",
    "                    target = i_score[int(triple[0])].clone()\n",
    "                    i_score[filter_out]=1e6 \n",
    "                    rank_triple=torch.sum((i_score < target).float()).cpu().item()+1\n",
    "                    rank.append(rank_triple)\n",
    "                        \n",
    "\n",
    "            else:\n",
    "                for triple, fact in zip(X, facts):\n",
    "                    X_i = np.ones([self.kg.n_entity, 4])\n",
    "                    for i in range(0, self.kg.n_entity):\n",
    "                        X_i[i, 0] = i\n",
    "                        X_i[i, 1] = triple[1]\n",
    "                        X_i[i, 2] = triple[2]\n",
    "                        X_i[i, 3] = triple[3]\n",
    "                    i_score = self.forward(X_i)\n",
    "                    if rev_set>0:\n",
    "                        X_rev = np.ones([self.kg.n_entity,4])\n",
    "                        for i in range(0, self.kg.n_entity):\n",
    "                            X_rev[i, 0] = triple[1]\n",
    "                            X_rev[i, 1] = i\n",
    "                            X_rev[i, 2] = triple[2]+self.kg.n_relation//2\n",
    "                            X_rev[i, 3] = triple[3]\n",
    "                        i_score = i_score + self.forward(X_rev).view(-1)\n",
    "                    if self.gpu:\n",
    "                        i_score = i_score.cuda()\n",
    "        \n",
    "                    filter_out = kg.to_skip_final['lhs'][(fact[1], fact[2],fact[3], fact[4])]                            \n",
    "                    target = i_score[int(triple[0])].clone()\n",
    "                    i_score[filter_out]=1e6 \n",
    "                    rank_triple=torch.sum((i_score < target).float()).cpu().item()+1\n",
    "                    rank.append(rank_triple)\n",
    "\n",
    "        return rank\n",
    "\n",
    "    def rank_right(self, X, facts, kg, timedisc, rev_set=0):\n",
    "        rank = []\n",
    "        with torch.no_grad():\n",
    "            if timedisc:\n",
    "                for triple, fact in zip(X, facts):\n",
    "                    X_i = np.ones([self.kg.n_entity, 4])\n",
    "                    Xe_i = np.ones([self.kg.n_entity, 4])\n",
    "                    for i in range(0, self.kg.n_entity):\n",
    "                        X_i[i, 0] = triple[0]\n",
    "                        X_i[i, 1] = i\n",
    "                        X_i[i, 2] = triple[2] if triple[3]>=0 else triple[2]+self.kg.n_relation\n",
    "                        X_i[i, 3] = triple[3] if triple[3]>=0 else triple[4]\n",
    "                        Xe_i[i, 0] = triple[0] \n",
    "                        Xe_i[i, 1] = i\n",
    "                        Xe_i[i, 2] = triple[2]+self.kg.n_relation if triple[4]>=0 else triple[2]\n",
    "                        Xe_i[i, 3] = triple[4] if triple[4]>=0 else triple[3]\n",
    "                    i_score = self.forward(X_i)+self.forward(Xe_i)\n",
    "                    if rev_set>0: \n",
    "                        X_rev = np.ones([self.kg.n_entity,4])\n",
    "                        Xe_rev = np.ones([self.kg.n_entity,4])\n",
    "                        for i in range(0, self.kg.n_entity):\n",
    "                            X_rev[i, 0] = i\n",
    "                            X_rev[i, 1] = triple[0]\n",
    "                            X_rev[i, 2] = triple[2]+self.kg.n_relation//2 if triple[3]>=0 else triple[2]+self.kg.n_relation+self.kg.n_relation//2\n",
    "                            X_rev[i, 3] = triple[3] if triple[3]>=0 else triple[4]\n",
    "                            Xe_rev[i, 0] = i\n",
    "                            Xe_rev[i, 1] = triple[0]\n",
    "                            Xe_rev[i, 2] = triple[2]+self.kg.n_relation//2+self.kg.n_relation if triple[4]>=0 else triple[2]+self.kg.n_relation//2\n",
    "                            Xe_rev[i, 3] = triple[4] if triple[4]>=0 else triple[3]\n",
    "                        i_score = i_score + self.forward(X_rev).view(-1)+ self.forward(Xe_rev).view(-1)\n",
    "                    if self.gpu:\n",
    "                        i_score = i_score.cuda()\n",
    "        \n",
    "                    filter_out = kg.to_skip_final['rhs'][(fact[0], fact[2],fact[3], fact[4])]       \n",
    "                    target = i_score[int(triple[1])].clone()\n",
    "                    i_score[filter_out]=1e6\n",
    "                    rank_triple=torch.sum((i_score < target).float()).cpu().item()+1\n",
    "        \n",
    "                    rank.append(rank_triple)\n",
    "                    \n",
    "            else:\n",
    "                for triple, fact in zip(X, facts):\n",
    "                    X_i = np.ones([self.kg.n_entity, 4])\n",
    "                    for i in range(0, self.kg.n_entity):\n",
    "                        X_i[i, 0] = triple[0]\n",
    "                        X_i[i, 1] = i\n",
    "                        X_i[i, 2] = triple[2]\n",
    "                        X_i[i, 3] = triple[3]\n",
    "                    i_score = self.forward(X_i)\n",
    "                    if rev_set>0: \n",
    "                        X_rev = np.ones([self.kg.n_entity,4])\n",
    "                        for i in range(0, self.kg.n_entity):\n",
    "                            X_rev[i, 0] = i\n",
    "                            X_rev[i, 1] = triple[0]\n",
    "                            X_rev[i, 2] = triple[2]+self.kg.n_relation//2\n",
    "                            X_rev[i, 3] = triple[3]\n",
    "                        i_score = i_score + self.forward(X_rev).view(-1)\n",
    "                    if self.gpu:\n",
    "                        i_score = i_score.cuda()\n",
    "        \n",
    "                    filter_out = kg.to_skip_final['rhs'][(fact[0], fact[2],fact[3], fact[4])]       \n",
    "                    target = i_score[int(triple[1])].clone()\n",
    "                    i_score[filter_out]=1e6\n",
    "                    rank_triple=torch.sum((i_score < target).float()).cpu().item()+1\n",
    "        \n",
    "                    rank.append(rank_triple)\n",
    "\n",
    "        return rank\n",
    "\n",
    "    def timepred(self, X):\n",
    "        rank = []\n",
    "        with torch.no_grad():\n",
    "            for triple in X:\n",
    "                X_i = np.ones([self.kg.n_day, len(triple)])\n",
    "                for i in range(self.kg.n_day):\n",
    "                    X_i[i, 0] = triple[0]\n",
    "                    X_i[i, 1] = triple[1]\n",
    "                    X_i[i, 2] = triple[2]\n",
    "                    X_i[i, 3:] = self.kg.time_dict[i]\n",
    "                i_score = self.forward(X_i)\n",
    "                if self.gpu:\n",
    "                    i_score = i_score.cuda()\n",
    "    \n",
    "                target = i_score[triple[3]]           \n",
    "                rank_triple=torch.sum((i_score < target).float()).cpu().item()+1\n",
    "                rank.append(rank_triple)\n",
    "\n",
    "        return rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20673d",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c094617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_rank(rank):\n",
    "    m_r = 0\n",
    "    N = len(rank)\n",
    "    for i in rank:\n",
    "        m_r = m_r + i / N\n",
    "\n",
    "    return m_r\n",
    "\n",
    "\n",
    "def mrr(rank):\n",
    "    mrr = 0\n",
    "    N = len(rank)\n",
    "    for i in rank:\n",
    "        mrr = mrr + 1 / i / N\n",
    "\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def hit_N(rank, N):\n",
    "    hit = 0\n",
    "    for i in rank:\n",
    "        if i <= N:\n",
    "            hit = hit + 1\n",
    "\n",
    "    hit = hit / len(rank)\n",
    "\n",
    "    return hit\n",
    "\n",
    "def get_minibatches(X, mb_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate minibatches from given dataset for training.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    X: np.array of M x 3\n",
    "        Contains the triplets from dataset. The entities and relations are\n",
    "        translated to its unique indices.\n",
    "\n",
    "    mb_size: int\n",
    "        Size of each minibatch.\n",
    "\n",
    "    shuffle: bool, default True\n",
    "        Whether to shuffle the dataset before dividing it into minibatches.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    mb_iter: generator\n",
    "        Example usage:\n",
    "        --------------\n",
    "        mb_iter = get_minibatches(X_train, mb_size)\n",
    "        for X_mb in mb_iter:\n",
    "            // do something with X_mb, the minibatch\n",
    "    \"\"\"\n",
    "    X_shuff = X.copy()\n",
    "    if shuffle:\n",
    "        X_shuff = skshuffle(X_shuff)\n",
    "\n",
    "    for i in range(0, X_shuff.shape[0], mb_size):\n",
    "        yield X_shuff[i:i + mb_size]\n",
    "\n",
    "\n",
    "def sample_negatives(X, C, kg):\n",
    "    \"\"\"\n",
    "    Perform negative sampling by corrupting head or tail of each triplets in\n",
    "    dataset.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    X: int matrix of M x 3, where M is the (mini)batch size\n",
    "        First column contains index of head entities.\n",
    "        Second column contains index of relationships.\n",
    "        Third column contains index of tail entities.\n",
    "\n",
    "    n_e: int\n",
    "        Number of entities in dataset.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_corr: int matrix of M x 3, where M is the (mini)batch size\n",
    "        Similar to input param X, but at each column, either first or third col\n",
    "        is subtituted with random entity.\n",
    "        \n",
    "    \"\"\"\n",
    "    M = X.shape[0]\n",
    "    X_corr = X\n",
    "    for i in range(C-1):\n",
    "        X_corr = np.concatenate((X_corr,X),0)\n",
    "    X_corr[:int(M*C/2),0]=torch.randint(kg.n_entity,[int(M*C/2)])        \n",
    "    X_corr[int(M*C/2):,1]=torch.randint(kg.n_entity,[int(M*C/2)]) \n",
    "\n",
    "    return X_corr\n",
    "\n",
    "\n",
    "def sample_negatives_t(X, C, n_day):\n",
    "    \"\"\"\n",
    "    Perform negative sampling by corrupting head or tail of each triplets in\n",
    "    dataset.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    X: int matrix of M x 4, where M is the (mini)batch size\n",
    "        First column contains index of head entities.\n",
    "        Second column contains index of relationships.\n",
    "        Third column contains index of tail entities.\n",
    "\n",
    "    n_e: int\n",
    "        Number of entities in dataset.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_corr: int matrix of M x 4, where M is the (mini)batch size\n",
    "        Similar to input param X, but at each column, either first or third col\n",
    "        is subtituted with random entity.\n",
    "    \"\"\"\n",
    "    M = X.shape[0]\n",
    "    X_corr = X\n",
    "    for i in range(C-1):\n",
    "        X_corr = torch.cat((X_corr,X),0)\n",
    "    X_corr[:,3]=torch.randint(n_day,[int(M*C)])        \n",
    "\n",
    "\n",
    "    return X_corr\n",
    "\n",
    "\n",
    "\n",
    "def train(task ='LinkPrediction',\n",
    "          modelname='ATISE',\n",
    "          data_dir='yago',\n",
    "          dim=500,\n",
    "          batch=512,\n",
    "          lr=0.1,\n",
    "          max_epoch=5000,\n",
    "          min_epoch=250,\n",
    "          gamma=1,\n",
    "          L = 'L1',\n",
    "          negsample_num=10,\n",
    "          timedisc = 0,\n",
    "          lossname = 'logloss',\n",
    "          cmin = 0.001,\n",
    "          cuda_able = True,\n",
    "          rev_set = 1,\n",
    "          temp = 0.5,\n",
    "          gran = 7,\n",
    "          count = 300\n",
    "          ):\n",
    "\n",
    "    randseed = 9999\n",
    "    np.random.seed(randseed)\n",
    "    torch.manual_seed(randseed)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Data Loading\n",
    "    \"\"\"\n",
    "    if data_dir == 'yago' or data_dir == 'wikidata':\n",
    "        kg = KnowledgeGraphYG(data_dir=data_dir, count = count,rev_set = rev_set)\n",
    "        n_day = kg.n_time\n",
    "        min_epoch=50\n",
    "    elif data_dir=='icews14':\n",
    "        n_day = 365\n",
    "        kg = KnowledgeGraph(data_dir=data_dir,gran=gran,rev_set = rev_set)\n",
    "    elif data_dir == 'icews05-15':\n",
    "        n_day = 4017\n",
    "        kg = KnowledgeGraph(data_dir=data_dir,gran=gran,rev_set = rev_set)      \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Create a model\n",
    "    \"\"\"\n",
    "\n",
    "    if modelname== 'TERO':\n",
    "        model = TeRo(kg, embedding_dim=dim, batch_size=batch, learning_rate=lr, gamma=gamma, L=L, gran=gran, n_day=kg.n_time,gpu=cuda_able)\n",
    "    if modelname=='ATISE':\n",
    "        model = ATISE(kg, embedding_dim=dim, batch_size=batch, learning_rate=lr, gamma=gamma, cmin=cmin, cmax=100*cmin, gpu=cuda_able)\n",
    "\n",
    "    if modelname == 'ATISE':\n",
    "        solver = torch.optim.Adam(model.parameters(), model.learning_rate)\n",
    "        optimizer = 'Adam'\n",
    "    else:\n",
    "        solver = torch.optim.Adagrad(model.parameters(), model.learning_rate)\n",
    "        optimizer = 'Adagrad'\n",
    "    \n",
    "\n",
    "    if timedisc == 0 or timedisc ==2:\n",
    "        train_pos = np.array(kg.training_triples)\n",
    "        validation_pos = np.array(kg.validation_triples)\n",
    "        test_pos = np.array(kg.test_triples)\n",
    "        \n",
    "    elif timedisc == 1:\n",
    "        train_pos = []\n",
    "        validation_pos = []\n",
    "        test_pos = []\n",
    "        for fact in kg.training_triples:\n",
    "            for time_index in range(fact[3],fact[4]+1):\n",
    "                train_pos.append([fact[0], fact[1], fact[2], time_index])\n",
    "        train_pos = np.array(train_pos)\n",
    "       # for fact in kg.validation_triples:\n",
    "       #     for time_index in range(fact[3],fact[4]+1):\n",
    "       #         validation_pos.append([fact[0], fact[1], fact[2], time_index])\n",
    "        validation_pos = np.array(kg.validation_triples)\n",
    "       # for fact in kg.test_triples:\n",
    "       #     for time_index in range(fact[3],fact[4]+1):\n",
    "       #         test_pos.append([fact[0], fact[1], fact[2], time_index])\n",
    "       # test_pos = np.array(test_pos)        \n",
    "        test_pos = np.array(kg.test_triples)\n",
    "\n",
    "        \n",
    "    losses = []\n",
    "    mrr_std = 0\n",
    "    C = negsample_num\n",
    "    patience = 0\n",
    "    path = os.path.join(data_dir,modelname,'timediscrete{:.0f}/dim{:.0f}/lr{:.4f}/neg_num{:.0f}/{:.0f}day/gamma{:.0f}/cmin{:.4f}'\n",
    "                        .format(timedisc,dim,lr,negsample_num,gran,gamma,cmin))\n",
    "    if timedisc: path = os.path.join(path,'{:.0f}count'.format(count))\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        print('path existed')\n",
    "        return\n",
    "    \n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    Training Process\n",
    "    \"\"\"\n",
    "    for epoch in range(max_epoch):\n",
    "        print('Epoch-{}'.format(epoch + 1))\n",
    "        print('————————————————')\n",
    "        it = 0\n",
    "        train_triple = list(get_minibatches(train_pos, batch, shuffle=True))\n",
    "        for iter_triple in train_triple:\n",
    "            if iter_triple.shape[0] < batch:\n",
    "                break\n",
    "            start = time.time()\n",
    "            if task=='TimePrediction':\n",
    "                iter_neg = sample_negatives_t(iter_triple, C, n_day)\n",
    "            else:\n",
    "                iter_neg = sample_negatives(iter_triple, C, kg)\n",
    "            if timedisc == 2:\n",
    "                end_miss = np.where(iter_triple[:,4:5]<0)[0]\n",
    "                start_miss = np.where(iter_triple[:,3:4]<0)[0]\n",
    "                neg_end_miss = np.where(iter_neg[:,4:5]<0)[0]\n",
    "                neg_start_miss = np.where(iter_neg[:,3:4]<0)[0]\n",
    "                \n",
    "                \n",
    "                iter_triple_e = np.delete(iter_triple,3,1)\n",
    "                iter_triple = np.delete(iter_triple,4,1)\n",
    "                \n",
    "                iter_triple_e[:,2:3] += kg.n_relation\n",
    "\n",
    "                iter_triple_e[end_miss,:]=iter_triple[end_miss,:]\n",
    "                iter_triple[start_miss,:]=iter_triple_e[start_miss,:]\n",
    "                \n",
    "                \n",
    "                iter_neg_e = np.delete(iter_neg,3,1)\n",
    "                iter_neg = np.delete(iter_neg,4,1)\n",
    "                \n",
    "                iter_neg_e[:,2:3] += kg.n_relation\n",
    "                \n",
    "                iter_neg_e[neg_end_miss,:]=iter_neg[neg_end_miss,:]\n",
    "                iter_neg[neg_start_miss,:]=iter_neg_e[neg_start_miss,:]\n",
    "                \n",
    "\n",
    "            pos_score = model.forward(iter_triple)\n",
    "            neg_score = model.forward(iter_neg)\n",
    "            if timedisc ==2:\n",
    "                pos_score += model.forward(iter_triple_e)\n",
    "                neg_score += model.forward(iter_neg_e)\n",
    "                \n",
    "            if lossname == 'logloss':\n",
    "                loss = model.log_rank_loss(pos_score, neg_score,temp=temp)\n",
    "            else:\n",
    "                loss = model.rank_loss(pos_score, neg_score)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            solver.zero_grad()\n",
    "            loss.backward()\n",
    "            solver.step()\n",
    "\n",
    "\n",
    "            if lossname == 'marginloss':\n",
    "                model.normalize_embeddings()\n",
    "            if modelname == 'ATISE':\n",
    "                model.regularization_embeddings()\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            if it % 33 == 0:\n",
    "                print('Iter-{}; loss: {:.4f};time per batch:{:.4f}s'.format(it, loss.item(), end - start))\n",
    "\n",
    "            it += 1\n",
    "\n",
    "        \"\"\"\n",
    "        Evaluation for Link Prediction\n",
    "        \"\"\"\n",
    "\n",
    "        if ((epoch+1)//min_epoch>epoch//min_epoch and epoch < max_epoch) :\n",
    "            if task == 'LinkPrediction':\n",
    "                rank = model.rank_left(validation_pos,kg.validation_facts,kg,timedisc,rev_set=rev_set)\n",
    "                rank_right = model.rank_right(validation_pos,kg.validation_facts,kg,timedisc,rev_set=rev_set)\n",
    "                rank = rank + rank_right\n",
    "            else:\n",
    "                rank = model.timepred(validation_pos)\n",
    "\n",
    "            m_rank = mean_rank(rank)\n",
    "            mean_rr = mrr(rank)\n",
    "            hit_1 = hit_N(rank, 1)\n",
    "            hit_3 = hit_N(rank, 3)\n",
    "            hit_5 = hit_N(rank, 5)\n",
    "            hit_10 = hit_N(rank, 10)\n",
    "            print('validation results:')\n",
    "            print('Mean Rank: {:.0f}'.format(m_rank))\n",
    "            print('Mean RR: {:.4f}'.format(mean_rr))\n",
    "            print('Hit@1: {:.4f}'.format(hit_1))\n",
    "            print('Hit@3: {:.4f}'.format(hit_3))\n",
    "            print('Hit@5: {:.4f}'.format(hit_5))\n",
    "            print('Hit@10: {:.4f}'.format(hit_10))\n",
    "            f = open(os.path.join(path, 'result{:.0f}.txt'.format(epoch)), 'w')\n",
    "            f.write('Mean Rank: {:.0f}\\n'.format(m_rank))\n",
    "            f.write('Mean RR: {:.4f}\\n'.format(mean_rr))\n",
    "            f.write('Hit@1: {:.4f}\\n'.format(hit_1))\n",
    "            f.write('Hit@3: {:.4f}\\n'.format(hit_3))\n",
    "            f.write('Hit@5: {:.4f}\\n'.format(hit_5))\n",
    "            f.write('Hit@10: {:.4f}\\n'.format(hit_10))\n",
    "            for loss in losses:\n",
    "                f.write(str(loss))\n",
    "                f.write('\\n')\n",
    "            f.close()\n",
    "            if mean_rr < mrr_std and patience<3:\n",
    "                patience+=1\n",
    "            elif (mean_rr < mrr_std and patience>=3) or epoch==max_epoch-1:\n",
    "                if epoch == max_epoch-1:\n",
    "                    torch.save(model.state_dict(), os.path.join(path, 'params.pkl'))\n",
    "                model.load_state_dict(torch.load(os.path.join(path,'params.pkl')))\n",
    "                if task == 'LinkPrediction':\n",
    "                    rank = model.rank_left(test_pos,kg.test_facts,kg,timedisc,rev_set=rev_set)\n",
    "                    rank_right = model.rank_right(test_pos,kg.test_facts,kg,timedisc,rev_set=rev_set)\n",
    "                    rank = rank + rank_right\n",
    "                else:\n",
    "                    rank = model.timepred(test_pos)\n",
    "\n",
    "\n",
    "                m_rank = mean_rank(rank)\n",
    "                mean_rr = mrr(rank)\n",
    "                hit_1 = hit_N(rank, 1)\n",
    "                hit_3 = hit_N(rank, 3)\n",
    "                hit_5 = hit_N(rank, 5)\n",
    "                hit_10 = hit_N(rank, 10)\n",
    "                print('test result:')\n",
    "                print('Mean Rank: {:.0f}'.format(m_rank))\n",
    "                print('Mean RR: {:.4f}'.format(mean_rr))\n",
    "                print('Hit@1: {:.4f}'.format(hit_1))\n",
    "                print('Hit@3: {:.4f}'.format(hit_3))\n",
    "                print('Hit@5: {:.4f}'.format(hit_5))\n",
    "                print('Hit@10: {:.4f}'.format(hit_10))\n",
    "                if epoch == max_epoch-1:\n",
    "                    f = open(os.path.join(path, 'test_result{:.0f}.txt'.format(epoch)), 'w')\n",
    "                else:\n",
    "                    f = open(os.path.join(path, 'test_result{:.0f}.txt'.format(epoch)), 'w')\n",
    "                f.write('Mean Rank: {:.0f}\\n'.format(m_rank))\n",
    "                f.write('Mean RR: {:.4f}\\n'.format(mean_rr))\n",
    "                f.write('Hit@1: {:.4f}\\n'.format(hit_1))\n",
    "                f.write('Hit@3: {:.4f}\\n'.format(hit_3))\n",
    "                f.write('Hit@5: {:.4f}\\n'.format(hit_5))\n",
    "                f.write('Hit@10: {:.4f}\\n'.format(hit_10))\n",
    "                for loss in losses:\n",
    "                    f.write(str(loss))\n",
    "                    f.write('\\n')\n",
    "                f.close()\n",
    "                break\n",
    "            if mean_rr>=mrr_std:\n",
    "                \n",
    "                torch.save(model.state_dict(), os.path.join(path, 'params.pkl'))\n",
    "                mrr_std = mean_rr\n",
    "                patience = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e9b87",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59125d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5abea0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Params()\n",
    "args.task = 'LinkPrediction'\n",
    "args.model = 'TERO'\n",
    "args.dataset = 'icews14'\n",
    "args.max_epoch = 5000\n",
    "args.dim = 500\n",
    "args.batch = 512\n",
    "args.lr = 0.1\n",
    "args.gamma = 110\n",
    "args.eta = 10\n",
    "args.timedisc = 0\n",
    "args.cuda = True\n",
    "args.loss = 'logloss'\n",
    "args.cmin = 0.005\n",
    "args.gran = 1\n",
    "args.thre = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f76498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print(args)\n",
    "    train(task=args.task,\n",
    "          modelname=args.model,\n",
    "          data_dir=args.dataset,\n",
    "          dim=args.dim,\n",
    "          batch=args.batch,\n",
    "          lr =args.lr,\n",
    "          max_epoch=args.max_epoch,\n",
    "          gamma = args.gamma,\n",
    "          lossname = args.loss,\n",
    "          negsample_num=args.eta,\n",
    "          timedisc = args.timedisc,\n",
    "          cuda_able = args.cuda,\n",
    "          cmin = args.cmin,\n",
    "          gran = args.gran,\n",
    "          count = args.thre\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3915c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Params object at 0x7f3d92ffc280>\n",
      "-----Loading entity dict-----\n",
      "#entity: 7129\n",
      "-----Loading relation dict-----\n",
      "#relation: 460\n",
      "-----Loading training triples-----\n",
      "#training triple: 145652\n",
      "-----Loading validation triples-----\n",
      "#validation triple: 8941\n",
      "-----Loading test triples------\n",
      "#test triple: 8963\n",
      "creating filtering lists\n",
      "data preprocess completed\n",
      "Epoch-1\n",
      "————————————————\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "Iter-0; loss: 54.0175;time per batch:0.0117s\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1035257/2115009602.py:102: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.softmax(temp * y_neg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "Iter-33; loss: 5.5224;time per batch:0.0079s\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "Iter-66; loss: 3.0208;time per batch:0.0547s\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "Iter-99; loss: 2.6475;time per batch:0.0077s\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "Iter-132; loss: 2.0175;time per batch:0.0543s\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "Iter-165; loss: 1.9610;time per batch:0.0076s\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "Iter-198; loss: 1.3909;time per batch:0.0093s\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "Iter-231; loss: 1.3744;time per batch:0.0075s\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "Iter-264; loss: 0.8867;time per batch:0.0072s\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "Epoch-2\n",
      "————————————————\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "Iter-0; loss: 0.5015;time per batch:0.0073s\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "Iter-33; loss: 0.5098;time per batch:0.0070s\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n",
      "y_pos.shape:  torch.Size([512]) | y_neg.shape:  torch.Size([5120])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(args):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(args)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmodelname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlossname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m          \u001b[49m\u001b[43mnegsample_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtimedisc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimedisc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcuda_able\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcmin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgran\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgran\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthre\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m          \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 271\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(task, modelname, data_dir, dim, batch, lr, max_epoch, min_epoch, gamma, L, negsample_num, timedisc, lossname, cmin, cuda_able, rev_set, temp, gran, count)\u001b[0m\n\u001b[1;32m    268\u001b[0m     neg_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(iter_neg_e)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lossname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 271\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_rank_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrank_loss(pos_score, neg_score)\n",
      "Cell \u001b[0;32mIn[4], line 102\u001b[0m, in \u001b[0;36mTeRo.log_rank_loss\u001b[0;34m(self, y_pos, y_neg, temp)\u001b[0m\n\u001b[1;32m    100\u001b[0m C \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(N \u001b[38;5;241m/\u001b[39m M)\n\u001b[1;32m    101\u001b[0m y_neg \u001b[38;5;241m=\u001b[39m y_neg\u001b[38;5;241m.\u001b[39mview(C, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_neg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m loss_pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(F\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m y_pos))\n\u001b[1;32m    104\u001b[0m loss_neg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(p \u001b[38;5;241m*\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(y_neg))\n",
      "File \u001b[0;32m~/anaconda3/envs/TestEnv/lib/python3.9/site-packages/torch/nn/functional.py:1841\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1839\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1841\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m(dim)\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
