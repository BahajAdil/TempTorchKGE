{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a86f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "# encoding: utf-8\n",
    "import os\n",
    "from collections import defaultdict as ddict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from numpy.random import RandomState\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caed5ef",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679a1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraph:\n",
    "    def __init__(self, data_dir, gran=1,rev_set=0):\n",
    "        self.data_dir = data_dir\n",
    "        self.entity_dict = {}\n",
    "        self.gran = gran\n",
    "        self.entities = []\n",
    "        self.relation_dict = {}\n",
    "        self.n_entity = 0\n",
    "        self.n_relation = 0\n",
    "        self.training_triples = []  # list of triples in the form of (h, t, r)\n",
    "        self.validation_triples = []\n",
    "        self.test_triples = []\n",
    "        self.training_facts = []\n",
    "        self.validation_facts = []\n",
    "        self.test_facts = []\n",
    "        self.n_training_triple = 0\n",
    "        self.n_validation_triple = 0\n",
    "        self.n_test_triple = 0\n",
    "        self.rev_set = rev_set\n",
    "        self.start_date = '2014-01-01' if self.data_dir == 'icews14' else '2005-01-01'\n",
    "        self.start_sec = time.mktime(time.strptime(self.start_date,'%Y-%m-%d'))\n",
    "        self.n_time=365 if self.data_dir == 'icews14' else 4017\n",
    "        self.to_skip_final = {'lhs': {}, 'rhs': {}}\n",
    "        '''load dicts and triples'''\n",
    "        self.load_dicts()\n",
    "        self.load_triples()\n",
    "        self.load_filters()\n",
    "        '''construct pools after loading'''\n",
    "        # self.training_triple_pool = set(self.training_triples)\n",
    "        # self.golden_triple_pool = set(self.training_triples) | set(self.validation_triples) | set(self.test_triples)\n",
    "\n",
    "    def load_dicts(self):\n",
    "        entity_dict_file = 'entity2id.txt'\n",
    "        relation_dict_file = 'relation2id.txt'\n",
    "        print('-----Loading entity dict-----')\n",
    "        entity_df = pd.read_table(os.path.join(self.data_dir, entity_dict_file), header=None)\n",
    "        self.entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
    "        self.n_entity = len(self.entity_dict)\n",
    "        self.entities = list(self.entity_dict.values())\n",
    "        print('#entity: {}'.format(self.n_entity))\n",
    "        print('-----Loading relation dict-----')\n",
    "        relation_df = pd.read_table(os.path.join(self.data_dir, relation_dict_file), header=None)\n",
    "        self.relation_dict = dict(zip(relation_df[0], relation_df[1]))\n",
    "        self.n_relation = len(self.relation_dict)\n",
    "        if self.rev_set>0: self.n_relation *= 2\n",
    "        print('#relation: {}'.format(self.n_relation))\n",
    "\n",
    "    def load_triples(self):\n",
    "        training_file = 'train.txt'\n",
    "        validation_file = 'valid.txt'\n",
    "        test_file = 'test.txt'\n",
    "        print('-----Loading training triples-----')\n",
    "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None)\n",
    "        training_df = np.array(training_df).tolist()\n",
    "        for triple in training_df:\n",
    "            end_sec = time.mktime(time.strptime(triple[3], '%Y-%m-%d'))\n",
    "            day = int((end_sec - self.start_sec) / (self.gran*24 * 60 * 60))\n",
    "            self.training_triples.append([self.entity_dict[triple[0]],self.entity_dict[triple[2]],self.relation_dict[triple[1]],day])\n",
    "            self.training_facts.append([self.entity_dict[triple[0]],self.entity_dict[triple[2]],self.relation_dict[triple[1]],triple[3],0])\n",
    "            if self.rev_set>0: self.training_triples.append([self.entity_dict[triple[2]],self.entity_dict[triple[0]],self.relation_dict[triple[1]]+self.n_relation//2,day])\n",
    "\n",
    "        self.n_training_triple = len(self.training_triples)\n",
    "        print('#training triple: {}'.format(self.n_training_triple))\n",
    "        print('-----Loading validation triples-----')\n",
    "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None)\n",
    "        validation_df = np.array(validation_df).tolist()\n",
    "        for triple in validation_df:\n",
    "            end_sec = time.mktime(time.strptime(triple[3], '%Y-%m-%d'))\n",
    "            day = int((end_sec - self.start_sec) / (self.gran*24 * 60 * 60))\n",
    "            self.validation_triples.append([self.entity_dict[triple[0]],self.entity_dict[triple[2]],self.relation_dict[triple[1]],day])\n",
    "            self.validation_facts.append([self.entity_dict[triple[0]],self.entity_dict[triple[2]],self.relation_dict[triple[1]],triple[3],0])\n",
    "\n",
    "        self.n_validation_triple = len(self.validation_triples)\n",
    "        print('#validation triple: {}'.format(self.n_validation_triple))\n",
    "        print('-----Loading test triples------')\n",
    "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None)\n",
    "        test_df = np.array(test_df).tolist()\n",
    "        for triple in test_df:\n",
    "            end_sec = time.mktime(time.strptime(triple[3], '%Y-%m-%d'))\n",
    "            day = int((end_sec - self.start_sec) / (self.gran*24 * 60 * 60))\n",
    "            self.test_triples.append(\n",
    "                    [self.entity_dict[triple[0]], self.entity_dict[triple[2]], self.relation_dict[triple[1]], day])\n",
    "            self.test_facts.append([self.entity_dict[triple[0]],self.entity_dict[triple[2]],self.relation_dict[triple[1]],triple[3],0])\n",
    "\n",
    "        self.n_test_triple = len(self.test_triples)\n",
    "        print('#test triple: {}'.format(self.n_test_triple))\n",
    "\n",
    "\n",
    "    def load_filters(self):\n",
    "        print(\"creating filtering lists\")\n",
    "        to_skip = {'lhs': defaultdict(set), 'rhs': defaultdict(set)}\n",
    "        facts_pool = [self.training_facts,self.validation_facts,self.test_facts]\n",
    "        for facts in facts_pool:\n",
    "            for fact in facts:\n",
    "                to_skip['lhs'][(fact[1], fact[2],fact[3], fact[4])].add(fact[0])  # left prediction\n",
    "                to_skip['rhs'][(fact[0], fact[2],fact[3], fact[4])].add(fact[1])  # right prediction\n",
    "                \n",
    "        for kk, skip in to_skip.items():\n",
    "            for k, v in skip.items():\n",
    "                self.to_skip_final[kk][k] = sorted(list(v))\n",
    "        print(\"data preprocess completed\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a6f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphYG:\n",
    "    def __init__(self, data_dir, count=300, rev_set=0):\n",
    "        self.data_dir = data_dir\n",
    "        self.entity_dict = {}\n",
    "        self.entities = []\n",
    "        self.relation_dict = {}\n",
    "        self.n_entity = 0\n",
    "        self.n_relation = 0\n",
    "        self.training_triples = []  # list of triples in the form of (h, t, r)\n",
    "        self.validation_triples = []\n",
    "        self.test_triples = []\n",
    "        self.training_facts = []\n",
    "        self.validation_facts = []\n",
    "        self.test_facts = []\n",
    "        self.n_training_triple = 0\n",
    "        self.n_validation_triple = 0\n",
    "        self.n_test_triple = 0\n",
    "        self.n_time = 0\n",
    "        self.start_year= -500\n",
    "        self.end_year = 3000\n",
    "        self.year_class=[]\n",
    "        self.year2id = dict()\n",
    "        self.rev_set = rev_set\n",
    "        self.fact_count = count\n",
    "        self.to_skip_final = {'lhs': {}, 'rhs': {}}\n",
    "        '''load dicts and triples'''\n",
    "        self.time_list()\n",
    "        self.load_dicts()\n",
    "        self.load_triples()\n",
    "        self.load_filters()\n",
    "        '''construct pools after loading'''\n",
    "        # self.training_triple_pool = set(self.training_triples)\n",
    "        # self.golden_triple_pool = set(self.training_triples) | set(self.validation_triples) | set(self.test_triples)\n",
    "\n",
    "    def load_dicts(self):\n",
    "        entity_dict_file = 'entity2id.txt'\n",
    "        relation_dict_file = 'relation2id.txt'\n",
    "        print('-----Loading entity dict-----')\n",
    "        entity_df = pd.read_table(os.path.join(self.data_dir, entity_dict_file), header=None)\n",
    "        self.entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
    "        self.n_entity = len(self.entity_dict)\n",
    "        self.entities = list(self.entity_dict.values())\n",
    "        print('#entity: {}'.format(self.n_entity))\n",
    "        print('-----Loading relation dict-----')\n",
    "        relation_df = pd.read_table(os.path.join(self.data_dir, relation_dict_file), header=None)\n",
    "        self.relation_dict = dict(zip(relation_df[0], relation_df[1]))\n",
    "        self.n_relation = len(self.relation_dict)\n",
    "        if self.rev_set>0: self.n_relation *= 2\n",
    "        print('#relation: {}'.format(self.n_relation))\n",
    "\n",
    "    def time_list(self):\n",
    "        training_file = 'train.txt'\n",
    "        validation_file = 'valid.txt'\n",
    "        test_file = 'test.txt'\n",
    "        triple_file = 'triple2id.txt'\n",
    "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None)\n",
    "        training_df = np.array(training_df).tolist()\n",
    "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None)\n",
    "        validation_df = np.array(validation_df).tolist()\n",
    "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None)\n",
    "        test_df = np.array(test_df).tolist()\n",
    " #       triple_df = pd.read_table(os.path.join(self.data_dir, triple_file), header=None)\n",
    " #       triple_df = np.array(triple_df).tolist()\n",
    "        triple_df = np.concatenate([training_df,validation_df,test_df],axis=0)\n",
    "        n=0\n",
    "        \n",
    "        year_list=[]\n",
    "        for triple in triple_df:\n",
    "            n+=1\n",
    "            if triple[3][0]=='-':\n",
    "                start = -int(triple[3].split('-')[1])\n",
    "                year_list.append(start)\n",
    "            else:\n",
    "                start = triple[3].split('-')[0]\n",
    "                if start =='####':\n",
    "                    start = self.start_year\n",
    "                else:\n",
    "                    start = start.replace('#', '0')\n",
    "                    start = int(start)\n",
    "                    year_list.append(start)\n",
    "\n",
    "\n",
    "            if triple[4][0]=='-':\n",
    "                end = -int(triple[4].split('-')[1])\n",
    "                year_list.append(end)\n",
    "            else:\n",
    "                end = triple[4].split('-')[0]\n",
    "                if end =='####':\n",
    "                    end = self.end_year\n",
    "                else:\n",
    "                    end = end.replace('#', '0')\n",
    "                    end = int(end)\n",
    "                    year_list.append(end)\n",
    "\n",
    "#            for i in range(start,end):\n",
    "#                 year_list.append(i)\n",
    "            \n",
    "\n",
    "\n",
    "        year_list.sort()\n",
    "\n",
    "        freq=ddict(int)\n",
    "        for year in year_list:\n",
    "            freq[year]=freq[year]+1\n",
    "\n",
    "        year_class=[]\n",
    "        count=0\n",
    "        for key in sorted(freq.keys()):\n",
    "            count += freq[key]\n",
    "            if count>=self.fact_count:\n",
    "                year_class.append(key)\n",
    "                count=0\n",
    "        year_class[-1]=year_list[-1]\n",
    "\n",
    "        year2id = dict()\n",
    "        prev_year = year_list[0]\n",
    "        i = 0\n",
    "        for i, yr in enumerate(year_class): \n",
    "            year2id[(prev_year, yr)] = i\n",
    " #           if i>2: \n",
    "            prev_year = yr + 1\n",
    "\n",
    "        self.year2id=year2id\n",
    "        self.year_class = year_class\n",
    "        self.n_time = len(self.year2id.keys())\n",
    "\n",
    "\n",
    "    def load_triples(self):\n",
    "        training_file = 'train.txt'\n",
    "        validation_file = 'valid.txt'\n",
    "        test_file = 'test.txt'\n",
    "        print('-----Loading training triples-----')\n",
    "        training_df = pd.read_table(os.path.join(self.data_dir, training_file), header=None)\n",
    "        training_df = np.array(training_df).tolist()\n",
    "        for triple in training_df:\n",
    "            if triple[3].split('-')[0] == '####':\n",
    "                start=self.start_year\n",
    "                start_idx = 0\n",
    "            elif triple[3][0] == '-':\n",
    "                start=-int(triple[3].split('-')[1].replace('#', '0'))\n",
    "            elif triple[3][0] != '-':\n",
    "                start = int(triple[3].split('-')[0].replace('#','0'))\n",
    "            \n",
    "            if triple[4].split('-')[0] == '####':\n",
    "                end = self.end_year\n",
    "                end_idx = self.n_time-1\n",
    "            elif triple[4][0] == '-':\n",
    "                end =-int(triple[4].split('-')[1].replace('#', '0'))\n",
    "            elif triple[4][0] != '-':\n",
    "                end = int(triple[4].split('-')[0].replace('#','0'))\n",
    "        \n",
    "            for key, time_idx in sorted(self.year2id.items(), key=lambda x:x[1]):\n",
    "                if start>=key[0] and start<=key[1]:\n",
    "                    start_idx = time_idx\n",
    "                if end>=key[0] and end<=key[1]:\n",
    "                    end_idx = time_idx\n",
    "\n",
    "\n",
    "            self.training_triples.append([triple[0],triple[2],triple[1],start_idx,end_idx])\n",
    "            self.training_facts.append([triple[0],triple[2],triple[1],triple[3],triple[4]])\n",
    "            if self.rev_set>0: self.training_triples.append([triple[2],triple[0],triple[1]+self.n_relation//2,start_idx,end_idx])\n",
    "            # for day_idx in range(start_idx,end_idx+1):\n",
    "            #     try:\n",
    "            #         self.training_triples.append([triple[0],triple[2],triple[1],day_idx])\n",
    "            #     except KeyError:\n",
    "            #         continue\n",
    "        self.n_training_triple = len(self.training_triples)\n",
    "        print('#training triple: {}'.format(self.n_training_triple))\n",
    "        print('-----Loading validation triples-----')\n",
    "        validation_df = pd.read_table(os.path.join(self.data_dir, validation_file), header=None)\n",
    "        validation_df = np.array(validation_df).tolist()\n",
    "        for triple in validation_df:\n",
    "            if triple[3].split('-')[0] == '####':\n",
    "                start=self.start_year\n",
    "                start_idx = 0\n",
    "            elif triple[3][0] == '-':\n",
    "                start=-int(triple[3].split('-')[1].replace('#', '0'))\n",
    "            elif triple[3][0] != '-':\n",
    "                start = int(triple[3].split('-')[0].replace('#','0'))\n",
    "            \n",
    "            if triple[4].split('-')[0] == '####':\n",
    "                end = self.end_year\n",
    "                end_idx = self.n_time-1\n",
    "            elif triple[4][0] == '-':\n",
    "                end =-int(triple[4].split('-')[1].replace('#', '0'))\n",
    "            elif triple[4][0] != '-':\n",
    "                end = int(triple[4].split('-')[0].replace('#','0'))\n",
    "        \n",
    "            for key, time_idx in sorted(self.year2id.items(), key=lambda x:x[1]):\n",
    "                if start>=key[0] and start<=key[1]:\n",
    "                    start_idx = time_idx\n",
    "                if end>=key[0] and end<=key[1]:\n",
    "                    end_idx = time_idx\n",
    "            \n",
    "                    \n",
    "            self.validation_triples.append([triple[0],triple[2],triple[1],start_idx,end_idx])\n",
    "            self.validation_facts.append([triple[0],triple[2],triple[1],triple[3],triple[4]])\n",
    "            # for day_idx in range(start_idx,end_idx+1):\n",
    "            #     try:\n",
    "            #         self.validation_triples.append([triple[0],triple[2],triple[1],day_idx])\n",
    "            #     except KeyError:\n",
    "            #         continue\n",
    "        self.n_validation_triple = len(self.validation_triples)\n",
    "        print('#validation triple: {}'.format(self.n_validation_triple))\n",
    "        print('-----Loading test triples------')\n",
    "        test_df = pd.read_table(os.path.join(self.data_dir, test_file), header=None)\n",
    "        test_df = np.array(test_df).tolist()\n",
    "        for triple in test_df:\n",
    "            if triple[3].split('-')[0] == '####':\n",
    "                start=self.start_year\n",
    "                start_idx = 0\n",
    "            elif triple[3][0] == '-':\n",
    "                start=-int(triple[3].split('-')[1].replace('#', '0'))\n",
    "            elif triple[3][0] != '-':\n",
    "                start = int(triple[3].split('-')[0].replace('#','0'))\n",
    "            \n",
    "            if triple[4].split('-')[0] == '####':\n",
    "                end = self.end_year\n",
    "                end_idx = self.n_time-1\n",
    "            elif triple[4][0] == '-':\n",
    "                end =-int(triple[4].split('-')[1].replace('#', '0'))\n",
    "            elif triple[4][0] != '-':\n",
    "                end = int(triple[4].split('-')[0].replace('#','0'))\n",
    "        \n",
    "            for key, time_idx in sorted(self.year2id.items(), key=lambda x:x[1]):\n",
    "                if start>=key[0] and start<=key[1]:\n",
    "                    start_idx = time_idx\n",
    "                if end>=key[0] and end<=key[1]:\n",
    "                    end_idx = time_idx\n",
    "                    \n",
    "\n",
    "            self.test_triples.append([triple[0],triple[2],triple[1],start_idx,end_idx])\n",
    "            self.test_facts.append([triple[0],triple[2],triple[1],triple[3],triple[4]])\n",
    "            # for day_idx in range(start_idx,end_idx+1):\n",
    "            #     try:\n",
    "            #         self.test_triples.append([triple[0],triple[2],triple[1],day_idx])\n",
    "            #     except KeyError:\n",
    "            #         continue\n",
    "        self.n_test_triple = len(self.test_triples)\n",
    "        print('#test triple: {}'.format(self.n_test_triple))\n",
    "\n",
    "    def load_filters(self):\n",
    "        print(\"creating filtering lists\")\n",
    "        to_skip = {'lhs': defaultdict(set), 'rhs': defaultdict(set)}\n",
    "        facts_pool = [self.training_facts,self.validation_facts,self.test_facts]\n",
    "        for facts in facts_pool:\n",
    "            for fact in facts:\n",
    "                to_skip['lhs'][(fact[1], fact[2],fact[3],fact[4])].add(fact[0])  # left prediction\n",
    "                to_skip['rhs'][(fact[0], fact[2],fact[3],fact[4])].add(fact[1])  # right prediction\n",
    "                \n",
    "        for kk, skip in to_skip.items():\n",
    "            for k, v in skip.items():\n",
    "                self.to_skip_final[kk][k] = sorted(list(v))\n",
    "        print(\"data preprocess completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfddcf0d",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ab44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quat_dot_prod(q1, q2):\n",
    "    return torch.sum(q1* q2, -1)\n",
    "\n",
    "def hamilton_quat_prod(q1, q2):\n",
    "    s_a, x_a, y_a, z_a = torch.chunk(q1, 4, dim=-1)\n",
    "    s_b, x_b, y_b, z_b = torch.chunk(q2, 4, dim=-1)\n",
    "    \n",
    "    A = s_a * s_b - x_a * x_b - y_a * y_b - z_a * z_b\n",
    "    B = s_a * x_b + s_b * x_a + y_a * z_b - y_b * z_a\n",
    "    C = s_a * y_b + s_b * y_a + z_a * x_b - z_b * x_a\n",
    "    D = s_a * z_b + s_b * z_a + x_a * y_b - x_b * y_a\n",
    "\n",
    "    return torch.cat([A, B, C, D], dim=-1)\n",
    "\n",
    "def quat_norm(q):\n",
    "    s_b, x_b, y_b, z_b = torch.chunk(q, 4, dim=-1)\n",
    "    denominator_b = torch.sqrt(s_b ** 2 + x_b ** 2 + y_b ** 2 + z_b ** 2)\n",
    "    s_b = s_b / denominator_b\n",
    "    x_b = x_b / denominator_b\n",
    "    y_b = y_b / denominator_b\n",
    "    z_b = z_b / denominator_b\n",
    "    return torch.cat([s_b, x_b, y_b, z_b], dim=-1)\n",
    "\n",
    "def quaternion_init(in_features, out_features, criterion='he'):\n",
    "    fan_in = in_features\n",
    "    fan_out = out_features\n",
    "\n",
    "    if criterion == 'glorot':\n",
    "        s = 1. / np.sqrt(2 * (fan_in + fan_out))\n",
    "    elif criterion == 'he':\n",
    "        s = 1. / np.sqrt(2 * fan_in)\n",
    "    else:\n",
    "        raise ValueError('Invalid criterion: ', criterion)\n",
    "    rng = RandomState(123)\n",
    "\n",
    "    # Generating randoms and purely imaginary quaternions :\n",
    "    kernel_shape = (in_features, out_features)\n",
    "\n",
    "    number_of_weights = np.prod(kernel_shape)\n",
    "    v_i = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    v_j = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    v_k = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "\n",
    "    # Purely imaginary quaternions unitary\n",
    "    for i in range(0, number_of_weights):\n",
    "        norm = np.sqrt(v_i[i] ** 2 + v_j[i] ** 2 + v_k[i] ** 2) + 0.0001\n",
    "        v_i[i] /= norm\n",
    "        v_j[i] /= norm\n",
    "        v_k[i] /= norm\n",
    "    v_i = v_i.reshape(kernel_shape)\n",
    "    v_j = v_j.reshape(kernel_shape)\n",
    "    v_k = v_k.reshape(kernel_shape)\n",
    "\n",
    "    modulus = rng.uniform(low=-s, high=s, size=kernel_shape)\n",
    "    phase = rng.uniform(low=-np.pi, high=np.pi, size=kernel_shape)\n",
    "\n",
    "    weight_r = modulus * np.cos(phase)\n",
    "    weight_i = modulus * v_i * np.sin(phase)\n",
    "    weight_j = modulus * v_j * np.sin(phase)\n",
    "    weight_k = modulus * v_k * np.sin(phase)\n",
    "\n",
    "    return (weight_r, weight_i, weight_j, weight_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2345690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TQUATDE(nn.Module):\n",
    "    def __init__(self, kg, embedding_dim, batch_size, learning_rate, gran, gamma, n_day, gpu=True):\n",
    "        super(TQUATDE, self).__init__()\n",
    "        self.gpu = gpu\n",
    "        self.kg = kg\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.n_day = n_day\n",
    "        self.gran = gran\n",
    "\n",
    "        # Nets\n",
    "        self.emb_E = torch.nn.Embedding(self.kg.n_entity, self.embedding_dim * 4)\n",
    "        self.emb_R = torch.nn.Embedding(self.kg.n_relation*2, self.embedding_dim * 4)\n",
    "        self.emb_R_trans = torch.nn.Embedding(self.kg.n_relation*2, self.embedding_dim * 4)\n",
    "        self.emb_Time = torch.nn.Embedding(n_day, self.embedding_dim * 4)\n",
    "        \n",
    "        # Initialization\n",
    "        self.init_weights()\n",
    "        \n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "            \n",
    "    def init_weights(self):\n",
    "        r, i, j, k = quaternion_init(self.kg.n_entity, self.embedding_dim)\n",
    "        r, i, j, k = torch.from_numpy(r), torch.from_numpy(i), torch.from_numpy(j), torch.from_numpy(k)\n",
    "        vec1 = torch.cat([r, i, j, k], dim=1)\n",
    "        self.emb_E.weight.data = vec1.type_as(self.emb_E.weight.data)\n",
    "\n",
    "        s, x, y, z = quaternion_init(self.kg.n_relation*2, self.embedding_dim)\n",
    "        s, x, y, z = torch.from_numpy(s), torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(z)\n",
    "        vec2 = torch.cat([s, x, y, z], dim=1)\n",
    "        self.emb_R.data = vec2.type_as(self.emb_R.weight.data)\n",
    "\n",
    "        s, x, y, z = quaternion_init(self.kg.n_relation*2, self.embedding_dim)\n",
    "        s, x, y, z = torch.from_numpy(s), torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(z)\n",
    "        vec2 = torch.cat([s, x, y, z], dim=1)\n",
    "        self.emb_R_trans.data = vec2.type_as(self.emb_R_trans.weight.data)\n",
    "\n",
    "        r, i, j, k = quaternion_init(self.n_day, self.embedding_dim)\n",
    "        r, i, j, k = torch.from_numpy(r), torch.from_numpy(i), torch.from_numpy(j), torch.from_numpy(k)\n",
    "        vec1 = torch.cat([r, i, j, k], dim=1)\n",
    "        self.emb_Time.weight.data = vec1.type_as(self.emb_Time.weight.data)\n",
    "        \n",
    "    def _calc(self, h, r):\n",
    "        return hamilton_quat_prod(h, quat_norm(r))\n",
    "    \n",
    "    def _transfer(self, x, x_transfer, r_transfer):\n",
    "        ent_transfer = self._calc(x, x_transfer)\n",
    "        ent_rel_transfer = self._calc(ent_transfer, r_transfer)\n",
    "\n",
    "        return ent_rel_transfer\n",
    "    \n",
    "    def forward(self, X):\n",
    "        h_i, t_i, r_i, d_i = X[:, 0].astype(np.int64), X[:, 1].astype(np.int64), X[:, 2].astype(np.int64), X[:, 3].astype(np.int64)//self.gran\n",
    "\n",
    "        if self.gpu:\n",
    "            h_i = Variable(torch.from_numpy(h_i).cuda())\n",
    "            t_i = Variable(torch.from_numpy(t_i).cuda())\n",
    "            r_i = Variable(torch.from_numpy(r_i).cuda())\n",
    "            d_i = Variable(torch.from_numpy(d_i).cuda())\n",
    "        else:\n",
    "            h_i = Variable(torch.from_numpy(h_i))\n",
    "            t_i = Variable(torch.from_numpy(t_i))\n",
    "            r_i = Variable(torch.from_numpy(r_i))\n",
    "            d_i = Variable(torch.from_numpy(d_i))\n",
    "        h = self.emb_E(h_i)\n",
    "        r = self.emb_R(r_i)\n",
    "        t = self.emb_E(t_i)\n",
    "#         print('h.mean: ',h.mean().item())\n",
    "#         print('r.mean: ',r.mean().item())\n",
    "#         print('t.mean: ',t.mean().item())\n",
    "        # (h, r, t) transfer vector\n",
    "        time_transfer = self.emb_Time(d_i)\n",
    "        r_transfer = self.emb_R_trans(r_i)\n",
    "\n",
    "        h1 = self._transfer(h, time_transfer, r_transfer)\n",
    "        t1 = self._transfer(t, time_transfer, r_transfer)\n",
    "        # multiplication as QuatE\n",
    "        hr = self._calc(h1, r)\n",
    "        # Inner product as QuatE\n",
    "        score = quat_dot_prod(hr, t1)\n",
    "#         print('score.mean: ',score.mean().item())\n",
    "        return score\n",
    "\n",
    "    def normalize_embeddings(self):\n",
    "        self.emb_E_real.weight.data.renorm_(p=2, dim=0, maxnorm=1)\n",
    "        self.emb_E_img.weight.data.renorm_(p=2, dim=0, maxnorm=1)\n",
    "\n",
    "    def log_rank_loss(self, y_pos, y_neg, temp=0):\n",
    "#         print('y_pos.shape: ',y_pos.shape, '| y_neg.shape: ',y_neg.shape)\n",
    "        M = y_pos.size(0)\n",
    "        N = y_neg.size(0)\n",
    "        y_pos = self.gamma-y_pos\n",
    "        y_neg = self.gamma-y_neg\n",
    "        C = int(N / M)\n",
    "        y_neg = y_neg.view(C, -1).transpose(0, 1)\n",
    "        p = F.softmax(temp * y_neg)\n",
    "        loss_pos = torch.sum(F.softplus(-1 * y_pos))\n",
    "        loss_neg = torch.sum(p * F.softplus(y_neg))\n",
    "        loss = (loss_pos + loss_neg) / 2 / M\n",
    "        if self.gpu:\n",
    "            loss = loss.cuda()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def rank_loss(self, y_pos, y_neg):\n",
    "        M = y_pos.size(0)\n",
    "        N = y_neg.size(0)\n",
    "        C = int(N / M)\n",
    "        y_pos = y_pos.repeat(C)\n",
    "        if self.gpu:\n",
    "            target = Variable(torch.from_numpy(-np.ones(N, dtype=np.float32))).cuda()\n",
    "        else:\n",
    "            target = Variable(torch.from_numpy(-np.ones(N, dtype=np.float32))).cpu()\n",
    "        loss = nn.MarginRankingLoss(margin=self.gamma)\n",
    "        loss = loss(y_pos, y_neg, target)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def rank_left(self, X, facts, kg, timedisc, rev_set=0):\n",
    "        rank = []\n",
    "        with torch.no_grad():\n",
    "            if timedisc:\n",
    "                for triple, fact in zip(X, facts):\n",
    "                    X_i = np.ones([self.kg.n_entity, 4])\n",
    "                    Xe_i = np.ones([self.kg.n_entity, 4])\n",
    "                    for i in range(0, self.kg.n_entity):\n",
    "                        X_i[i, 0] = i\n",
    "                        X_i[i, 1] = triple[1]\n",
    "                        X_i[i, 2] = triple[2] if triple[3]>=0 else triple[2]+self.kg.n_relation\n",
    "                        X_i[i, 3] = triple[3] if triple[3]>=0 else triple[4]\n",
    "                        Xe_i[i, 0] = i\n",
    "                        Xe_i[i, 1] = triple[1]\n",
    "                        Xe_i[i, 2] = triple[2]+self.kg.n_relation if triple[4]>=0 else triple[2]\n",
    "                        Xe_i[i, 3] = triple[4] if triple[4]>=0 else triple[3]\n",
    "                    i_score = self.forward(X_i)+self.forward(Xe_i)\n",
    "                    if rev_set>0:\n",
    "                        X_rev = np.ones([self.kg.n_entity,4])\n",
    "                        Xe_rev = np.ones([self.kg.n_entity,4])\n",
    "                        for i in range(0, self.kg.n_entity):\n",
    "                            X_rev[i, 0] = triple[1]\n",
    "                            X_rev[i, 1] = i\n",
    "                            X_rev[i, 2] = triple[2]+self.kg.n_relation//2 if triple[3]>=0 else triple[2]+self.kg.n_relation+self.kg.n_relation//2\n",
    "                            X_rev[i, 3] = triple[3] if triple[3]>=0 else triple[4]\n",
    "                            Xe_rev[i, 0] = triple[1]\n",
    "                            Xe_rev[i, 1] = i\n",
    "                            Xe_rev[i, 2] = triple[2]+self.kg.n_relation//2+self.kg.n_relation if triple[4]>=0 else triple[2]+self.kg.n_relation//2\n",
    "                            Xe_rev[i, 3] = triple[4] if triple[4]>=0 else triple[3]\n",
    "                        i_score = i_score + self.forward(X_rev).view(-1)+self.forward(Xe_rev).view(-1)\n",
    "                    if self.gpu:\n",
    "                        i_score = i_score.cuda()\n",
    "        \n",
    "                    filter_out = kg.to_skip_final['lhs'][(fact[1], fact[2],fact[3], fact[4])]                            \n",
    "                    target = i_score[int(triple[0])].clone()\n",
    "                    i_score[filter_out]=1e6 \n",
    "                    rank_triple=torch.sum((i_score < target).float()).cpu().item()+1\n",
    "                    rank.append(rank_triple)\n",
    "                        \n",
    "\n",
    "            else:\n",
    "                for triple, fact in zip(X, facts):\n",
    "                    X_i = np.ones([self.kg.n_entity, 4])\n",
    "                    for i in range(0, self.kg.n_entity):\n",
    "                        X_i[i, 0] = i\n",
    "                        X_i[i, 1] = triple[1]\n",
    "                        X_i[i, 2] = triple[2]\n",
    "                        X_i[i, 3] = triple[3]\n",
    "                    i_score = self.forward(X_i)\n",
    "                    if rev_set>0:\n",
    "                        X_rev = np.ones([self.kg.n_entity,4])\n",
    "                        for i in range(0, self.kg.n_entity):\n",
    "                            X_rev[i, 0] = triple[1]\n",
    "                            X_rev[i, 1] = i\n",
    "                            X_rev[i, 2] = triple[2]+self.kg.n_relation//2\n",
    "                            X_rev[i, 3] = triple[3]\n",
    "                        i_score = i_score + self.forward(X_rev).view(-1)\n",
    "                    if self.gpu:\n",
    "                        i_score = i_score.cuda()\n",
    "        \n",
    "                    filter_out = kg.to_skip_final['lhs'][(fact[1], fact[2],fact[3], fact[4])]                            \n",
    "                    target = i_score[int(triple[0])].clone()\n",
    "                    i_score[filter_out]=1e6 \n",
    "                    rank_triple=torch.sum((i_score < target).float()).cpu().item()+1\n",
    "                    rank.append(rank_triple)\n",
    "\n",
    "        return rank\n",
    "\n",
    "    def rank_right(self, X, facts, kg, timedisc, rev_set=0):\n",
    "        rank = []\n",
    "        with torch.no_grad():\n",
    "            if timedisc:\n",
    "                for triple, fact in zip(X, facts):\n",
    "                    X_i = np.ones([self.kg.n_entity, 4])\n",
    "                    Xe_i = np.ones([self.kg.n_entity, 4])\n",
    "                    for i in range(0, self.kg.n_entity):\n",
    "                        X_i[i, 0] = triple[0]\n",
    "                        X_i[i, 1] = i\n",
    "                        X_i[i, 2] = triple[2] if triple[3]>=0 else triple[2]+self.kg.n_relation\n",
    "                        X_i[i, 3] = triple[3] if triple[3]>=0 else triple[4]\n",
    "                        Xe_i[i, 0] = triple[0] \n",
    "                        Xe_i[i, 1] = i\n",
    "                        Xe_i[i, 2] = triple[2]+self.kg.n_relation if triple[4]>=0 else triple[2]\n",
    "                        Xe_i[i, 3] = triple[4] if triple[4]>=0 else triple[3]\n",
    "                    i_score = self.forward(X_i)+self.forward(Xe_i)\n",
    "                    if rev_set>0: \n",
    "                        X_rev = np.ones([self.kg.n_entity,4])\n",
    "                        Xe_rev = np.ones([self.kg.n_entity,4])\n",
    "                        for i in range(0, self.kg.n_entity):\n",
    "                            X_rev[i, 0] = i\n",
    "                            X_rev[i, 1] = triple[0]\n",
    "                            X_rev[i, 2] = triple[2]+self.kg.n_relation//2 if triple[3]>=0 else triple[2]+self.kg.n_relation+self.kg.n_relation//2\n",
    "                            X_rev[i, 3] = triple[3] if triple[3]>=0 else triple[4]\n",
    "                            Xe_rev[i, 0] = i\n",
    "                            Xe_rev[i, 1] = triple[0]\n",
    "                            Xe_rev[i, 2] = triple[2]+self.kg.n_relation//2+self.kg.n_relation if triple[4]>=0 else triple[2]+self.kg.n_relation//2\n",
    "                            Xe_rev[i, 3] = triple[4] if triple[4]>=0 else triple[3]\n",
    "                        i_score = i_score + self.forward(X_rev).view(-1)+ self.forward(Xe_rev).view(-1)\n",
    "                    if self.gpu:\n",
    "                        i_score = i_score.cuda()\n",
    "        \n",
    "                    filter_out = kg.to_skip_final['rhs'][(fact[0], fact[2],fact[3], fact[4])]       \n",
    "                    target = i_score[int(triple[1])].clone()\n",
    "                    i_score[filter_out]=1e6\n",
    "                    rank_triple=torch.sum((i_score < target).float()).cpu().item()+1\n",
    "        \n",
    "                    rank.append(rank_triple)\n",
    "                    \n",
    "            else:\n",
    "                for triple, fact in zip(X, facts):\n",
    "                    X_i = np.ones([self.kg.n_entity, 4])\n",
    "                    for i in range(0, self.kg.n_entity):\n",
    "                        X_i[i, 0] = triple[0]\n",
    "                        X_i[i, 1] = i\n",
    "                        X_i[i, 2] = triple[2]\n",
    "                        X_i[i, 3] = triple[3]\n",
    "                    i_score = self.forward(X_i)\n",
    "                    if rev_set>0: \n",
    "                        X_rev = np.ones([self.kg.n_entity,4])\n",
    "                        for i in range(0, self.kg.n_entity):\n",
    "                            X_rev[i, 0] = i\n",
    "                            X_rev[i, 1] = triple[0]\n",
    "                            X_rev[i, 2] = triple[2]+self.kg.n_relation//2\n",
    "                            X_rev[i, 3] = triple[3]\n",
    "                        i_score = i_score + self.forward(X_rev).view(-1)\n",
    "                    if self.gpu:\n",
    "                        i_score = i_score.cuda()\n",
    "        \n",
    "                    filter_out = kg.to_skip_final['rhs'][(fact[0], fact[2],fact[3], fact[4])]       \n",
    "                    target = i_score[int(triple[1])].clone()\n",
    "                    i_score[filter_out]=1e6\n",
    "                    rank_triple=torch.sum((i_score < target).float()).cpu().item()+1\n",
    "        \n",
    "                    rank.append(rank_triple)\n",
    "\n",
    "        return rank\n",
    "\n",
    "    def timepred(self, X):\n",
    "        rank = []\n",
    "        with torch.no_grad():\n",
    "            for triple in X:\n",
    "                X_i = np.ones([self.kg.n_day, len(triple)])\n",
    "                for i in range(self.kg.n_day):\n",
    "                    X_i[i, 0] = triple[0]\n",
    "                    X_i[i, 1] = triple[1]\n",
    "                    X_i[i, 2] = triple[2]\n",
    "                    X_i[i, 3:] = self.kg.time_dict[i]\n",
    "                i_score = self.forward(X_i)\n",
    "                if self.gpu:\n",
    "                    i_score = i_score.cuda()\n",
    "    \n",
    "                target = i_score[triple[3]]           \n",
    "                rank_triple=torch.sum((i_score < target).float()).cpu().item()+1\n",
    "                rank.append(rank_triple)\n",
    "\n",
    "        return rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d977f",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91ff5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_rank(rank):\n",
    "    m_r = 0\n",
    "    N = len(rank)\n",
    "    for i in rank:\n",
    "        m_r = m_r + i / N\n",
    "\n",
    "    return m_r\n",
    "\n",
    "\n",
    "def mrr(rank):\n",
    "    mrr = 0\n",
    "    N = len(rank)\n",
    "    for i in rank:\n",
    "        mrr = mrr + 1 / i / N\n",
    "\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def hit_N(rank, N):\n",
    "    hit = 0\n",
    "    for i in rank:\n",
    "        if i <= N:\n",
    "            hit = hit + 1\n",
    "\n",
    "    hit = hit / len(rank)\n",
    "\n",
    "    return hit\n",
    "\n",
    "def get_minibatches(X, mb_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate minibatches from given dataset for training.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    X: np.array of M x 3\n",
    "        Contains the triplets from dataset. The entities and relations are\n",
    "        translated to its unique indices.\n",
    "\n",
    "    mb_size: int\n",
    "        Size of each minibatch.\n",
    "\n",
    "    shuffle: bool, default True\n",
    "        Whether to shuffle the dataset before dividing it into minibatches.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    mb_iter: generator\n",
    "        Example usage:\n",
    "        --------------\n",
    "        mb_iter = get_minibatches(X_train, mb_size)\n",
    "        for X_mb in mb_iter:\n",
    "            // do something with X_mb, the minibatch\n",
    "    \"\"\"\n",
    "    X_shuff = X.copy()\n",
    "    if shuffle:\n",
    "        X_shuff = skshuffle(X_shuff)\n",
    "\n",
    "    for i in range(0, X_shuff.shape[0], mb_size):\n",
    "        yield X_shuff[i:i + mb_size]\n",
    "\n",
    "\n",
    "def sample_negatives(X, C, kg):\n",
    "    \"\"\"\n",
    "    Perform negative sampling by corrupting head or tail of each triplets in\n",
    "    dataset.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    X: int matrix of M x 3, where M is the (mini)batch size\n",
    "        First column contains index of head entities.\n",
    "        Second column contains index of relationships.\n",
    "        Third column contains index of tail entities.\n",
    "\n",
    "    n_e: int\n",
    "        Number of entities in dataset.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_corr: int matrix of M x 3, where M is the (mini)batch size\n",
    "        Similar to input param X, but at each column, either first or third col\n",
    "        is subtituted with random entity.\n",
    "        \n",
    "    \"\"\"\n",
    "    M = X.shape[0]\n",
    "    X_corr = X\n",
    "    for i in range(C-1):\n",
    "        X_corr = np.concatenate((X_corr,X),0)\n",
    "    X_corr[:int(M*C/2),0]=torch.randint(kg.n_entity,[int(M*C/2)])        \n",
    "    X_corr[int(M*C/2):,1]=torch.randint(kg.n_entity,[int(M*C/2)]) \n",
    "\n",
    "    return X_corr\n",
    "\n",
    "\n",
    "def sample_negatives_t(X, C, n_day):\n",
    "    \"\"\"\n",
    "    Perform negative sampling by corrupting head or tail of each triplets in\n",
    "    dataset.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    X: int matrix of M x 4, where M is the (mini)batch size\n",
    "        First column contains index of head entities.\n",
    "        Second column contains index of relationships.\n",
    "        Third column contains index of tail entities.\n",
    "\n",
    "    n_e: int\n",
    "        Number of entities in dataset.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_corr: int matrix of M x 4, where M is the (mini)batch size\n",
    "        Similar to input param X, but at each column, either first or third col\n",
    "        is subtituted with random entity.\n",
    "    \"\"\"\n",
    "    M = X.shape[0]\n",
    "    X_corr = X\n",
    "    for i in range(C-1):\n",
    "        X_corr = torch.cat((X_corr,X),0)\n",
    "    X_corr[:,3]=torch.randint(n_day,[int(M*C)])        \n",
    "\n",
    "\n",
    "    return X_corr\n",
    "\n",
    "\n",
    "\n",
    "def train(task ='LinkPrediction',\n",
    "          modelname='ATISE',\n",
    "          data_dir='yago',\n",
    "          dim=500,\n",
    "          batch=512,\n",
    "          lr=0.1,\n",
    "          max_epoch=5000,\n",
    "          min_epoch=250,\n",
    "          gamma=1,\n",
    "          negsample_num=10,\n",
    "          timedisc = 0,\n",
    "          lossname = 'logloss',\n",
    "          cmin = 0.001,\n",
    "          cuda_able = True,\n",
    "          rev_set = 1,\n",
    "          temp = 0.5,\n",
    "          gran = 7,\n",
    "          count = 300\n",
    "          ):\n",
    "\n",
    "    randseed = 9999\n",
    "    np.random.seed(randseed)\n",
    "    torch.manual_seed(randseed)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Data Loading\n",
    "    \"\"\"\n",
    "    if data_dir == 'yago' or data_dir == 'wikidata':\n",
    "        kg = KnowledgeGraphYG(data_dir=data_dir, count = count,rev_set = rev_set)\n",
    "        n_day = kg.n_time\n",
    "        min_epoch=50\n",
    "    elif data_dir=='icews14':\n",
    "        n_day = 365\n",
    "        kg = KnowledgeGraph(data_dir=data_dir,gran=gran,rev_set = rev_set)\n",
    "    elif data_dir == 'icews05-15':\n",
    "        n_day = 4017\n",
    "        kg = KnowledgeGraph(data_dir=data_dir,gran=gran,rev_set = rev_set)      \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Create a model\n",
    "    \"\"\"\n",
    "\n",
    "    if modelname== 'TQUATDE':\n",
    "        model = TQUATDE(kg, embedding_dim=dim, batch_size=batch, learning_rate=lr, gamma=gamma, gran=gran, n_day=kg.n_time,gpu=cuda_able)\n",
    "    \n",
    "    solver = torch.optim.Adam(model.parameters(), model.learning_rate)\n",
    "    optimizer = 'Adam'\n",
    "    \n",
    "    if timedisc == 0 or timedisc ==2:\n",
    "        train_pos = np.array(kg.training_triples)\n",
    "        validation_pos = np.array(kg.validation_triples)\n",
    "        test_pos = np.array(kg.test_triples)\n",
    "        \n",
    "    elif timedisc == 1:\n",
    "        train_pos = []\n",
    "        validation_pos = []\n",
    "        test_pos = []\n",
    "        for fact in kg.training_triples:\n",
    "            for time_index in range(fact[3],fact[4]+1):\n",
    "                train_pos.append([fact[0], fact[1], fact[2], time_index])\n",
    "        train_pos = np.array(train_pos)\n",
    "       # for fact in kg.validation_triples:\n",
    "       #     for time_index in range(fact[3],fact[4]+1):\n",
    "       #         validation_pos.append([fact[0], fact[1], fact[2], time_index])\n",
    "        validation_pos = np.array(kg.validation_triples)\n",
    "       # for fact in kg.test_triples:\n",
    "       #     for time_index in range(fact[3],fact[4]+1):\n",
    "       #         test_pos.append([fact[0], fact[1], fact[2], time_index])\n",
    "       # test_pos = np.array(test_pos)        \n",
    "        test_pos = np.array(kg.test_triples)\n",
    "\n",
    "        \n",
    "    losses = []\n",
    "    mrr_std = 0\n",
    "    C = negsample_num\n",
    "    patience = 0\n",
    "    path = os.path.join(data_dir,modelname,'timediscrete{:.0f}/dim{:.0f}/lr{:.4f}/neg_num{:.0f}/{:.0f}day/gamma{:.0f}/cmin{:.4f}'\n",
    "                        .format(timedisc,dim,lr,negsample_num,gran,gamma,cmin))\n",
    "    if timedisc: path = os.path.join(path,'{:.0f}count'.format(count))\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        print('path existed')\n",
    "        return\n",
    "    \n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    Training Process\n",
    "    \"\"\"\n",
    "    for epoch in range(max_epoch):\n",
    "        print('Epoch-{}'.format(epoch + 1))\n",
    "        print('————————————————')\n",
    "        it = 0\n",
    "        train_triple = list(get_minibatches(train_pos, batch, shuffle=True))\n",
    "        for iter_triple in train_triple:\n",
    "            if iter_triple.shape[0] < batch:\n",
    "                break\n",
    "            start = time.time()\n",
    "            if task=='TimePrediction':\n",
    "                iter_neg = sample_negatives_t(iter_triple, C, n_day)\n",
    "            else:\n",
    "                iter_neg = sample_negatives(iter_triple, C, kg)\n",
    "            if timedisc == 2:\n",
    "                end_miss = np.where(iter_triple[:,4:5]<0)[0]\n",
    "                start_miss = np.where(iter_triple[:,3:4]<0)[0]\n",
    "                neg_end_miss = np.where(iter_neg[:,4:5]<0)[0]\n",
    "                neg_start_miss = np.where(iter_neg[:,3:4]<0)[0]\n",
    "                \n",
    "                \n",
    "                iter_triple_e = np.delete(iter_triple,3,1)\n",
    "                iter_triple = np.delete(iter_triple,4,1)\n",
    "                \n",
    "                iter_triple_e[:,2:3] += kg.n_relation\n",
    "\n",
    "                iter_triple_e[end_miss,:]=iter_triple[end_miss,:]\n",
    "                iter_triple[start_miss,:]=iter_triple_e[start_miss,:]\n",
    "                \n",
    "                \n",
    "                iter_neg_e = np.delete(iter_neg,3,1)\n",
    "                iter_neg = np.delete(iter_neg,4,1)\n",
    "                \n",
    "                iter_neg_e[:,2:3] += kg.n_relation\n",
    "                \n",
    "                iter_neg_e[neg_end_miss,:]=iter_neg[neg_end_miss,:]\n",
    "                iter_neg[neg_start_miss,:]=iter_neg_e[neg_start_miss,:]\n",
    "                \n",
    "\n",
    "            pos_score = model.forward(iter_triple)\n",
    "            neg_score = model.forward(iter_neg)\n",
    "#             print('pos_score.mean: ',pos_score.mean(), '| neg_score.mean: ',neg_score.mean())\n",
    "            if timedisc ==2:\n",
    "                pos_score += model.forward(iter_triple_e)\n",
    "                neg_score += model.forward(iter_neg_e)\n",
    "                \n",
    "            if lossname == 'logloss':\n",
    "                loss = model.log_rank_loss(pos_score, neg_score,temp=temp)\n",
    "            else:\n",
    "                loss = model.rank_loss(pos_score, neg_score)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            solver.zero_grad()\n",
    "            loss.backward()\n",
    "            solver.step()\n",
    "\n",
    "\n",
    "#             if lossname == 'marginloss':\n",
    "#                 model.normalize_embeddings()\n",
    "#             if modelname == 'ATISE':\n",
    "#                 model.regularization_embeddings()\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            if it % 33 == 0:\n",
    "                print('Iter-{}; loss: {:.4f};time per batch:{:.4f}s'.format(it, loss.item(), end - start))\n",
    "\n",
    "            it += 1\n",
    "\n",
    "        \"\"\"\n",
    "        Evaluation for Link Prediction\n",
    "        \"\"\"\n",
    "\n",
    "        if ((epoch+1)//min_epoch>epoch//min_epoch and epoch < max_epoch) :\n",
    "            if task == 'LinkPrediction':\n",
    "                rank = model.rank_left(validation_pos,kg.validation_facts,kg,timedisc,rev_set=rev_set)\n",
    "                rank_right = model.rank_right(validation_pos,kg.validation_facts,kg,timedisc,rev_set=rev_set)\n",
    "                rank = rank + rank_right\n",
    "            else:\n",
    "                rank = model.timepred(validation_pos)\n",
    "\n",
    "            m_rank = mean_rank(rank)\n",
    "            mean_rr = mrr(rank)\n",
    "            hit_1 = hit_N(rank, 1)\n",
    "            hit_3 = hit_N(rank, 3)\n",
    "            hit_5 = hit_N(rank, 5)\n",
    "            hit_10 = hit_N(rank, 10)\n",
    "            print('validation results:')\n",
    "            print('Mean Rank: {:.0f}'.format(m_rank))\n",
    "            print('Mean RR: {:.4f}'.format(mean_rr))\n",
    "            print('Hit@1: {:.4f}'.format(hit_1))\n",
    "            print('Hit@3: {:.4f}'.format(hit_3))\n",
    "            print('Hit@5: {:.4f}'.format(hit_5))\n",
    "            print('Hit@10: {:.4f}'.format(hit_10))\n",
    "            f = open(os.path.join(path, 'result{:.0f}.txt'.format(epoch)), 'w')\n",
    "            f.write('Mean Rank: {:.0f}\\n'.format(m_rank))\n",
    "            f.write('Mean RR: {:.4f}\\n'.format(mean_rr))\n",
    "            f.write('Hit@1: {:.4f}\\n'.format(hit_1))\n",
    "            f.write('Hit@3: {:.4f}\\n'.format(hit_3))\n",
    "            f.write('Hit@5: {:.4f}\\n'.format(hit_5))\n",
    "            f.write('Hit@10: {:.4f}\\n'.format(hit_10))\n",
    "            for loss in losses:\n",
    "                f.write(str(loss))\n",
    "                f.write('\\n')\n",
    "            f.close()\n",
    "            if mean_rr < mrr_std and patience<3:\n",
    "                patience+=1\n",
    "            elif (mean_rr < mrr_std and patience>=3) or epoch==max_epoch-1:\n",
    "                if epoch == max_epoch-1:\n",
    "                    torch.save(model.state_dict(), os.path.join(path, 'params.pkl'))\n",
    "                model.load_state_dict(torch.load(os.path.join(path,'params.pkl')))\n",
    "                if task == 'LinkPrediction':\n",
    "                    rank = model.rank_left(test_pos,kg.test_facts,kg,timedisc,rev_set=rev_set)\n",
    "                    rank_right = model.rank_right(test_pos,kg.test_facts,kg,timedisc,rev_set=rev_set)\n",
    "                    rank = rank + rank_right\n",
    "                else:\n",
    "                    rank = model.timepred(test_pos)\n",
    "\n",
    "\n",
    "                m_rank = mean_rank(rank)\n",
    "                mean_rr = mrr(rank)\n",
    "                hit_1 = hit_N(rank, 1)\n",
    "                hit_3 = hit_N(rank, 3)\n",
    "                hit_5 = hit_N(rank, 5)\n",
    "                hit_10 = hit_N(rank, 10)\n",
    "                print('test result:')\n",
    "                print('Mean Rank: {:.0f}'.format(m_rank))\n",
    "                print('Mean RR: {:.4f}'.format(mean_rr))\n",
    "                print('Hit@1: {:.4f}'.format(hit_1))\n",
    "                print('Hit@3: {:.4f}'.format(hit_3))\n",
    "                print('Hit@5: {:.4f}'.format(hit_5))\n",
    "                print('Hit@10: {:.4f}'.format(hit_10))\n",
    "                if epoch == max_epoch-1:\n",
    "                    f = open(os.path.join(path, 'test_result{:.0f}.txt'.format(epoch)), 'w')\n",
    "                else:\n",
    "                    f = open(os.path.join(path, 'test_result{:.0f}.txt'.format(epoch)), 'w')\n",
    "                f.write('Mean Rank: {:.0f}\\n'.format(m_rank))\n",
    "                f.write('Mean RR: {:.4f}\\n'.format(mean_rr))\n",
    "                f.write('Hit@1: {:.4f}\\n'.format(hit_1))\n",
    "                f.write('Hit@3: {:.4f}\\n'.format(hit_3))\n",
    "                f.write('Hit@5: {:.4f}\\n'.format(hit_5))\n",
    "                f.write('Hit@10: {:.4f}\\n'.format(hit_10))\n",
    "                for loss in losses:\n",
    "                    f.write(str(loss))\n",
    "                    f.write('\\n')\n",
    "                f.close()\n",
    "                break\n",
    "            if mean_rr>=mrr_std:\n",
    "                \n",
    "                torch.save(model.state_dict(), os.path.join(path, 'params.pkl'))\n",
    "                mrr_std = mean_rr\n",
    "                patience = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cedfa9",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e68da3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad37c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Params()\n",
    "args.task = 'LinkPrediction'\n",
    "args.model = 'TQUATDE'\n",
    "args.dataset = 'icews14'\n",
    "args.max_epoch = 5000\n",
    "args.dim = 200\n",
    "args.batch = 512\n",
    "args.lr = 0.001\n",
    "args.gamma = 10\n",
    "# args.eta = 10\n",
    "args.timedisc = 0\n",
    "args.cuda = True\n",
    "args.loss = 'logloss'\n",
    "# rank_loss logloss\n",
    "args.cmin = 0.005\n",
    "args.gran = 1\n",
    "args.thre = 1\n",
    "args.negsample_num = 30\n",
    "args.temp = 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e08f35e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print(args)\n",
    "    train(task = args.task,\n",
    "          modelname = args.model,\n",
    "          data_dir = args.dataset,\n",
    "          dim = args.dim,\n",
    "          batch = args.batch,\n",
    "          min_epoch = 10,\n",
    "          negsample_num = args.negsample_num,\n",
    "          temp = args.temp,\n",
    "          lr = args.lr,\n",
    "          max_epoch = args.max_epoch,\n",
    "          gamma = args.gamma,\n",
    "          lossname = args.loss,\n",
    "#           negsample_num = args.eta,\n",
    "          timedisc = args.timedisc,\n",
    "          cuda_able = args.cuda,\n",
    "          cmin = args.cmin,\n",
    "          gran = args.gran,\n",
    "          count = args.thre\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71b6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Params object at 0x7f8eabeec460>\n",
      "-----Loading entity dict-----\n",
      "#entity: 7129\n",
      "-----Loading relation dict-----\n",
      "#relation: 460\n",
      "-----Loading training triples-----\n",
      "#training triple: 145652\n",
      "-----Loading validation triples-----\n",
      "#validation triple: 8941\n",
      "-----Loading test triples------\n",
      "#test triple: 8963\n",
      "creating filtering lists\n",
      "data preprocess completed\n",
      "Epoch-1\n",
      "————————————————\n",
      "Iter-0; loss: 5.0000;time per batch:0.0489s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126092/3259640808.py:99: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.softmax(temp * y_neg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-33; loss: 5.0000;time per batch:0.1137s\n",
      "Iter-66; loss: 4.9986;time per batch:0.0463s\n",
      "Iter-99; loss: 4.9901;time per batch:0.1083s\n",
      "Iter-132; loss: 4.9648;time per batch:0.0451s\n",
      "Iter-165; loss: 4.8958;time per batch:0.1082s\n",
      "Iter-198; loss: 4.7648;time per batch:0.0451s\n",
      "Iter-231; loss: 4.5160;time per batch:0.1078s\n",
      "Iter-264; loss: 4.2039;time per batch:0.0519s\n",
      "Epoch-2\n",
      "————————————————\n",
      "Iter-0; loss: 3.9156;time per batch:0.0456s\n",
      "Iter-33; loss: 3.3675;time per batch:0.1083s\n",
      "Iter-66; loss: 2.6393;time per batch:0.0449s\n",
      "Iter-99; loss: 2.0277;time per batch:0.1089s\n",
      "Iter-132; loss: 1.4450;time per batch:0.0449s\n",
      "Iter-165; loss: 1.1632;time per batch:0.1083s\n",
      "Iter-198; loss: 0.9638;time per batch:0.0449s\n",
      "Iter-231; loss: 0.9231;time per batch:0.1072s\n",
      "Iter-264; loss: 0.7415;time per batch:0.0449s\n",
      "Epoch-3\n",
      "————————————————\n",
      "Iter-0; loss: 0.6828;time per batch:0.0447s\n",
      "Iter-33; loss: 0.6645;time per batch:0.1083s\n",
      "Iter-66; loss: 0.6266;time per batch:0.0445s\n",
      "Iter-99; loss: 0.6289;time per batch:0.1084s\n",
      "Iter-132; loss: 0.5877;time per batch:0.0449s\n",
      "Iter-165; loss: 0.5307;time per batch:0.1092s\n",
      "Iter-198; loss: 0.5116;time per batch:0.0454s\n",
      "Iter-231; loss: 0.5024;time per batch:0.1081s\n",
      "Iter-264; loss: 0.4859;time per batch:0.0447s\n",
      "Epoch-4\n",
      "————————————————\n",
      "Iter-0; loss: 0.4750;time per batch:0.0448s\n",
      "Iter-33; loss: 0.3841;time per batch:0.1085s\n",
      "Iter-66; loss: 0.4278;time per batch:0.0449s\n",
      "Iter-99; loss: 0.4348;time per batch:0.1077s\n",
      "Iter-132; loss: 0.3959;time per batch:0.0450s\n",
      "Iter-165; loss: 0.3623;time per batch:0.1084s\n",
      "Iter-198; loss: 0.3518;time per batch:0.0515s\n",
      "Iter-231; loss: 0.3888;time per batch:0.0510s\n",
      "Iter-264; loss: 0.3871;time per batch:0.0517s\n",
      "Epoch-5\n",
      "————————————————\n",
      "Iter-0; loss: 0.3241;time per batch:0.0479s\n",
      "Iter-33; loss: 0.2885;time per batch:0.0480s\n",
      "Iter-66; loss: 0.3048;time per batch:0.0993s\n",
      "Iter-99; loss: 0.2985;time per batch:0.1099s\n",
      "Iter-132; loss: 0.2956;time per batch:0.1089s\n",
      "Iter-165; loss: 0.2796;time per batch:0.0964s\n",
      "Iter-198; loss: 0.2953;time per batch:0.1081s\n",
      "Iter-231; loss: 0.2886;time per batch:0.0959s\n",
      "Iter-264; loss: 0.2768;time per batch:0.1080s\n",
      "Epoch-6\n",
      "————————————————\n",
      "Iter-0; loss: 0.2516;time per batch:0.1011s\n",
      "Iter-33; loss: 0.2285;time per batch:0.1081s\n",
      "Iter-66; loss: 0.2275;time per batch:0.1098s\n",
      "Iter-99; loss: 0.2349;time per batch:0.0980s\n",
      "Iter-132; loss: 0.2308;time per batch:0.1082s\n",
      "Iter-165; loss: 0.2166;time per batch:0.0550s\n",
      "Iter-198; loss: 0.2253;time per batch:0.1094s\n",
      "Iter-231; loss: 0.2233;time per batch:0.0446s\n",
      "Iter-264; loss: 0.2084;time per batch:0.1082s\n",
      "Epoch-7\n",
      "————————————————\n",
      "Iter-0; loss: 0.1811;time per batch:0.1128s\n",
      "Iter-33; loss: 0.1924;time per batch:0.1089s\n",
      "Iter-66; loss: 0.2094;time per batch:0.1069s\n",
      "Iter-99; loss: 0.1778;time per batch:0.0840s\n",
      "Iter-132; loss: 0.1723;time per batch:0.1092s\n",
      "Iter-165; loss: 0.1795;time per batch:0.0449s\n",
      "Iter-198; loss: 0.1711;time per batch:0.1081s\n",
      "Iter-231; loss: 0.1650;time per batch:0.0453s\n",
      "Iter-264; loss: 0.1658;time per batch:0.1080s\n",
      "Epoch-8\n",
      "————————————————\n",
      "Iter-0; loss: 0.1504;time per batch:0.1127s\n",
      "Iter-33; loss: 0.1573;time per batch:0.0448s\n",
      "Iter-66; loss: 0.1637;time per batch:0.1082s\n",
      "Iter-99; loss: 0.1599;time per batch:0.0454s\n",
      "Iter-132; loss: 0.1441;time per batch:0.1082s\n",
      "Iter-165; loss: 0.1570;time per batch:0.0469s\n",
      "Iter-198; loss: 0.1582;time per batch:0.1092s\n",
      "Iter-231; loss: 0.1374;time per batch:0.0447s\n",
      "Iter-264; loss: 0.1483;time per batch:0.0456s\n",
      "Epoch-9\n",
      "————————————————\n",
      "Iter-0; loss: 0.1429;time per batch:0.0448s\n",
      "Iter-33; loss: 0.1441;time per batch:0.1081s\n",
      "Iter-66; loss: 0.1301;time per batch:0.0448s\n",
      "Iter-99; loss: 0.1475;time per batch:0.1085s\n",
      "Iter-132; loss: 0.1302;time per batch:0.0452s\n",
      "Iter-165; loss: 0.1349;time per batch:0.1082s\n",
      "Iter-198; loss: 0.1393;time per batch:0.0549s\n",
      "Iter-231; loss: 0.1426;time per batch:0.1089s\n",
      "Iter-264; loss: 0.1256;time per batch:0.1088s\n",
      "Epoch-10\n",
      "————————————————\n",
      "Iter-0; loss: 0.1246;time per batch:0.0543s\n",
      "Iter-33; loss: 0.1293;time per batch:0.1087s\n",
      "Iter-66; loss: 0.1275;time per batch:0.1079s\n",
      "Iter-99; loss: 0.1076;time per batch:0.0986s\n",
      "Iter-132; loss: 0.1123;time per batch:0.1079s\n",
      "Iter-165; loss: 0.1344;time per batch:0.0448s\n",
      "Iter-198; loss: 0.1177;time per batch:0.1082s\n",
      "Iter-231; loss: 0.1379;time per batch:0.0451s\n",
      "Iter-264; loss: 0.1199;time per batch:0.1104s\n",
      "validation results:\n",
      "Mean Rank: 520\n",
      "Mean RR: 0.3990\n",
      "Hit@1: 0.3024\n",
      "Hit@3: 0.4491\n",
      "Hit@5: 0.5089\n",
      "Hit@10: 0.5841\n",
      "Epoch-11\n",
      "————————————————\n",
      "Iter-0; loss: 0.1069;time per batch:0.0450s\n",
      "Iter-33; loss: 0.0921;time per batch:0.1086s\n",
      "Iter-66; loss: 0.1157;time per batch:0.0451s\n",
      "Iter-99; loss: 0.0984;time per batch:0.1065s\n",
      "Iter-132; loss: 0.0989;time per batch:0.0479s\n",
      "Iter-165; loss: 0.1015;time per batch:0.1083s\n",
      "Iter-198; loss: 0.0940;time per batch:0.0451s\n",
      "Iter-231; loss: 0.1137;time per batch:0.1082s\n",
      "Iter-264; loss: 0.1049;time per batch:0.0457s\n",
      "Epoch-12\n",
      "————————————————\n",
      "Iter-0; loss: 0.0873;time per batch:0.0453s\n",
      "Iter-33; loss: 0.1096;time per batch:0.1080s\n",
      "Iter-66; loss: 0.0920;time per batch:0.0453s\n",
      "Iter-99; loss: 0.0840;time per batch:0.1099s\n",
      "Iter-132; loss: 0.1278;time per batch:0.0450s\n",
      "Iter-165; loss: 0.0865;time per batch:0.1080s\n",
      "Iter-198; loss: 0.1061;time per batch:0.1080s\n",
      "Iter-231; loss: 0.0936;time per batch:0.0454s\n",
      "Iter-264; loss: 0.1155;time per batch:0.1080s\n",
      "Epoch-13\n",
      "————————————————\n",
      "Iter-0; loss: 0.0850;time per batch:0.1124s\n",
      "Iter-33; loss: 0.0914;time per batch:0.0463s\n",
      "Iter-66; loss: 0.0912;time per batch:0.1078s\n",
      "Iter-99; loss: 0.1090;time per batch:0.0453s\n",
      "Iter-132; loss: 0.0833;time per batch:0.1081s\n",
      "Iter-165; loss: 0.0880;time per batch:0.0449s\n",
      "Iter-198; loss: 0.0964;time per batch:0.1081s\n",
      "Iter-231; loss: 0.0961;time per batch:0.0463s\n",
      "Iter-264; loss: 0.0963;time per batch:0.1094s\n",
      "Epoch-14\n",
      "————————————————\n",
      "Iter-0; loss: 0.0883;time per batch:0.1133s\n",
      "Iter-33; loss: 0.0865;time per batch:0.0451s\n",
      "Iter-66; loss: 0.0937;time per batch:0.1080s\n",
      "Iter-99; loss: 0.0998;time per batch:0.0452s\n",
      "Iter-132; loss: 0.0856;time per batch:0.1081s\n",
      "Iter-165; loss: 0.0907;time per batch:0.0451s\n",
      "Iter-198; loss: 0.1057;time per batch:0.1084s\n",
      "Iter-231; loss: 0.0837;time per batch:0.0454s\n",
      "Iter-264; loss: 0.0775;time per batch:0.1097s\n",
      "Epoch-15\n",
      "————————————————\n",
      "Iter-0; loss: 0.0912;time per batch:0.1121s\n",
      "Iter-33; loss: 0.0904;time per batch:0.0451s\n",
      "Iter-66; loss: 0.0865;time per batch:0.1082s\n",
      "Iter-99; loss: 0.0700;time per batch:0.0451s\n",
      "Iter-132; loss: 0.0795;time per batch:0.1081s\n",
      "Iter-165; loss: 0.0913;time per batch:0.0451s\n",
      "Iter-198; loss: 0.0864;time per batch:0.1081s\n",
      "Iter-231; loss: 0.0870;time per batch:0.0450s\n",
      "Iter-264; loss: 0.0938;time per batch:0.1080s\n",
      "Epoch-16\n",
      "————————————————\n",
      "Iter-0; loss: 0.0641;time per batch:0.1111s\n",
      "Iter-33; loss: 0.0963;time per batch:0.0862s\n",
      "Iter-66; loss: 0.0687;time per batch:0.1082s\n",
      "Iter-99; loss: 0.0775;time per batch:0.0455s\n",
      "Iter-132; loss: 0.0847;time per batch:0.1083s\n",
      "Iter-165; loss: 0.0717;time per batch:0.0451s\n",
      "Iter-198; loss: 0.0835;time per batch:0.1078s\n",
      "Iter-231; loss: 0.0925;time per batch:0.0463s\n",
      "Iter-264; loss: 0.0818;time per batch:0.1091s\n",
      "Epoch-17\n",
      "————————————————\n",
      "Iter-0; loss: 0.0906;time per batch:0.1130s\n",
      "Iter-33; loss: 0.0719;time per batch:0.1080s\n",
      "Iter-66; loss: 0.0709;time per batch:0.0480s\n",
      "Iter-99; loss: 0.0717;time per batch:0.0526s\n",
      "Iter-132; loss: 0.0796;time per batch:0.0510s\n",
      "Iter-165; loss: 0.0868;time per batch:0.0513s\n",
      "Iter-198; loss: 0.0737;time per batch:0.0509s\n",
      "Iter-231; loss: 0.0801;time per batch:0.0449s\n",
      "Iter-264; loss: 0.0776;time per batch:0.1080s\n",
      "Epoch-18\n",
      "————————————————\n",
      "Iter-0; loss: 0.1059;time per batch:0.1134s\n",
      "Iter-33; loss: 0.0728;time per batch:0.0449s\n",
      "Iter-66; loss: 0.0681;time per batch:0.1082s\n",
      "Iter-99; loss: 0.0945;time per batch:0.0451s\n",
      "Iter-132; loss: 0.0663;time per batch:0.1148s\n",
      "Iter-165; loss: 0.0628;time per batch:0.0454s\n",
      "Iter-198; loss: 0.0911;time per batch:0.1081s\n",
      "Iter-231; loss: 0.0689;time per batch:0.0451s\n",
      "Iter-264; loss: 0.0610;time per batch:0.1092s\n",
      "Epoch-19\n",
      "————————————————\n",
      "Iter-0; loss: 0.0934;time per batch:0.1114s\n",
      "Iter-33; loss: 0.0659;time per batch:0.0449s\n",
      "Iter-66; loss: 0.0969;time per batch:0.1082s\n",
      "Iter-99; loss: 0.0675;time per batch:0.0456s\n",
      "Iter-132; loss: 0.0750;time per batch:0.0452s\n",
      "Iter-165; loss: 0.0685;time per batch:0.1090s\n",
      "Iter-198; loss: 0.0840;time per batch:0.0448s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-231; loss: 0.0760;time per batch:0.1083s\n",
      "Iter-264; loss: 0.0764;time per batch:0.0472s\n",
      "Epoch-20\n",
      "————————————————\n",
      "Iter-0; loss: 0.0651;time per batch:0.0451s\n",
      "Iter-33; loss: 0.0715;time per batch:0.1083s\n",
      "Iter-66; loss: 0.1052;time per batch:0.0449s\n",
      "Iter-99; loss: 0.0700;time per batch:0.1079s\n",
      "Iter-132; loss: 0.0597;time per batch:0.0455s\n",
      "Iter-165; loss: 0.0750;time per batch:0.1091s\n",
      "Iter-198; loss: 0.0780;time per batch:0.0452s\n",
      "Iter-231; loss: 0.0565;time per batch:0.1080s\n",
      "Iter-264; loss: 0.0946;time per batch:0.0455s\n",
      "validation results:\n",
      "Mean Rank: 629\n",
      "Mean RR: 0.4008\n",
      "Hit@1: 0.3029\n",
      "Hit@3: 0.4521\n",
      "Hit@5: 0.5144\n",
      "Hit@10: 0.5883\n",
      "Epoch-21\n",
      "————————————————\n",
      "Iter-0; loss: 0.0508;time per batch:0.0454s\n",
      "Iter-33; loss: 0.0775;time per batch:0.1091s\n",
      "Iter-66; loss: 0.0815;time per batch:0.0450s\n",
      "Iter-99; loss: 0.0556;time per batch:0.0518s\n",
      "Iter-132; loss: 0.0795;time per batch:0.0494s\n",
      "Iter-165; loss: 0.0887;time per batch:0.0526s\n",
      "Iter-198; loss: 0.0730;time per batch:0.0521s\n",
      "Iter-231; loss: 0.0584;time per batch:0.0521s\n",
      "Iter-264; loss: 0.0720;time per batch:0.0497s\n",
      "Epoch-22\n",
      "————————————————\n",
      "Iter-0; loss: 0.0966;time per batch:0.0451s\n",
      "Iter-33; loss: 0.0695;time per batch:0.1084s\n",
      "Iter-66; loss: 0.0780;time per batch:0.0450s\n",
      "Iter-99; loss: 0.0768;time per batch:0.1083s\n",
      "Iter-132; loss: 0.0726;time per batch:0.0448s\n",
      "Iter-165; loss: 0.0618;time per batch:0.1091s\n",
      "Iter-198; loss: 0.0832;time per batch:0.0447s\n",
      "Iter-231; loss: 0.0668;time per batch:0.1082s\n",
      "Iter-264; loss: 0.0671;time per batch:0.0450s\n",
      "Epoch-23\n",
      "————————————————\n",
      "Iter-0; loss: 0.0675;time per batch:0.0455s\n",
      "Iter-33; loss: 0.0552;time per batch:0.1083s\n",
      "Iter-66; loss: 0.0767;time per batch:0.0452s\n",
      "Iter-99; loss: 0.0719;time per batch:0.1090s\n",
      "Iter-132; loss: 0.0728;time per batch:0.0466s\n",
      "Iter-165; loss: 0.0561;time per batch:0.1090s\n",
      "Iter-198; loss: 0.0681;time per batch:0.0452s\n",
      "Iter-231; loss: 0.0631;time per batch:0.1083s\n",
      "Iter-264; loss: 0.0725;time per batch:0.0449s\n",
      "Epoch-24\n",
      "————————————————\n",
      "Iter-0; loss: 0.0626;time per batch:0.0449s\n",
      "Iter-33; loss: 0.0561;time per batch:0.1084s\n",
      "Iter-66; loss: 0.0643;time per batch:0.0927s\n",
      "Iter-99; loss: 0.0567;time per batch:0.1082s\n",
      "Iter-132; loss: 0.0516;time per batch:0.1080s\n",
      "Iter-165; loss: 0.0723;time per batch:0.1135s\n",
      "Iter-198; loss: 0.0577;time per batch:0.0844s\n",
      "Iter-231; loss: 0.0680;time per batch:0.1082s\n",
      "Iter-264; loss: 0.0538;time per batch:0.0459s\n",
      "Epoch-25\n",
      "————————————————\n",
      "Iter-0; loss: 0.0525;time per batch:0.1113s\n",
      "Iter-33; loss: 0.0621;time per batch:0.1079s\n",
      "Iter-66; loss: 0.0546;time per batch:0.0973s\n",
      "Iter-99; loss: 0.0762;time per batch:0.1082s\n",
      "Iter-132; loss: 0.0648;time per batch:0.0450s\n",
      "Iter-165; loss: 0.0683;time per batch:0.1087s\n",
      "Iter-198; loss: 0.0813;time per batch:0.0453s\n",
      "Iter-231; loss: 0.0791;time per batch:0.1083s\n",
      "Iter-264; loss: 0.0672;time per batch:0.0452s\n",
      "Epoch-26\n",
      "————————————————\n",
      "Iter-0; loss: 0.0580;time per batch:0.0452s\n",
      "Iter-33; loss: 0.0698;time per batch:0.1086s\n",
      "Iter-66; loss: 0.0473;time per batch:0.0451s\n",
      "Iter-99; loss: 0.0622;time per batch:0.1077s\n",
      "Iter-132; loss: 0.0704;time per batch:0.0451s\n",
      "Iter-165; loss: 0.0718;time per batch:0.1077s\n",
      "Iter-198; loss: 0.0646;time per batch:0.0459s\n",
      "Iter-231; loss: 0.0589;time per batch:0.1089s\n",
      "Iter-264; loss: 0.0694;time per batch:0.0449s\n",
      "Epoch-27\n",
      "————————————————\n",
      "Iter-0; loss: 0.0613;time per batch:0.0448s\n",
      "Iter-33; loss: 0.0565;time per batch:0.1082s\n",
      "Iter-66; loss: 0.0809;time per batch:0.0463s\n",
      "Iter-99; loss: 0.0556;time per batch:0.1080s\n",
      "Iter-132; loss: 0.0740;time per batch:0.0451s\n",
      "Iter-165; loss: 0.0681;time per batch:0.1093s\n",
      "Iter-198; loss: 0.0676;time per batch:0.0447s\n",
      "Iter-231; loss: 0.0652;time per batch:0.1079s\n",
      "Iter-264; loss: 0.0638;time per batch:0.0447s\n",
      "Epoch-28\n",
      "————————————————\n",
      "Iter-0; loss: 0.0693;time per batch:0.1114s\n",
      "Iter-33; loss: 0.0723;time per batch:0.0920s\n",
      "Iter-66; loss: 0.0759;time per batch:0.1083s\n",
      "Iter-99; loss: 0.0676;time per batch:0.0459s\n",
      "Iter-132; loss: 0.0548;time per batch:0.1084s\n",
      "Iter-165; loss: 0.0549;time per batch:0.0449s\n",
      "Iter-198; loss: 0.0666;time per batch:0.1082s\n",
      "Iter-231; loss: 0.0578;time per batch:0.0448s\n",
      "Iter-264; loss: 0.0660;time per batch:0.1095s\n",
      "Epoch-29\n",
      "————————————————\n",
      "Iter-0; loss: 0.0736;time per batch:0.1137s\n",
      "Iter-33; loss: 0.0437;time per batch:0.0460s\n",
      "Iter-66; loss: 0.0498;time per batch:0.1082s\n",
      "Iter-99; loss: 0.0722;time per batch:0.0446s\n",
      "Iter-132; loss: 0.0638;time per batch:0.1090s\n",
      "Iter-165; loss: 0.0602;time per batch:0.0448s\n",
      "Iter-198; loss: 0.0627;time per batch:0.1080s\n",
      "Iter-231; loss: 0.0563;time per batch:0.0516s\n",
      "Iter-264; loss: 0.0343;time per batch:0.0520s\n",
      "Epoch-30\n",
      "————————————————\n",
      "Iter-0; loss: 0.0515;time per batch:0.0487s\n",
      "Iter-33; loss: 0.0790;time per batch:0.0481s\n",
      "Iter-66; loss: 0.0501;time per batch:0.0515s\n",
      "Iter-99; loss: 0.0838;time per batch:0.0507s\n",
      "Iter-132; loss: 0.0779;time per batch:0.1079s\n",
      "Iter-165; loss: 0.0744;time per batch:0.0448s\n",
      "Iter-198; loss: 0.0717;time per batch:0.1082s\n",
      "Iter-231; loss: 0.0691;time per batch:0.0451s\n",
      "Iter-264; loss: 0.0657;time per batch:0.1083s\n",
      "validation results:\n",
      "Mean Rank: 710\n",
      "Mean RR: 0.3981\n",
      "Hit@1: 0.3024\n",
      "Hit@3: 0.4474\n",
      "Hit@5: 0.5082\n",
      "Hit@10: 0.5810\n",
      "Epoch-31\n",
      "————————————————\n",
      "Iter-0; loss: 0.0548;time per batch:0.1078s\n",
      "Iter-33; loss: 0.0695;time per batch:0.0449s\n",
      "Iter-66; loss: 0.0804;time per batch:0.1082s\n",
      "Iter-99; loss: 0.0419;time per batch:0.0451s\n",
      "Iter-132; loss: 0.0563;time per batch:0.1081s\n",
      "Iter-165; loss: 0.0654;time per batch:0.0452s\n",
      "Iter-198; loss: 0.0605;time per batch:0.1080s\n",
      "Iter-231; loss: 0.0578;time per batch:0.0449s\n",
      "Iter-264; loss: 0.0572;time per batch:0.1078s\n",
      "Epoch-32\n",
      "————————————————\n",
      "Iter-0; loss: 0.0511;time per batch:0.1133s\n",
      "Iter-33; loss: 0.0516;time per batch:0.0450s\n",
      "Iter-66; loss: 0.0551;time per batch:0.1084s\n",
      "Iter-99; loss: 0.0409;time per batch:0.0458s\n",
      "Iter-132; loss: 0.0468;time per batch:0.1087s\n",
      "Iter-165; loss: 0.0483;time per batch:0.0451s\n",
      "Iter-198; loss: 0.0756;time per batch:0.1082s\n",
      "Iter-231; loss: 0.0602;time per batch:0.0447s\n",
      "Iter-264; loss: 0.0667;time per batch:0.1083s\n",
      "Epoch-33\n",
      "————————————————\n",
      "Iter-0; loss: 0.0653;time per batch:0.1115s\n",
      "Iter-33; loss: 0.0486;time per batch:0.0454s\n",
      "Iter-66; loss: 0.0459;time per batch:0.1099s\n",
      "Iter-99; loss: 0.0820;time per batch:0.0448s\n",
      "Iter-132; loss: 0.0755;time per batch:0.1091s\n",
      "Iter-165; loss: 0.0494;time per batch:0.0452s\n",
      "Iter-198; loss: 0.0440;time per batch:0.1083s\n",
      "Iter-231; loss: 0.0662;time per batch:0.0457s\n",
      "Iter-264; loss: 0.0598;time per batch:0.1085s\n",
      "Epoch-34\n",
      "————————————————\n",
      "Iter-0; loss: 0.0474;time per batch:0.0545s\n",
      "Iter-33; loss: 0.0504;time per batch:0.0485s\n",
      "Iter-66; loss: 0.0669;time per batch:0.0512s\n",
      "Iter-99; loss: 0.0580;time per batch:0.0522s\n",
      "Iter-132; loss: 0.0365;time per batch:0.0508s\n",
      "Iter-165; loss: 0.0558;time per batch:0.1104s\n",
      "Iter-198; loss: 0.0394;time per batch:0.0862s\n",
      "Iter-231; loss: 0.0572;time per batch:0.1082s\n",
      "Iter-264; loss: 0.0628;time per batch:0.0448s\n",
      "Epoch-35\n",
      "————————————————\n",
      "Iter-0; loss: 0.0684;time per batch:0.1088s\n",
      "Iter-33; loss: 0.0610;time per batch:0.1082s\n",
      "Iter-66; loss: 0.0542;time per batch:0.0450s\n",
      "Iter-99; loss: 0.0706;time per batch:0.1081s\n",
      "Iter-132; loss: 0.0676;time per batch:0.0451s\n",
      "Iter-165; loss: 0.0451;time per batch:0.1079s\n",
      "Iter-198; loss: 0.0641;time per batch:0.0479s\n",
      "Iter-231; loss: 0.0644;time per batch:0.1081s\n",
      "Iter-264; loss: 0.0568;time per batch:0.0882s\n",
      "Epoch-36\n",
      "————————————————\n",
      "Iter-0; loss: 0.0506;time per batch:0.1121s\n",
      "Iter-33; loss: 0.0581;time per batch:0.0452s\n",
      "Iter-66; loss: 0.0547;time per batch:0.1079s\n",
      "Iter-99; loss: 0.0535;time per batch:0.0454s\n",
      "Iter-132; loss: 0.0683;time per batch:0.1077s\n",
      "Iter-165; loss: 0.0526;time per batch:0.0552s\n",
      "Iter-198; loss: 0.0542;time per batch:0.1081s\n",
      "Iter-231; loss: 0.0520;time per batch:0.0455s\n",
      "Iter-264; loss: 0.0390;time per batch:0.1082s\n",
      "Epoch-37\n",
      "————————————————\n",
      "Iter-0; loss: 0.0577;time per batch:0.1114s\n",
      "Iter-33; loss: 0.0438;time per batch:0.0460s\n",
      "Iter-66; loss: 0.0606;time per batch:0.1084s\n",
      "Iter-99; loss: 0.0634;time per batch:0.0448s\n",
      "Iter-132; loss: 0.0434;time per batch:0.1069s\n",
      "Iter-165; loss: 0.0515;time per batch:0.0474s\n",
      "Iter-198; loss: 0.0670;time per batch:0.1082s\n",
      "Iter-231; loss: 0.0739;time per batch:0.0452s\n",
      "Iter-264; loss: 0.0719;time per batch:0.1084s\n",
      "Epoch-38\n",
      "————————————————\n",
      "Iter-0; loss: 0.0533;time per batch:0.1119s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-33; loss: 0.0635;time per batch:0.0451s\n",
      "Iter-66; loss: 0.0526;time per batch:0.1093s\n",
      "Iter-99; loss: 0.0551;time per batch:0.0448s\n",
      "Iter-132; loss: 0.0626;time per batch:0.1082s\n",
      "Iter-165; loss: 0.0493;time per batch:0.0451s\n",
      "Iter-198; loss: 0.0300;time per batch:0.1082s\n",
      "Iter-231; loss: 0.0702;time per batch:0.0450s\n",
      "Iter-264; loss: 0.0495;time per batch:0.1060s\n",
      "Epoch-39\n",
      "————————————————\n",
      "Iter-0; loss: 0.0487;time per batch:0.1152s\n",
      "Iter-33; loss: 0.0717;time per batch:0.0843s\n",
      "Iter-66; loss: 0.0211;time per batch:0.1092s\n",
      "Iter-99; loss: 0.0460;time per batch:0.0733s\n",
      "Iter-132; loss: 0.0522;time per batch:0.1079s\n",
      "Iter-165; loss: 0.0462;time per batch:0.0875s\n",
      "Iter-198; loss: 0.0493;time per batch:0.1084s\n",
      "Iter-231; loss: 0.0656;time per batch:0.0789s\n",
      "Iter-264; loss: 0.0624;time per batch:0.1090s\n",
      "Epoch-40\n",
      "————————————————\n",
      "Iter-0; loss: 0.0505;time per batch:0.0634s\n",
      "Iter-33; loss: 0.0547;time per batch:0.1082s\n",
      "Iter-66; loss: 0.0431;time per batch:0.0499s\n",
      "Iter-99; loss: 0.0416;time per batch:0.1081s\n",
      "Iter-132; loss: 0.0435;time per batch:0.0540s\n",
      "Iter-165; loss: 0.0648;time per batch:0.1139s\n",
      "Iter-198; loss: 0.0552;time per batch:0.1144s\n",
      "Iter-231; loss: 0.0580;time per batch:0.1081s\n",
      "Iter-264; loss: 0.0774;time per batch:0.1081s\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984c300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
