{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d5cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import empty, zeros, cat\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from torchkge.data_structures import SmallKG\n",
    "from torchkge.exceptions import NotYetEvaluatedError\n",
    "from torchkge.sampling import PositionalNegativeSampler\n",
    "from torchkge.utils import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch\n",
    "import os\n",
    "from os.path import join\n",
    "# from evaluation import TemporalLinkPredictionEvaluator\n",
    "\n",
    "from torchkge.utils import datasets\n",
    "import torch\n",
    "from pandas import read_csv, concat\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from torchkge.utils import datasets\n",
    "import torch\n",
    "from pandas import read_csv, concat\n",
    "# from data_utils import TemporalKnowledgeGraph\n",
    "from numpy.random import RandomState\n",
    "\n",
    "from torch import tensor, bernoulli, randint, ones, rand, cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d95459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from os import environ, makedirs\n",
    "from os.path import exists, expanduser, join\n",
    "from torch.utils.data import Dataset\n",
    "from torchkge.exceptions import SizeMismatchError, WrongArgumentsError, SanityError\n",
    "from torchkge.utils.operations import get_dictionaries\n",
    "from torch import cat, eq, int64, long, randperm, tensor, Tensor, zeros_like\n",
    "from collections import defaultdict\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "# from utils import get_temporal_dictionaries\n",
    "from pandas import read_csv, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c8a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.optim import Adam\n",
    "from torchkge.sampling import BernoulliNegativeSampler, UniformNegativeSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b089c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkge.utils import MarginLoss, BinaryCrossEntropyLoss\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from abc import ABC, abstractmethod\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a94ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "from torchkge.utils.dissimilarities import l1_dissimilarity, l2_dissimilarity, \\\n",
    "    l1_torus_dissimilarity, l2_torus_dissimilarity, el2_torus_dissimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "721167df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from torch import tensor, bernoulli, randint, ones, rand, cat\n",
    "\n",
    "from torchkge.exceptions import NotYetImplementedError\n",
    "from torchkge.utils.data import DataLoader\n",
    "from torchkge.utils.operations import get_bernoulli_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e2b79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b3ffc",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5e58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tem_dict = {\n",
    "    '0y': 0, '1y': 1, '2y': 2, '3y': 3, '4y': 4, '5y': 5, '6y': 6, '7y': 7, '8y': 8, '9y': 9,\n",
    "    '0m': 10, '1m': 11, '2m': 12, '3m': 13, '4m': 14, '5m': 15, '6m': 16, '7m': 17, '8m': 18, '9m': 19,\n",
    "    '0d': 20, '1d': 21, '2d': 22, '3d': 23, '4d': 24, '5d': 25, '6d': 26, '7d': 27, '8d': 28, '9d': 29\n",
    "}\n",
    "\n",
    "class Params():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "def read_json_lines(file_name):\n",
    "    lines = []\n",
    "    with open(file_name) as file_in:\n",
    "        for line in file_in:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "def get_expriment_tests(results):\n",
    "    res = []\n",
    "    for exp in results:\n",
    "        train_params = exp['train_params']\n",
    "        test_results = exp['test_results']\n",
    "        train_params.update(test_results)\n",
    "        res.append(train_params)\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "\n",
    "def write_json_lines(file_name,dict_data):\n",
    "    json_string = json.dumps(dict_data)\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(json_string+\"\\n\")\n",
    "\n",
    "def experiment_exists(all_exp_data_df, exp_data_dict):\n",
    "    for k in exp_data_dict:\n",
    "        all_exp_data_df = all_exp_data_df[all_exp_data_df[k]==exp_data_dict[k]]\n",
    "        if all_exp_data_df.shape[0]==0:\n",
    "            return False\n",
    "    return True\n",
    "def check_experiment(exp_file_name):\n",
    "    res = read_json_lines(exp_file_name)\n",
    "    res = get_expriment_tests(res)\n",
    "    return res\n",
    "    # experiment_exists(all_exp_data_df, exp_data_dict)\n",
    "def transform_time_V2(years, months, days):\n",
    "    all_data = []\n",
    "    for year, month, day in zip(years, months, days):\n",
    "        tem_id_list = []\n",
    "        for j in range(len(year)):\n",
    "            token = year[j:j+1]+'y'\n",
    "            tem_id_list.append(tem_dict[token])\n",
    "        # print(tem_id_list)\n",
    "        # exit()\n",
    "\n",
    "        for j in range(1):\n",
    "            # print(month[1])\n",
    "            # exit()\n",
    "            token1 = month[0]+'m'\n",
    "            tem_id_list.append(tem_dict[token1])\n",
    "            token2 = month[0]+'m'\n",
    "            tem_id_list.append(tem_dict[token2])\n",
    "\n",
    "\n",
    "        for j in range(len(day)):\n",
    "            token = day[j:j+1]+'d'\n",
    "            tem_id_list.append(tem_dict[token])\n",
    "            \n",
    "        all_data.append(torch.tensor(tem_id_list))\n",
    "    return all_data\n",
    "\n",
    "def transform_time(raw_time):\n",
    "    year, month, day = raw_time.split(\"-\")\n",
    "    tem_id_list = []\n",
    "    for j in range(len(year)):\n",
    "        token = year[j:j+1]+'y'\n",
    "        tem_id_list.append(tem_dict[token])\n",
    "    # print(tem_id_list)\n",
    "    # exit()\n",
    "\n",
    "    for j in range(1):\n",
    "        # print(month[1])\n",
    "        # exit()\n",
    "        token1 = month[0]+'m'\n",
    "        tem_id_list.append(tem_dict[token1])\n",
    "        token2 = month[0]+'m'\n",
    "        tem_id_list.append(tem_dict[token2])\n",
    "\n",
    "\n",
    "    for j in range(len(day)):\n",
    "        token = day[j:j+1]+'d'\n",
    "        tem_id_list.append(tem_dict[token])\n",
    "    return tem_id_list\n",
    "\n",
    "def transform_time_v3(raw_time):\n",
    "    date = list(map(float, raw_time.split(\"-\")))\n",
    "    return date\n",
    "\n",
    "def transform_time_v4(raw_time):\n",
    "    year, month, day = raw_time.split(\"-\")\n",
    "    year, month, day = int(year), int(month), int(day)\n",
    "    return month + day\n",
    "\n",
    "def transform_time_v5(raw_time):\n",
    "    year, month, day = raw_time.split(\"-\")\n",
    "    return [int(year), int(month), int(day)]\n",
    "\n",
    "def get_temporal_dictionaries(df, mode='simple'):\n",
    "\n",
    "    tmp = list(set(df['start_time'].unique()).union(set(df['end_time'].unique())))\n",
    "    if mode == 'simple_time':\n",
    "        # return {timee: i for i, timee in enumerate(sorted(tmp))}\n",
    "        return {timee: i for i, timee in enumerate(sorted([datetime.strptime(dt, \"%Y-%m-%d\") for dt in tmp]))}\n",
    "    elif mode == 'simple':\n",
    "        # return {timee: i for i, timee in enumerate(sorted(tmp))}\n",
    "        return {timee: i for i, timee in enumerate(sorted([dt for dt in tmp]))}\n",
    "    elif mode == 'seq':\n",
    "        return {timee: torch.tensor(transform_time(timee)) for i, timee in enumerate(sorted(tmp))}\n",
    "        # return {timee: torch.tensor(transform_time(timee)) for i, timee in enumerate(sorted([datetime.strptime(dt, \"%Y-%m-%d\") for dt in tmp]))}\n",
    "    elif mode == 'ymd':\n",
    "        return {timee: transform_time_v4(timee) for i, timee in enumerate(sorted(tmp))}\n",
    "    elif mode == 'ymd_':\n",
    "        return {timee: torch.tensor(transform_time_v5(timee)) for i, timee in enumerate(sorted(tmp))}\n",
    "\n",
    "\n",
    "def cconv(a, b):\n",
    "    return torch.fft.ifft(torch.fft.fft(a) * torch.fft.fft(b)).real\n",
    "\n",
    "\n",
    "def ccorr(a, b):\n",
    "    return torch.fft.ifft(torch.conj(torch.fft.fft(a)) * torch.fft.fft(b)).real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61935a69",
   "metadata": {},
   "source": [
    "##  Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ca6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalKnowledgeGraph(Dataset):\n",
    "\n",
    "    def __init__(self, df=None, time_mode = None,kg=None, ent2ix=None, rel2ix=None, time2ix = None,\n",
    "                 dict_of_heads=None, dict_of_tails=None, dict_of_rels=None,\n",
    "                 temp_dict_of_heads=None, temp_dict_of_tails=None, temp_dict_of_rels=None):\n",
    "\n",
    "        if df is None:\n",
    "            if kg is None:\n",
    "                raise WrongArgumentsError(\"Please provide at least one \"\n",
    "                                          \"argument of `df` and kg`\")\n",
    "            else:\n",
    "                try:\n",
    "                    assert (type(kg) == dict) & ('heads' in kg.keys()) & \\\n",
    "                           ('tails' in kg.keys()) & \\\n",
    "                           ('relations' in kg.keys())& \\\n",
    "                            ('start_time' in kg.keys())& \\\n",
    "                            ('end_time' in kg.keys())\n",
    "                    \n",
    "                except AssertionError:\n",
    "                    raise WrongArgumentsError(\"Keys in the `kg` dict should \"\n",
    "                                              \"contain `heads`, `tails`, \"\n",
    "                                              \"`relations`.\")\n",
    "                try:\n",
    "                    assert (rel2ix is not None) & (ent2ix is not None)\n",
    "                except AssertionError:\n",
    "                    raise WrongArgumentsError(\"Please provide the two \"\n",
    "                                              \"dictionaries ent2ix and rel2ix \"\n",
    "                                              \"if building from `kg`.\")\n",
    "        else:\n",
    "            if kg is not None:\n",
    "                raise WrongArgumentsError(\"`df` and kg` arguments should not \"\n",
    "                                          \"both be provided.\")\n",
    "\n",
    "        if ent2ix is None:\n",
    "            self.ent2ix = get_dictionaries(df, ent=True)\n",
    "        else:\n",
    "            self.ent2ix = ent2ix\n",
    "\n",
    "        if rel2ix is None:\n",
    "            self.rel2ix = get_dictionaries(df, ent=False)\n",
    "        else:\n",
    "            self.rel2ix = rel2ix\n",
    "        \n",
    "        if time_mode is not None:\n",
    "            self.time_mode = time_mode\n",
    "            \n",
    "        if time2ix is None:\n",
    "            self.time2ix = get_temporal_dictionaries(df, mode = self.time_mode)\n",
    "        else:\n",
    "            self.time2ix = time2ix\n",
    "        \n",
    "        self.n_ent = max(self.ent2ix.values()) + 1\n",
    "        self.n_rel = max(self.rel2ix.values()) + 1\n",
    "        time_val = list(self.time2ix.values())\n",
    "        \n",
    "        if isinstance(time_val[0], torch.Tensor):\n",
    "            self.n_time = int(torch.cat(time_val).max()) + 1\n",
    "        else: \n",
    "            self.n_time = max(time_val) + 1\n",
    "            \n",
    "#         print('self.n_time: ',self.n_time)\n",
    "        if df is not None:\n",
    "            # build kg from a pandas dataframe\n",
    "            self.n_facts = len(df)\n",
    "            self.head_idx = tensor(df['from'].map(self.ent2ix).values).long()\n",
    "            self.tail_idx = tensor(df['to'].map(self.ent2ix).values).long()\n",
    "            self.relations = tensor(df['rel'].map(self.rel2ix).values).long()\n",
    "#             self.start_time = tensor(df['start_time'].map(self.rel2ix).values).long()\n",
    "#             self.end_time = tensor(df['end_time'].map(self.rel2ix).values).long()\n",
    "#             print(self.time2ix)\n",
    "            self.start_time = list(df['start_time'].map(self.time2ix).values)\n",
    "#             print(self.start_time)\n",
    "#             print('-'*22)\n",
    "#             print(\"type(self.start_time[0]): \",type(self.start_time[0]))\n",
    "#             print('type(self.start_time): ',type(self.start_time))\n",
    "#             if isinstance(self.start_time[0], torch.Tensor):\n",
    "#                 print('llllll')\n",
    "            if isinstance(self.start_time, list) & isinstance(self.start_time[0], torch.Tensor):\n",
    "                self.start_time = torch.stack(self.start_time)\n",
    "            else:\n",
    "                self.start_time = torch.tensor(self.start_time)\n",
    "            \n",
    "            self.start_time = self.start_time.long()\n",
    "            \n",
    "            self.end_time = list(df['end_time'].map(self.time2ix).values)\n",
    "            if isinstance(self.end_time, list) & isinstance(self.end_time[0], torch.Tensor):\n",
    "                self.end_time = torch.stack(self.end_time)\n",
    "            else:\n",
    "                self.end_time = torch.tensor(self.end_time)   \n",
    "            self.end_time = self.end_time.long()\n",
    "            \n",
    "        else:\n",
    "            # build kg from another kg\n",
    "            self.n_facts = kg['heads'].shape[0]\n",
    "            self.head_idx = kg['heads']\n",
    "            self.tail_idx = kg['tails']\n",
    "            self.relations = kg['relations']\n",
    "            self.start_time = kg['start_time']\n",
    "            self.end_time = kg['end_time']\n",
    "\n",
    "        if dict_of_heads is None or dict_of_tails is None or dict_of_rels is None:\n",
    "            self.dict_of_heads = defaultdict(set)\n",
    "            self.dict_of_tails = defaultdict(set)\n",
    "            self.dict_of_rels = defaultdict(set)\n",
    "            self.temp_dict_of_heads = defaultdict(set)\n",
    "            self.temp_dict_of_tails = defaultdict(set)\n",
    "            self.temp_dict_of_rels = defaultdict(set)\n",
    "#             self.dict_of_start_time = defaultdict(set)\n",
    "#             self.dict_of_end_time = defaultdict(set)\n",
    "            self.evaluate_dicts()\n",
    "\n",
    "        else:\n",
    "            self.dict_of_heads = dict_of_heads\n",
    "            self.dict_of_tails = dict_of_tails\n",
    "            self.dict_of_rels = dict_of_rels\n",
    "            self.temp_dict_of_heads = temp_dict_of_heads\n",
    "            self.temp_dict_of_tails = temp_dict_of_tails\n",
    "            self.temp_dict_of_rels = temp_dict_of_rels\n",
    "#             self.dict_of_start_time = dict_of_start_time\n",
    "#             self.dict_of_end_time = dict_of_end_time\n",
    "        try:\n",
    "            self.sanity_check()\n",
    "        except AssertionError:\n",
    "            raise SanityError(\"Please check the sanity of arguments.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_facts\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (self.head_idx[item].item(),\n",
    "                self.tail_idx[item].item(),\n",
    "                self.relations[item].item())\n",
    "\n",
    "    def sanity_check(self):\n",
    "        assert (type(self.dict_of_heads) == defaultdict) & \\\n",
    "               (type(self.dict_of_tails) == defaultdict) & \\\n",
    "               (type(self.dict_of_rels) == defaultdict) & \\\n",
    "                (type(self.temp_dict_of_heads) == defaultdict) & \\\n",
    "               (type(self.temp_dict_of_tails) == defaultdict) & \\\n",
    "               (type(self.temp_dict_of_rels) == defaultdict)\n",
    "        assert (type(self.ent2ix) == dict) & (type(self.rel2ix) == dict)\n",
    "        assert (len(self.ent2ix) == self.n_ent) & \\\n",
    "               (len(self.rel2ix) == self.n_rel)\n",
    "        assert (type(self.head_idx) == Tensor) & \\\n",
    "               (type(self.tail_idx) == Tensor) & \\\n",
    "               (type(self.relations) == Tensor)\n",
    "        assert (self.head_idx.dtype == int64) & \\\n",
    "               (self.tail_idx.dtype == int64) & (self.relations.dtype == int64)\n",
    "        assert (len(self.head_idx) == len(self.tail_idx) == len(self.relations))\n",
    "\n",
    "    def split_kg(self, share=0.8, sizes=None, validation=False):\n",
    "        if sizes is not None:\n",
    "            try:\n",
    "                if len(sizes) == 3:\n",
    "                    try:\n",
    "                        assert (sizes[0] + sizes[1] + sizes[2] == self.n_facts)\n",
    "                    except AssertionError:\n",
    "                        raise WrongArgumentsError('Sizes should sum to the '\n",
    "                                                  'number of facts.')\n",
    "                elif len(sizes) == 2:\n",
    "                    try:\n",
    "                        assert (sizes[0] + sizes[1] == self.n_facts)\n",
    "                    except AssertionError:\n",
    "                        raise WrongArgumentsError('Sizes should sum to the '\n",
    "                                                  'number of facts.')\n",
    "                else:\n",
    "                    raise SizeMismatchError('Tuple `sizes` should be of '\n",
    "                                            'length 2 or 3.')\n",
    "            except AssertionError:\n",
    "                raise SizeMismatchError('Tuple `sizes` should sum up to the '\n",
    "                                        'number of facts in the knowledge '\n",
    "                                        'graph.')\n",
    "        else:\n",
    "            assert share < 1\n",
    "\n",
    "        if ((sizes is not None) and (len(sizes) == 3)) or \\\n",
    "                ((sizes is None) and validation):\n",
    "            # return training, validation and a testing graphs\n",
    "\n",
    "            if (sizes is None) and validation:\n",
    "                mask_tr, mask_val, mask_te = self.get_mask(share,\n",
    "                                                           validation=True)\n",
    "            else:\n",
    "                mask_tr = cat([tensor([1 for _ in range(sizes[0])]),\n",
    "                               tensor([0 for _ in range(sizes[1] + sizes[2])])]).bool()\n",
    "                mask_val = cat([tensor([0 for _ in range(sizes[0])]),\n",
    "                                tensor([1 for _ in range(sizes[1])]),\n",
    "                                tensor([0 for _ in range(sizes[2])])]).bool()\n",
    "                mask_te = ~(mask_tr | mask_val)\n",
    "\n",
    "            return (TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_tr],\n",
    "                            'tails': self.tail_idx[mask_tr],\n",
    "                            'relations': self.relations[mask_tr],\n",
    "                           'start_time': self.start_time[mask_tr],\n",
    "                           'end_time': self.end_time[mask_tr]},\n",
    "                            ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                            dict_of_heads=self.dict_of_heads,\n",
    "                            dict_of_tails=self.dict_of_tails,\n",
    "                            dict_of_rels=self.dict_of_rels,\n",
    "                            temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                            temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                            temp_dict_of_rels = self.temp_dict_of_rels\n",
    "                            ),\n",
    "                    TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_val],\n",
    "                            'tails': self.tail_idx[mask_val],\n",
    "                            'relations': self.relations[mask_val],\n",
    "                           'start_time':  self.start_time[mask_val],\n",
    "                           'end_time': self.end_time[mask_val]},\n",
    "                        ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                        dict_of_heads=self.dict_of_heads,\n",
    "                        dict_of_tails=self.dict_of_tails,\n",
    "                        dict_of_rels=self.dict_of_rels,\n",
    "                        temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                        temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                        temp_dict_of_rels = self.temp_dict_of_rels),\n",
    "                    TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_te],\n",
    "                            'tails': self.tail_idx[mask_te],\n",
    "                            'relations': self.relations[mask_te],\n",
    "                           'start_time': self.start_time[mask_te],\n",
    "                           'end_time': self.end_time[mask_te]},\n",
    "                        ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                        dict_of_heads=self.dict_of_heads,\n",
    "                        dict_of_tails=self.dict_of_tails,\n",
    "                        dict_of_rels=self.dict_of_rels,\n",
    "                        temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                        temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                        temp_dict_of_rels = self.temp_dict_of_rels\n",
    "                        ))\n",
    "        else:\n",
    "            # return training and testing graphs\n",
    "\n",
    "            assert (((sizes is not None) and len(sizes) == 2) or\n",
    "                    ((sizes is None) and not validation))\n",
    "            if sizes is None:\n",
    "                mask_tr, mask_te = self.get_mask(share, validation=False)\n",
    "            else:\n",
    "                mask_tr = cat([tensor([1 for _ in range(sizes[0])]),\n",
    "                               tensor([0 for _ in range(sizes[1])])]).bool()\n",
    "                mask_te = ~mask_tr\n",
    "            return (TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_tr],\n",
    "                            'tails': self.tail_idx[mask_tr],\n",
    "                            'relations': self.relations[mask_tr],\n",
    "                           'start_time': self.start_time[mask_tr],\n",
    "                           'end_time': self.end_time[mask_tr]},\n",
    "                        ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                        dict_of_heads=self.dict_of_heads,\n",
    "                        dict_of_tails=self.dict_of_tails,\n",
    "                        dict_of_rels=self.dict_of_rels,\n",
    "                        temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                        temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                        temp_dict_of_rels = self.temp_dict_of_rels\n",
    "                        ),\n",
    "                    TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_te],\n",
    "                            'tails': self.tail_idx[mask_te],\n",
    "                            'relations': self.relations[mask_te],\n",
    "                           'start_time': self.start_time[mask_te],\n",
    "                           'end_time': self.end_time[mask_te]},\n",
    "                        ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                        dict_of_heads=self.dict_of_heads,\n",
    "                        dict_of_tails=self.dict_of_tails,\n",
    "                        dict_of_rels=self.dict_of_rels,\n",
    "                        temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                        temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                        temp_dict_of_rels = self.temp_dict_of_rels\n",
    "                        ))\n",
    "\n",
    "    def get_mask(self, share, validation=False):\n",
    "\n",
    "        uniques_r, counts_r = self.relations.unique(return_counts=True)\n",
    "        uniques_e, _ = cat((self.head_idx,\n",
    "                            self.tail_idx)).unique(return_counts=True)\n",
    "\n",
    "        mask = zeros_like(self.relations).bool()\n",
    "        if validation:\n",
    "            mask_val = zeros_like(self.relations).bool()\n",
    "\n",
    "        # splitting relations among subsets\n",
    "        for i, r in enumerate(uniques_r):\n",
    "            rand = randperm(counts_r[i].item())\n",
    "\n",
    "            # list of indices k such that relations[k] == r\n",
    "            sub_mask = eq(self.relations, r).nonzero(as_tuple=False)[:, 0]\n",
    "\n",
    "            assert len(sub_mask) == counts_r[i].item()\n",
    "\n",
    "            if validation:\n",
    "                train_size, val_size, test_size = self.get_sizes(counts_r[i].item(),\n",
    "                                                                 share=share,\n",
    "                                                                 validation=True)\n",
    "                mask[sub_mask[rand[:train_size]]] = True\n",
    "                mask_val[sub_mask[rand[train_size:train_size + val_size]]] = True\n",
    "\n",
    "            else:\n",
    "                train_size, test_size = self.get_sizes(counts_r[i].item(),\n",
    "                                                       share=share,\n",
    "                                                       validation=False)\n",
    "                mask[sub_mask[rand[:train_size]]] = True\n",
    "\n",
    "        # adding missing entities to the train set\n",
    "        u = cat((self.head_idx[mask], self.tail_idx[mask])).unique()\n",
    "        if len(u) < self.n_ent:\n",
    "            missing_entities = tensor(list(set(uniques_e.tolist()) -\n",
    "                                           set(u.tolist())), dtype=long)\n",
    "            for e in missing_entities:\n",
    "                sub_mask = ((self.head_idx == e) |\n",
    "                            (self.tail_idx == e)).nonzero(as_tuple=False)[:, 0]\n",
    "                rand = randperm(len(sub_mask))\n",
    "                sizes = self.get_sizes(mask.shape[0],\n",
    "                                       share=share,\n",
    "                                       validation=validation)\n",
    "                mask[sub_mask[rand[:sizes[0]]]] = True\n",
    "                if validation:\n",
    "                    mask_val[sub_mask[rand[:sizes[0]]]] = False\n",
    "\n",
    "        if validation:\n",
    "            assert not (mask & mask_val).any().item()\n",
    "            return mask, mask_val, ~(mask | mask_val)\n",
    "        else:\n",
    "            return mask, ~mask\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sizes(count, share, validation=False):\n",
    "        if count == 1:\n",
    "            if validation:\n",
    "                return 1, 0, 0\n",
    "            else:\n",
    "                return 1, 0\n",
    "        if count == 2:\n",
    "            if validation:\n",
    "                return 1, 1, 0\n",
    "            else:\n",
    "                return 1, 1\n",
    "\n",
    "        n_train = int(count * share)\n",
    "        assert n_train < count\n",
    "        if n_train == 0:\n",
    "            n_train += 1\n",
    "\n",
    "        if not validation:\n",
    "            return n_train, count - n_train\n",
    "        else:\n",
    "            if count - n_train == 1:\n",
    "                n_train -= 1\n",
    "                return n_train, 1, 1\n",
    "            else:\n",
    "                n_val = int(int(count - n_train) / 2)\n",
    "                return n_train, n_val, count - n_train - n_val\n",
    "\n",
    "    def evaluate_dicts(self):\n",
    "        for i in range(self.n_facts):\n",
    "            self.dict_of_heads[(self.tail_idx[i].item(),\n",
    "                                self.relations[i].item())].add(self.head_idx[i].item())\n",
    "            self.dict_of_tails[(self.head_idx[i].item(),\n",
    "                                self.relations[i].item())].add(self.tail_idx[i].item())\n",
    "            self.dict_of_rels[(self.head_idx[i].item(),\n",
    "                               self.tail_idx[i].item())].add(self.relations[i].item())\n",
    "            \n",
    "            self.temp_dict_of_heads[(self.tail_idx[i].item(),\n",
    "                                    self.relations[i].item(),\n",
    "                                     self.start_time[i].item(),\n",
    "                                     self.end_time[i].item())].add(self.head_idx[i].item())\n",
    "            self.temp_dict_of_tails[(self.head_idx[i].item(),\n",
    "                                self.relations[i].item(),\n",
    "                                     self.start_time[i].item(),\n",
    "                                     self.end_time[i].item())].add(self.tail_idx[i].item())\n",
    "            self.temp_dict_of_rels[(self.head_idx[i].item(),\n",
    "                                   self.tail_idx[i].item(),\n",
    "                                    self.start_time[i].item(),\n",
    "                                     self.end_time[i].item())].add(self.relations[i].item())\n",
    "\n",
    "    def get_df(self):\n",
    "        ix2ent = {v: k for k, v in self.ent2ix.items()}\n",
    "        ix2rel = {v: k for k, v in self.rel2ix.items()}\n",
    "\n",
    "        df = DataFrame(cat((self.head_idx.view(1, -1),\n",
    "                            self.tail_idx.view(1, -1),\n",
    "                            self.relations.view(1, -1))).transpose(0, 1).numpy(),\n",
    "                       columns=['from', 'to', 'rel'])\n",
    "\n",
    "        df['from'] = df['from'].apply(lambda x: ix2ent[x])\n",
    "        df['to'] = df['to'].apply(lambda x: ix2ent[x])\n",
    "        df['rel'] = df['rel'].apply(lambda x: ix2rel[x])\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class SmallKG(Dataset):\n",
    "    def __init__(self, heads, tails, relations):\n",
    "        assert heads.shape == tails.shape == relations.shape\n",
    "        self.head_idx = heads\n",
    "        self.tail_idx = tails\n",
    "        self.relations = relations\n",
    "        self.length = heads.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.head_idx[item].item(), self.tail_idx[item].item(), self.relations[item].item()\n",
    "\n",
    "\n",
    "\n",
    "def get_data_home(data_home=None):\n",
    "    if data_home is None:\n",
    "        data_home = environ.get('TORCHKGE_DATA',\n",
    "                                join('~', 'torchkge_data'))\n",
    "    data_home = expanduser(data_home)\n",
    "    if not exists(data_home):\n",
    "        makedirs(data_home)\n",
    "    return data_home\n",
    "\n",
    "\n",
    "def clear_data_home(data_home=None):\n",
    "    data_home = get_data_home(data_home)\n",
    "    shutil.rmtree(data_home)\n",
    "\n",
    "\n",
    "def get_n_batches(n, b_size):\n",
    "    n_batch = n // b_size\n",
    "    if n % b_size > 0:\n",
    "        n_batch += 1\n",
    "    return n_batch\n",
    "\n",
    "\n",
    "class TempDataLoader:\n",
    "    \"\"\"This class is inspired from :class:`torch.utils.dataloader.DataLoader`.\n",
    "    It is however way simpler.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, kg, batch_size, use_cuda=None):\n",
    "\n",
    "        self.h = kg.head_idx\n",
    "        self.t = kg.tail_idx\n",
    "        self.r = kg.relations\n",
    "        self.start_time = kg.start_time\n",
    "        self.end_time = kg.end_time\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if use_cuda is not None and use_cuda == 'all':\n",
    "            self.h = self.h.cuda()\n",
    "            self.t = self.t.cuda()\n",
    "            self.r = self.r.cuda()\n",
    "            self.start_time = self.start_time.cuda()\n",
    "            self.end_time = self.end_time.cuda()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return get_n_batches(len(self.h), self.batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return _TempDataLoaderIter(self)\n",
    "\n",
    "\n",
    "class _TempDataLoaderIter:\n",
    "    def __init__(self, loader):\n",
    "        self.h = loader.h\n",
    "        self.t = loader.t\n",
    "        self.r = loader.r\n",
    "        self.start_time = loader.start_time\n",
    "        self.end_time = loader.end_time\n",
    "        \n",
    "        self.use_cuda = loader.use_cuda\n",
    "        self.batch_size = loader.batch_size\n",
    "\n",
    "        self.n_batches = get_n_batches(len(self.h), self.batch_size)\n",
    "        self.current_batch = 0\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_batch == self.n_batches:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            i = self.current_batch\n",
    "            self.current_batch += 1\n",
    "\n",
    "            tmp_h = self.h[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            tmp_t = self.t[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            tmp_r = self.r[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            tmp_start_time = self.start_time[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            tmp_end_time = self.end_time[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            \n",
    "            if self.use_cuda is not None and self.use_cuda == 'batch':\n",
    "                return tmp_h.cuda(), tmp_t.cuda(), tmp_r.cuda(), tmp_start_time.cuda(), tmp_end_time.cuda()\n",
    "            else:\n",
    "                return tmp_h, tmp_t, tmp_r, tmp_start_time, tmp_end_time\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "def load_icews14(data_home=None):\n",
    "    data_path = data_home + '/icews14'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode='seq')\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4586f",
   "metadata": {},
   "source": [
    "## Quat Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e07c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_mul_with_unit_norm(*, Q_1, Q_2):\n",
    "    a_h, b_h, c_h, d_h = Q_1  # = {a_h + b_h i + c_h j + d_h k : a_r, b_r, c_r, d_r \\in R^k}\n",
    "    a_r, b_r, c_r, d_r = Q_2  # = {a_r + b_r i + c_r j + d_r k : a_r, b_r, c_r, d_r \\in R^k}\n",
    "\n",
    "    # Normalize the relation to eliminate the scaling effect\n",
    "    denominator = torch.sqrt(a_r ** 2 + b_r ** 2 + c_r ** 2 + d_r ** 2)\n",
    "    p = a_r / denominator\n",
    "    q = b_r / denominator\n",
    "    u = c_r / denominator\n",
    "    v = d_r / denominator\n",
    "    #  Q'=E Hamilton product R\n",
    "    r_val = a_h * p - b_h * q - c_h * u - d_h * v\n",
    "    i_val = a_h * q + b_h * p + c_h * v - d_h * u\n",
    "    j_val = a_h * u - b_h * v + c_h * p + d_h * q\n",
    "    k_val = a_h * v + b_h * u - c_h * q + d_h * p\n",
    "    return r_val, i_val, j_val, k_val\n",
    "\n",
    "\n",
    "def quaternion_mul(*, Q_1, Q_2):\n",
    "    a_h, b_h, c_h, d_h = Q_1  # = {a_h + b_h i + c_h j + d_h k : a_r, b_r, c_r, d_r \\in R^k}\n",
    "    a_r, b_r, c_r, d_r = Q_2  # = {a_r + b_r i + c_r j + d_r k : a_r, b_r, c_r, d_r \\in R^k}\n",
    "    r_val = a_h * a_r - b_h * b_r - c_h * c_r - d_h * d_r\n",
    "    i_val = a_h * b_r + b_h * a_r + c_h * d_r - d_h * c_r\n",
    "    j_val = a_h * c_r - b_h * d_r + c_h * a_r + d_h * b_r\n",
    "    k_val = a_h * d_r + b_h * c_r - c_h * b_r + d_h * a_r\n",
    "    return r_val, i_val, j_val, k_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0cc0c",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d2881cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_name, time_mode):\n",
    "    if data_name == 'icews14':\n",
    "        return load_icews14(data_home='data', time_mode = time_mode)\n",
    "    elif data_name == 'gdelt':\n",
    "        return load_gdelt(data_home='data', time_mode = time_mode)\n",
    "    elif data_name == 'icews15':\n",
    "        return load_icews15(data_home='data', time_mode = time_mode)\n",
    "    elif data_name == 'yago11k':\n",
    "        return load_yago11k(data_home='data', time_mode = time_mode)\n",
    "    elif data_name == 'wikidata12k':\n",
    "        return load_wikidata12k(data_home='data', time_mode = time_mode)\n",
    "    else:\n",
    "        datas = ['icews14']\n",
    "        \n",
    "        print('Choose One of the Following Datasets: ',datas)\n",
    "\n",
    "def load_icews14(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/icews14'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "def load_gdelt(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/gdelt'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time'])\n",
    "    df1['end_time'] = '1111-11-11'\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time'])\n",
    "    df2['end_time'] = '1111-11-11'\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time'])\n",
    "    df3['end_time'] = '1111-11-11'\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "def load_icews15(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/icews05-15'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "def load_yago11k(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/YAGO11k'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    df['start_time'] = df['start_time'].str.replace('##','01')\n",
    "    df['end_time'] = df['end_time'].str.replace('##','01')\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "def load_wikidata12k(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/WIKIDATA12k'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    df['start_time'] = df['start_time'].str.replace('##','01')\n",
    "    df['end_time'] = df['end_time'].str.replace('##','01')\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f62e3b",
   "metadata": {},
   "source": [
    "## Eval Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a3ba6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_targets(dictionary, key1, key2, key3, key4, true_idx, i):\n",
    "    try:\n",
    "        if key4 is not None:\n",
    "            true_targets = dictionary[key1[i].item(), key2[i].item(), key3[i].item(), key4[i].item()].copy()\n",
    "        else:\n",
    "            true_targets = dictionary[key1[i].item(), key2[i].item(), key3[i].item()].copy()\n",
    "            \n",
    "        if true_idx is not None:\n",
    "            true_targets.remove(true_idx[i].item())\n",
    "            if len(true_targets) > 0:\n",
    "                return tensor(list(true_targets)).long()\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return tensor(list(true_targets)).long()\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c38f999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_scores(scores, self.kg.dict_of_tails, h_idx, r_idx, t_idx, start_time, end_time)\n",
    "def filter_scores(scores, dictionary, key1, key2, key3, key4, true_idx):\n",
    "    # filter out the true negative samples by assigning - inf score.\n",
    "    b_size = scores.shape[0]\n",
    "    filt_scores = scores.clone()\n",
    "\n",
    "    for i in range(b_size):\n",
    "        true_targets = get_true_targets(dictionary, key1, key2, key3, key4, true_idx, i)\n",
    "        if true_targets is None:\n",
    "            continue\n",
    "        filt_scores[i][true_targets] = - float('Inf')\n",
    "\n",
    "    return filt_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0447b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(data, true, low_values=False):\n",
    "    true_data = data.gather(1, true.long().view(-1, 1))\n",
    "\n",
    "    if low_values:\n",
    "        return (data <= true_data).sum(dim=1)\n",
    "    else:\n",
    "        return (data >= true_data).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c67061",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b416309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalLinkPredictionEvaluator(object):\n",
    "\n",
    "    def __init__(self, model, knowledge_graph):\n",
    "        self.model = model\n",
    "        self.kg = knowledge_graph\n",
    "\n",
    "        self.rank_true_heads = empty(size=(knowledge_graph.n_facts,)).long()\n",
    "        self.rank_true_tails = empty(size=(knowledge_graph.n_facts,)).long()\n",
    "        self.filt_rank_true_heads = empty(size=(knowledge_graph.n_facts,)).long()\n",
    "        self.filt_rank_true_tails = empty(size=(knowledge_graph.n_facts,)).long()\n",
    "\n",
    "        self.evaluated = False\n",
    "\n",
    "    def evaluate(self, b_size, verbose=True):\n",
    "        use_cuda = next(self.model.parameters()).is_cuda\n",
    "\n",
    "        if use_cuda:\n",
    "            dataloader = TempDataLoader(self.kg, batch_size=b_size, use_cuda='batch')\n",
    "            self.rank_true_heads = self.rank_true_heads.cuda()\n",
    "            self.rank_true_tails = self.rank_true_tails.cuda()\n",
    "            self.filt_rank_true_heads = self.filt_rank_true_heads.cuda()\n",
    "            self.filt_rank_true_tails = self.filt_rank_true_tails.cuda()\n",
    "        else:\n",
    "            dataloader = TempDataLoader(self.kg, batch_size=b_size)\n",
    "\n",
    "        for i, batch in tqdm(enumerate(dataloader), total=len(dataloader),\n",
    "                             unit='batch', disable=(not verbose),\n",
    "                             desc='Link prediction evaluation'):\n",
    "            h_idx, t_idx, r_idx, start_time, end_time = batch[0], batch[1], batch[2], batch[3], batch[4]\n",
    "                        \n",
    "            h_emb, t_emb, r_emb, candidates = self.model.inference_prepare_candidates(h_idx, t_idx, r_idx, start_time, end_time, entities=True)\n",
    "\n",
    "            scores = self.model.inference_scoring_function(h_emb, candidates, r_emb, start_time, end_time)\n",
    "            filt_scores = filter_scores(scores, self.kg.temp_dict_of_tails, h_idx, r_idx, start_time, end_time, t_idx)\n",
    "            self.rank_true_tails[i * b_size: (i + 1) * b_size] = get_rank(scores, t_idx).detach()\n",
    "            self.filt_rank_true_tails[i * b_size: (i + 1) * b_size] = get_rank(filt_scores, t_idx).detach()\n",
    "\n",
    "            scores = self.model.inference_scoring_function(candidates, t_emb, r_emb, start_time, end_time)\n",
    "            filt_scores = filter_scores(scores, self.kg.temp_dict_of_heads, t_idx, r_idx, start_time, end_time, h_idx)\n",
    "            self.rank_true_heads[i * b_size: (i + 1) * b_size] = get_rank(scores, h_idx).detach()\n",
    "            self.filt_rank_true_heads[i * b_size: (i + 1) * b_size] = get_rank(filt_scores, h_idx).detach()\n",
    "\n",
    "        self.evaluated = True\n",
    "\n",
    "        if use_cuda:\n",
    "            self.rank_true_heads = self.rank_true_heads.cpu()\n",
    "            self.rank_true_tails = self.rank_true_tails.cpu()\n",
    "            self.filt_rank_true_heads = self.filt_rank_true_heads.cpu()\n",
    "            self.filt_rank_true_tails = self.filt_rank_true_tails.cpu()\n",
    "\n",
    "    def mean_rank(self):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "        sum_ = (self.rank_true_heads.float().mean() +\n",
    "                self.rank_true_tails.float().mean()).item()\n",
    "        filt_sum = (self.filt_rank_true_heads.float().mean() +\n",
    "                    self.filt_rank_true_tails.float().mean()).item()\n",
    "        # return sum_ / 2, filt_sum / 2\n",
    "        return {'mr':sum_ / 2, 'filt_mr':filt_sum / 2}\n",
    "\n",
    "    def hit_at_k_heads(self, k=10):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "        head_hit = (self.rank_true_heads <= k).float().mean()\n",
    "        filt_head_hit = (self.filt_rank_true_heads <= k).float().mean()\n",
    "\n",
    "        # return head_hit.item(), filt_head_hit.item()\n",
    "        return {'head_hit_'+str(k):head_hit.item(), 'filt_head_hit_'+str(k): filt_head_hit.item()}\n",
    "\n",
    "    def hit_at_k_tails(self, k=10):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "        tail_hit = (self.rank_true_tails <= k).float().mean()\n",
    "        filt_tail_hit = (self.filt_rank_true_tails <= k).float().mean()\n",
    "\n",
    "        # return tail_hit.item(), filt_tail_hit.item()\n",
    "        return {'tail_hit_'+str(k):tail_hit.item(), 'filt_tail_hit_'+str(k):filt_tail_hit.item()}\n",
    "\n",
    "    def hit_at_k(self, k=10):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "\n",
    "        head_hit = self.hit_at_k_heads(k=k)\n",
    "        head_hit, filt_head_hit = head_hit['head_hit_'+str(k)], head_hit['filt_head_hit_'+str(k)]\n",
    "\n",
    "        tail_hit = self.hit_at_k_tails(k=k)\n",
    "        tail_hit, filt_tail_hit = tail_hit['tail_hit_'+str(k)], tail_hit['filt_tail_hit_'+str(k)]\n",
    "\n",
    "        # return (head_hit + tail_hit) / 2, (filt_head_hit + filt_tail_hit) / 2\n",
    "        return {'hit_'+str(k): (head_hit + tail_hit) / 2, 'filt_hit_'+str(k): (filt_head_hit + filt_tail_hit) / 2}\n",
    "\n",
    "    def mrr(self):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "        res = {}\n",
    "        head_mrr = (self.rank_true_heads.float()**(-1)).mean()\n",
    "        res['head_mrr'] = head_mrr.item()\n",
    "        tail_mrr = (self.rank_true_tails.float()**(-1)).mean()\n",
    "        res['tail_mrr'] = tail_mrr.item()\n",
    "        filt_head_mrr = (self.filt_rank_true_heads.float()**(-1)).mean()\n",
    "        res['filt_head_mrr'] = filt_head_mrr.item()\n",
    "        filt_tail_mrr = (self.filt_rank_true_tails.float()**(-1)).mean()    \n",
    "        res['filt_tail_mrr'] = filt_tail_mrr.item()\n",
    "        res['mrr'] = (head_mrr + tail_mrr).item() / 2\n",
    "        res['filt_mrr'] = (filt_head_mrr + filt_tail_mrr).item() / 2\n",
    "        return res\n",
    "\n",
    "    def get_results(self, k=None):\n",
    "        res = {}\n",
    "        if k is None:\n",
    "            k = 10\n",
    "\n",
    "        for i in range(1, k + 1):\n",
    "            hits_res = self.hit_at_k(k=i)\n",
    "            res.update(hits_res)\n",
    "\n",
    "        mr_res = self.mean_rank()\n",
    "        res.update(mr_res)\n",
    "        mrr_res = self.mrr()\n",
    "        res.update(mrr_res)\n",
    "        return res\n",
    "\n",
    "    def print_results(self, k=None, n_digits=3):\n",
    "        if k is None:\n",
    "            k = 10\n",
    "\n",
    "        if k is not None and type(k) == int:\n",
    "            print('Hit@{} : {} \\t\\t Filt. Hit@{} : {}'.format(\n",
    "                k, round(self.hit_at_k(k=k)[0], n_digits),\n",
    "                k, round(self.hit_at_k(k=k)[1], n_digits)))\n",
    "        if k is not None and type(k) == list:\n",
    "            for i in k:\n",
    "                print('Hit@{} : {} \\t\\t Filt. Hit@{} : {}'.format(\n",
    "                    i, round(self.hit_at_k(k=i)[0], n_digits),\n",
    "                    i, round(self.hit_at_k(k=i)[1], n_digits)))\n",
    "\n",
    "        print('Mean Rank : {} \\t Filt. Mean Rank : {}'.format(\n",
    "            int(self.mean_rank()[0]), int(self.mean_rank()[1])))\n",
    "        print('MRR : {} \\t\\t Filt. MRR : {}'.format(\n",
    "            round(self.mrr()[0], n_digits), round(self.mrr()[1], n_digits)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24164d4e",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b3a7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss_name, args):\n",
    "    if loss_name == 'MarginLoss':\n",
    "        return MarginLoss(args.margin)\n",
    "    elif loss_name == 'BinaryCrossEntropyLoss':\n",
    "        return BinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017960ed",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a99e3",
   "metadata": {},
   "source": [
    "## Interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f838ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempModel(Module):\n",
    "    def __init__(self, n_entities, n_relations):\n",
    "        super().__init__()\n",
    "        self.n_ent = n_entities\n",
    "        self.n_rel = n_relations\n",
    "\n",
    "    def forward(self, heads, tails, relations, start_time, end_time, negative_heads, negative_tails, negative_relations=None):\n",
    "        pos = self.scoring_function(heads, tails, relations, start_time, end_time)\n",
    "\n",
    "        if negative_relations is None:\n",
    "            negative_relations = relations\n",
    "\n",
    "        if negative_heads.shape[0] > negative_relations.shape[0]:\n",
    "            # in that case, several negative samples are sampled from each fact\n",
    "            n_neg = int(negative_heads.shape[0] / negative_relations.shape[0])\n",
    "            # print('pos.shape: ', pos.shape)\n",
    "            pos = pos.repeat(n_neg)\n",
    "            neg = self.scoring_function(negative_heads,\n",
    "                                        negative_tails,\n",
    "                                        negative_relations.repeat(n_neg),\n",
    "                                        start_time.repeat(n_neg, 1) if start_time.dim() ==2 & start_time.shape[-1]>1 else start_time.repeat(n_neg),\n",
    "                                        end_time.repeat(n_neg, 1) if end_time.dim() ==2 & end_time.shape[-1]>1 else end_time.repeat(n_neg))\n",
    "        else:\n",
    "            neg = self.scoring_function(negative_heads,\n",
    "                                        negative_tails,\n",
    "                                        negative_relations, start_time, end_time)\n",
    "\n",
    "        return pos, neg\n",
    "\n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_scoring_function(self, h, t, r, time):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx, entities=True):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class TempTranslationModel(TempModel):\n",
    "    def __init__(self, n_entities, n_relations, dissimilarity_type):\n",
    "        super().__init__(n_entities, n_relations)\n",
    "\n",
    "        assert dissimilarity_type in ['L1', 'L2', 'torus_L1', 'torus_L2',\n",
    "                                      'torus_eL2']\n",
    "\n",
    "        if dissimilarity_type == 'L1':\n",
    "            self.dissimilarity = l1_dissimilarity\n",
    "        elif dissimilarity_type == 'L2':\n",
    "            self.dissimilarity = l2_dissimilarity\n",
    "        elif dissimilarity_type == 'torus_L1':\n",
    "            self.dissimilarity = l1_torus_dissimilarity\n",
    "        elif dissimilarity_type == 'torus_L2':\n",
    "            self.dissimilarity = l2_torus_dissimilarity\n",
    "        else:\n",
    "            self.dissimilarity = el2_torus_dissimilarity\n",
    "\n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx, entities=True):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_scoring_function(self, proj_h, proj_t, r, start_time, end_time):\n",
    "        b_size = proj_h.shape[0]\n",
    "\n",
    "        if len(r.shape) == 2:\n",
    "            if len(proj_t.shape) == 3:\n",
    "                assert (len(proj_h.shape) == 2)\n",
    "                # this is the tail completion case in link prediction\n",
    "                hr = (proj_h + r).view(b_size, 1, r.shape[1])\n",
    "                return - self.dissimilarity(hr, proj_t)\n",
    "            else:\n",
    "                assert (len(proj_h.shape) == 3) & (len(proj_t.shape) == 2)\n",
    "                # this is the head completion case in link prediction\n",
    "                r_ = r.view(b_size, 1, r.shape[1])\n",
    "                t_ = proj_t.view(b_size, 1, r.shape[1])\n",
    "                return - self.dissimilarity(proj_h + r_, t_)\n",
    "        elif len(r.shape) == 3:\n",
    "            # this is the relation prediction case\n",
    "            # Two cases possible:\n",
    "            # * proj_ent.shape == (b_size, self.n_rel, self.emb_dim) -> projection depending on relations\n",
    "            # * proj_ent.shape == (b_size, self.emb_dim) -> no projection\n",
    "            proj_h = proj_h.view(b_size, -1, self.emb_dim)\n",
    "            proj_t = proj_t.view(b_size, -1, self.emb_dim)\n",
    "            return - self.dissimilarity(proj_h + r, proj_t)\n",
    "\n",
    "\n",
    "class TempBilinearModel(TempModel):\n",
    "\n",
    "    def __init__(self, emb_dim, n_entities, n_relations):\n",
    "        super().__init__(n_entities, n_relations)\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_scoring_function(self, h, t, r, start_time, end_time):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx, start_time, end_time, entities=True):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571444a8",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "903e3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_reg(h,t,r):\n",
    "    s_a, x_a, y_a, z_a = torch.chunk(h, 4, dim =-1)\n",
    "    s_c, x_c, y_c, z_c = torch.chunk(t, 4, dim =-1)\n",
    "    s_b, x_b, y_b, z_b = torch.chunk(t, 4, dim =-1)\n",
    "    regul = (torch.mean( torch.abs(s_a) ** 2)\n",
    "                 + torch.mean( torch.abs(x_a) ** 2)\n",
    "                 + torch.mean( torch.abs(y_a) ** 2)\n",
    "                 + torch.mean( torch.abs(z_a) ** 2)\n",
    "                 + torch.mean( torch.abs(s_c) ** 2)\n",
    "                 + torch.mean( torch.abs(x_c) ** 2)\n",
    "                 + torch.mean( torch.abs(y_c) ** 2)\n",
    "                 + torch.mean( torch.abs(z_c) ** 2)\n",
    "                 )\n",
    "    regul2 =  (torch.mean( torch.abs(s_b) ** 2 )\n",
    "             + torch.mean( torch.abs(x_b) ** 2 )\n",
    "             + torch.mean( torch.abs(y_b) ** 2 )\n",
    "             + torch.mean( torch.abs(z_b) ** 2 ))\n",
    "    return 0.2 * (regul + regul2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfffea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_init(in_features, out_features, criterion='he'):\n",
    "    fan_in = in_features\n",
    "    fan_out = out_features\n",
    "\n",
    "    if criterion == 'glorot':\n",
    "        s = 1. / np.sqrt(2 * (fan_in + fan_out))\n",
    "    elif criterion == 'he':\n",
    "        s = 1. / np.sqrt(2 * fan_in)\n",
    "    else:\n",
    "        raise ValueError('Invalid criterion: ', criterion)\n",
    "    rng = RandomState(123)\n",
    "\n",
    "    # Generating randoms and purely imaginary quaternions :\n",
    "    kernel_shape = (in_features, out_features)\n",
    "\n",
    "    number_of_weights = np.prod(kernel_shape)\n",
    "    v_i = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    v_j = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    v_k = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "\n",
    "    # Purely imaginary quaternions unitary\n",
    "    for i in range(0, number_of_weights):\n",
    "        norm = np.sqrt(v_i[i] ** 2 + v_j[i] ** 2 + v_k[i] ** 2) + 0.0001\n",
    "        v_i[i] /= norm\n",
    "        v_j[i] /= norm\n",
    "        v_k[i] /= norm\n",
    "    v_i = v_i.reshape(kernel_shape)\n",
    "    v_j = v_j.reshape(kernel_shape)\n",
    "    v_k = v_k.reshape(kernel_shape)\n",
    "\n",
    "    modulus = rng.uniform(low=-s, high=s, size=kernel_shape)\n",
    "    phase = rng.uniform(low=-np.pi, high=np.pi, size=kernel_shape)\n",
    "\n",
    "    weight_r = modulus * np.cos(phase)\n",
    "    weight_i = modulus * v_i * np.sin(phase)\n",
    "    weight_j = modulus * v_j * np.sin(phase)\n",
    "    weight_k = modulus * v_k * np.sin(phase)\n",
    "\n",
    "    return (weight_r, weight_i, weight_j, weight_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2a62989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oct_init(in_features, out_features, criterion='he'):\n",
    "    fan_in = in_features\n",
    "    fan_out = out_features\n",
    "\n",
    "    if criterion == 'glorot':\n",
    "        s = 1. / np.sqrt(2 * (fan_in + fan_out))\n",
    "    elif criterion == 'he':\n",
    "        s = 1. / np.sqrt(2 * fan_in)\n",
    "    else:\n",
    "        raise ValueError('Invalid criterion: ', criterion)\n",
    "    rng = RandomState(123)\n",
    "\n",
    "    # Generating randoms and purely imaginary quaternions :\n",
    "    kernel_shape = (in_features, out_features)\n",
    "\n",
    "    number_of_weights = np.prod(kernel_shape)\n",
    "    v_i = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    v_j = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    v_k = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    v_l = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    v_m = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    v_n = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    v_o = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    # Purely imaginary quaternions unitary\n",
    "    for i in range(0, number_of_weights):\n",
    "        norm = np.sqrt(v_i[i] ** 2 + v_j[i] ** 2 + v_k[i] ** 2) + 0.0001\n",
    "        v_i[i] /= norm\n",
    "        v_j[i] /= norm\n",
    "        v_k[i] /= norm\n",
    "        v_l[i] /= norm\n",
    "        v_m[i] /= norm\n",
    "        v_n[i] /= norm\n",
    "        v_o[i] /= norm\n",
    "    v_i = v_i.reshape(kernel_shape)\n",
    "    v_j = v_j.reshape(kernel_shape)\n",
    "    v_k = v_k.reshape(kernel_shape)\n",
    "    v_l = v_l.reshape(kernel_shape)\n",
    "    v_m = v_m.reshape(kernel_shape)\n",
    "    v_n = v_n.reshape(kernel_shape)\n",
    "    v_o = v_o.reshape(kernel_shape)\n",
    "\n",
    "    modulus = rng.uniform(low=-s, high=s, size=kernel_shape)\n",
    "    phase = rng.uniform(low=-np.pi, high=np.pi, size=kernel_shape)\n",
    "\n",
    "    weight_r = modulus * np.cos(phase)\n",
    "    weight_i = modulus * v_i * np.sin(phase)\n",
    "    weight_j = modulus * v_j * np.sin(phase)\n",
    "    weight_k = modulus * v_k * np.sin(phase)\n",
    "    weight_l = modulus * v_l * np.sin(phase)\n",
    "    weight_m = modulus * v_m * np.sin(phase)\n",
    "    weight_n = modulus * v_n * np.sin(phase)\n",
    "    weight_o = modulus * v_o * np.sin(phase)\n",
    "    return (weight_r, weight_i, weight_j, weight_k, weight_l, weight_m, weight_n, weight_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6adc51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quat_norm(q):\n",
    "    s_b, x_b, y_b, z_b = torch.chunk(q, 4, dim=-1)\n",
    "    denominator_b = torch.sqrt(s_b ** 2 + x_b ** 2 + y_b ** 2 + z_b ** 2)\n",
    "    s_b = s_b / denominator_b\n",
    "    x_b = x_b / denominator_b\n",
    "    y_b = y_b / denominator_b\n",
    "    z_b = z_b / denominator_b\n",
    "    return torch.cat([s_b, x_b, y_b, z_b], dim=-1)\n",
    "\n",
    "def hamilton_quat_prod(q1, q2):\n",
    "    s_a, x_a, y_a, z_a = torch.chunk(q1, 4, dim=-1)\n",
    "    s_b, x_b, y_b, z_b = torch.chunk(q2, 4, dim=-1)\n",
    "    \n",
    "    A = s_a * s_b - x_a * x_b - y_a * y_b - z_a * z_b\n",
    "    B = s_a * x_b + s_b * x_a + y_a * z_b - y_b * z_a\n",
    "    C = s_a * y_b + s_b * y_a + z_a * x_b - z_b * x_a\n",
    "    D = s_a * z_b + s_b * z_a + x_a * y_b - x_b * y_a\n",
    "\n",
    "    return torch.cat([A, B, C, D], dim=-1)\n",
    "\n",
    "def quat_dot_prod(q1, q2):\n",
    "    return torch.sum(q1* q2, -1)\n",
    "\n",
    "def _onorm(q):\n",
    "    r_1, r_2, r_3, r_4, r_5, r_6, r_7, r_8 = torch.chunk(q, 8, dim=-1)\n",
    "    denominator = torch.sqrt(r_1 ** 2 + r_2 ** 2 + r_3 ** 2 + r_4 ** 2\n",
    "                             + r_5 ** 2 + r_6 ** 2 + r_7 ** 2 + r_8 ** 2)\n",
    "    r_1 = r_1 / denominator\n",
    "    r_2 = r_2 / denominator\n",
    "    r_3 = r_3 / denominator\n",
    "    r_4 = r_4 / denominator\n",
    "    r_5 = r_5 / denominator\n",
    "    r_6 = r_6 / denominator\n",
    "    r_7 = r_7 / denominator\n",
    "    r_8 = r_8 / denominator\n",
    "\n",
    "    return  torch.cat([r_1, r_2, r_3, r_4, r_5, r_6, r_7, r_8], dim=-1)\n",
    "\n",
    "def _qmult(s_a, x_a, y_a, z_a, s_b, x_b, y_b, z_b):\n",
    "    A = s_a * s_b - x_a * x_b - y_a * y_b - z_a * z_b\n",
    "    B = s_a * x_b + s_b * x_a + y_a * z_b - y_b * z_a\n",
    "    C = s_a * y_b + s_b * y_a + z_a * x_b - z_b * x_a\n",
    "    D = s_a * z_b + s_b * z_a + x_a * y_b - x_b * y_a\n",
    "    return A, B, C, D\n",
    "\n",
    "def _qstar(a, b, c, d):\n",
    "    return a, -b, -c, -d\n",
    "\n",
    "def _omult(q1, q2):\n",
    "    a_1, a_2, a_3, a_4, b_1, b_2, b_3, b_4 = torch.chunk(q1, 8, dim=-1)\n",
    "    c_1, c_2, c_3, c_4, d_1, d_2, d_3, d_4 = torch.chunk(q2, 8, dim=-1)\n",
    "    d_1_star, d_2_star, d_3_star, d_4_star = _qstar(d_1, d_2, d_3, d_4)\n",
    "    c_1_star, c_2_star, c_3_star, c_4_star = _qstar(c_1, c_2, c_3, c_4)\n",
    "\n",
    "    o_1, o_2, o_3, o_4 = _qmult(a_1, a_2, a_3, a_4, c_1, c_2, c_3, c_4 )\n",
    "    o_1s, o_2s, o_3s, o_4s = _qmult(d_1_star, d_2_star, d_3_star, d_4_star,  b_1, b_2, b_3, b_4)\n",
    "\n",
    "    o_5, o_6, o_7, o_8 = _qmult(d_1, d_2, d_3, d_4, a_1, a_2, a_3, a_4 )\n",
    "    o_5s, o_6s, o_7s, o_8s = _qmult( b_1, b_2, b_3, b_4, c_1_star, c_2_star, c_3_star, c_4_star)\n",
    "\n",
    "    return  torch.cat([o_1 - o_1s, o_2 - o_2s, o_3 - o_3s, o_4 - o_4s,\n",
    "            o_5 + o_5s, o_6 + o_6s, o_7 + o_7s, o_8 + o_8s], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "113aac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDYCE_Quat(TempBilinearModel):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args.emb_dim, args.n_entities, args.n_relations)\n",
    "        self.emb_dim = args.emb_dim\n",
    "        self.tem_total = args.tem_total\n",
    "        self.quat_emb_dim = self.emb_dim * 4\n",
    "        self.ent_emb = nn.Embedding(self.n_ent, self.quat_emb_dim)\n",
    "        self.rel_emb = nn.Embedding(self.n_rel, self.quat_emb_dim)\n",
    "        self.rel_transfer = nn.Embedding(self.n_rel, self.quat_emb_dim)\n",
    "        self.time_transfer = nn.Embedding(self.tem_total, self.quat_emb_dim)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        r, i, j, k = quaternion_init(self.n_ent, self.emb_dim)\n",
    "        r, i, j, k = torch.from_numpy(r), torch.from_numpy(i), torch.from_numpy(j), torch.from_numpy(k)\n",
    "        vec1 = torch.cat([r, i, j, k], dim=1)\n",
    "        self.ent_emb.weight.data = vec1.type_as(self.ent_emb.weight.data)\n",
    "\n",
    "        s, x, y, z = quaternion_init(self.n_rel, self.emb_dim)\n",
    "        s, x, y, z = torch.from_numpy(s), torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(z)\n",
    "        vec2 = torch.cat([s, x, y, z], dim=1)\n",
    "        self.rel_emb.data = vec2.type_as(self.rel_emb.weight.data)\n",
    "\n",
    "        s, x, y, z = quaternion_init(self.n_rel, self.emb_dim)\n",
    "        s, x, y, z = torch.from_numpy(s), torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(z)\n",
    "        vec2 = torch.cat([s, x, y, z], dim=1)\n",
    "        self.rel_transfer.data = vec2.type_as(self.rel_transfer.weight.data)\n",
    "\n",
    "        r, i, j, k = quaternion_init(self.n_ent, self.emb_dim)\n",
    "        r, i, j, k = torch.from_numpy(r), torch.from_numpy(i), torch.from_numpy(j), torch.from_numpy(k)\n",
    "        vec1 = torch.cat([r, i, j, k], dim=1)\n",
    "        self.time_transfer.weight.data = vec1.type_as(self.time_transfer.weight.data)\n",
    "\n",
    "    def _calc(self, h, r):\n",
    "        return hamilton_quat_prod(h, quat_norm(r))\n",
    "\n",
    "    def eval_calc(self, h, r):\n",
    "        r = quat_norm(r)\n",
    "        if (len(h.shape) == 3) & (len(r.shape) == 2):\n",
    "            r = r.view(r.shape[0], 1, r.shape[1])\n",
    "        elif (len(r.shape) == 3) & (len(h.shape) == 2):\n",
    "            r = r.view(r.shape[0], 1, r.shape[1])\n",
    "        return hamilton_quat_prod(h, r)\n",
    "\n",
    "    def _transfer(self, x, x_transfer, r_transfer):\n",
    "        ent_transfer = self._calc(x, x_transfer)\n",
    "        ent_rel_transfer = self._calc(ent_transfer, r_transfer)\n",
    "\n",
    "        return ent_rel_transfer\n",
    "\n",
    "    def eval_transfer(self, x, x_transfer, r_transfer):\n",
    "        ent_transfer = self.eval_calc(x, x_transfer)\n",
    "        # print('ent_transfer.shape: ',ent_transfer.shape)\n",
    "        # print('r_transfer.shape: ',r_transfer.shape)\n",
    "        ent_rel_transfer = self.eval_calc(ent_transfer, r_transfer)\n",
    "\n",
    "        return ent_rel_transfer\n",
    "    \n",
    "    \n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        h = self.ent_emb(h_idx)\n",
    "        r = self.rel_emb(r_idx)\n",
    "        t = self.ent_emb(t_idx)\n",
    "        # (h, r, t) transfer vector\n",
    "        time_transfer = self.time_transfer(start_time_idx)\n",
    "        r_transfer = self.rel_transfer(r_idx)\n",
    "\n",
    "        h1 = self._transfer(h, time_transfer, r_transfer)\n",
    "        t1 = self._transfer(t, time_transfer, r_transfer)\n",
    "        # multiplication as QuatE\n",
    "        hr = self._calc(h1, r)\n",
    "        # Inner product as QuatE\n",
    "        score = quat_dot_prod(hr, t1)\n",
    "        return score\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        self.normalize_parameters()\n",
    "        return self.ent_emb.weight.data, self.rel_emb.weight.data\n",
    "\n",
    "    def inference_scoring_function(self, h, t, r, start_time, end_time):\n",
    "        b_size = h.shape[0]\n",
    "        emb_dim_ = 4*self.emb_dim\n",
    "        if len(t.shape) == 3:\n",
    "            assert (len(h.shape) == 2) & (len(r.shape) == 2)\n",
    "            r, r_transfer, time_transfer = torch.chunk(r, 3, dim=-1)\n",
    "            h1 = self.eval_transfer(h, time_transfer, r_transfer)\n",
    "            # print('t.shape: ',t.shape)\n",
    "            t1 = self.eval_transfer(t, time_transfer, r_transfer)\n",
    "            hr = self.eval_calc(h1, r)\n",
    "            hr = hr.view(hr.shape[0],1,hr.shape[1])\n",
    "            # print('t1.shape: ',t1.shape, ' hr.shape: ',hr.shape)\n",
    "            score = torch.sum(hr * t1, -1)\n",
    "            return score\n",
    "\n",
    "        elif len(h.shape) == 3:\n",
    "            assert (len(t.shape) == 2) & (len(r.shape) == 2)\n",
    "            # this is the head completion case in link prediction\n",
    "            r, r_transfer, time_transfer = torch.chunk(r, 3, dim=-1)\n",
    "            # this is the head completion case in link prediction\n",
    "            h1 = self.eval_transfer(h, time_transfer, r_transfer)\n",
    "            t1 = self.eval_transfer(t, time_transfer, r_transfer)\n",
    "            hr = self.eval_calc(h1, r)\n",
    "            # print('t1.shape: ',t1.shape, ' hr.shape: ',hr.shape)\n",
    "            t1 = t1.view(t1.shape[0],1,t1.shape[1])\n",
    "            score = torch.sum(hr * t1, -1)\n",
    "            return score\n",
    "        elif len(r.shape) == 3:\n",
    "            assert (len(h.shape) == 2) & (len(t.shape) == 2)\n",
    "            # this is the relation prediction case\n",
    "            r, r_transfer, time_transfer = torch.chunk(r, 3, dim=-1)\n",
    "            h1 = self.eval_transfer(h, time_transfer, r_transfer)\n",
    "            t1 = self.eval_transfer(t, time_transfer, r_transfer)\n",
    "            hr = self.eval_calc(h1, r)\n",
    "            t1 = t1.view(t1.shape[0],1,hr.shape[1])# needs to be addapted\n",
    "            score = torch.sum(hr * t1, -1)# needs to be addapted\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx,start_time_idx, end_time_idx, entities=True):\n",
    "        b_size = h_idx.shape[0]\n",
    "\n",
    "        h = self.ent_emb(h_idx)\n",
    "        t = self.ent_emb(t_idx)\n",
    "        r = self.rel_emb(r_idx)\n",
    "        \n",
    "        time_transfer = self.time_transfer(start_time_idx)\n",
    "        r_transfer = self.rel_transfer(r_idx)\n",
    "\n",
    "        if entities:\n",
    "            all_ents = torch.tensor(range(self.n_ent)).to(device)\n",
    "            candidates = self.ent_emb(all_ents)\n",
    "            candidates = candidates.expand(b_size, self.n_ent, candidates.shape[-1])\n",
    "        else:\n",
    "            all_rels = torch.tensor(range(self.n_rel)).to(device)\n",
    "            candidates = self.rel_emb(all_rels)\n",
    "            candidates = candidates.expand(b_size, self.n_rel, candidates.shape[-1])\n",
    "        return h, t, torch.cat([r, r_transfer, time_transfer], dim = -1), candidates    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7327c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDYCE_Oct(TempBilinearModel):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args.emb_dim, args.n_entities, args.n_relations)\n",
    "        self.emb_dim = args.emb_dim\n",
    "        self.tem_total = args.tem_total\n",
    "        self.quat_emb_dim = self.emb_dim * 8\n",
    "        self.ent_emb = nn.Embedding(self.n_ent, self.quat_emb_dim)\n",
    "        self.rel_emb = nn.Embedding(self.n_rel, self.quat_emb_dim)\n",
    "        self.rel_transfer = nn.Embedding(self.n_rel, self.quat_emb_dim)\n",
    "        self.time_transfer = nn.Embedding(self.tem_total, self.quat_emb_dim)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        r, i, j, k, l, m, n, o = oct_init(self.n_ent, self.emb_dim)\n",
    "        r, i, j, k = torch.from_numpy(r), torch.from_numpy(i), torch.from_numpy(j), torch.from_numpy(k)\n",
    "        l, m, n, o = torch.from_numpy(l), torch.from_numpy(m), torch.from_numpy(n), torch.from_numpy(o)\n",
    "        vec1 = torch.cat([r, i, j, k, l, m, n, o], dim=1)\n",
    "        self.ent_emb.weight.data = vec1.type_as(self.ent_emb.weight.data)\n",
    "\n",
    "        s, x, y, z, l, m, n, o = oct_init(self.n_rel, self.emb_dim)\n",
    "        s, x, y, z = torch.from_numpy(s), torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(z)\n",
    "        l, m, n, o = torch.from_numpy(l), torch.from_numpy(m), torch.from_numpy(n), torch.from_numpy(o)\n",
    "        vec2 = torch.cat([s, x, y, z, l, m, n, o], dim=1)\n",
    "        self.rel_emb.weight.data = vec2.type_as(self.rel_emb.weight.data)\n",
    "        \n",
    "        r, i, j, k, l, m, n, o = oct_init(self.tem_total, self.emb_dim)\n",
    "        r, i, j, k = torch.from_numpy(r), torch.from_numpy(i), torch.from_numpy(j), torch.from_numpy(k)\n",
    "        l, m, n, o = torch.from_numpy(l), torch.from_numpy(m), torch.from_numpy(n), torch.from_numpy(o)\n",
    "        vec1 = torch.cat([r, i, j, k, l, m, n, o], dim=1)\n",
    "        self.time_transfer.weight.data = vec1.type_as(self.time_transfer.weight.data)\n",
    "\n",
    "        s, x, y, z, l, m, n, o = oct_init(self.n_rel, self.emb_dim)\n",
    "        s, x, y, z = torch.from_numpy(s), torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(z)\n",
    "        l, m, n, o = torch.from_numpy(l), torch.from_numpy(m), torch.from_numpy(n), torch.from_numpy(o)\n",
    "        vec2 = torch.cat([s, x, y, z, l, m, n, o], dim=1)\n",
    "        self.rel_transfer.weight.data = vec2.type_as(self.rel_transfer.weight.data)\n",
    "\n",
    "    def _calc(self, h, r):\n",
    "        return _omult(h, _onorm(r))\n",
    "\n",
    "    def eval_calc(self, h, r):\n",
    "        r = _onorm(r)\n",
    "        if (len(h.shape) == 3) & (len(r.shape) == 2):\n",
    "            r = r.view(r.shape[0], 1, r.shape[1])\n",
    "        elif (len(r.shape) == 3) & (len(h.shape) == 2):\n",
    "            r = r.view(r.shape[0], 1, r.shape[1])\n",
    "        return _omult(h, r)\n",
    "\n",
    "    def _transfer(self, x, x_transfer, r_transfer):\n",
    "        ent_transfer = self._calc(x, x_transfer)\n",
    "        ent_rel_transfer = self._calc(ent_transfer, r_transfer)\n",
    "\n",
    "        return ent_rel_transfer\n",
    "\n",
    "    def eval_transfer(self, x, x_transfer, r_transfer):\n",
    "        ent_transfer = self.eval_calc(x, x_transfer)\n",
    "        # print('ent_transfer.shape: ',ent_transfer.shape)\n",
    "        # print('r_transfer.shape: ',r_transfer.shape)\n",
    "        ent_rel_transfer = self.eval_calc(ent_transfer, r_transfer)\n",
    "\n",
    "        return ent_rel_transfer\n",
    "    \n",
    "    \n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        h = self.ent_emb(h_idx)\n",
    "        r = self.rel_emb(r_idx)\n",
    "        t = self.ent_emb(t_idx)\n",
    "\n",
    "        # (h, r, t) transfer vector\n",
    "        time_transfer = self.time_transfer(start_time_idx)\n",
    "        r_transfer = self.rel_transfer(r_idx)\n",
    "\n",
    "        h1 = self._transfer(h, time_transfer, r_transfer)\n",
    "        t1 = self._transfer(t, time_transfer, r_transfer)\n",
    "        # multiplication as QuatE\n",
    "        hr = self._calc(h1, r)\n",
    "        # Inner product as QuatE\n",
    "        score = quat_dot_prod(hr, t1)\n",
    "        return score\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        self.normalize_parameters()\n",
    "        return self.ent_emb.weight.data, self.rel_emb.weight.data\n",
    "\n",
    "    def inference_scoring_function(self, h, t, r, start_time, end_time):\n",
    "        b_size = h.shape[0]\n",
    "        emb_dim_ = 4*self.emb_dim\n",
    "        if len(t.shape) == 3:\n",
    "            assert (len(h.shape) == 2) & (len(r.shape) == 2)\n",
    "            r, r_transfer, time_transfer = torch.chunk(r, 3, dim=-1)\n",
    "            h1 = self.eval_transfer(h, time_transfer, r_transfer)\n",
    "            # print('t.shape: ',t.shape)\n",
    "            t1 = self.eval_transfer(t, time_transfer, r_transfer)\n",
    "            hr = self.eval_calc(h1, r)\n",
    "            hr = hr.view(hr.shape[0],1,hr.shape[1])\n",
    "            # print('t1.shape: ',t1.shape, ' hr.shape: ',hr.shape)\n",
    "            score = torch.sum(hr * t1, -1)\n",
    "            return score\n",
    "\n",
    "        elif len(h.shape) == 3:\n",
    "            assert (len(t.shape) == 2) & (len(r.shape) == 2)\n",
    "            # this is the head completion case in link prediction\n",
    "            r, r_transfer, time_transfer = torch.chunk(r, 3, dim=-1)\n",
    "            # this is the head completion case in link prediction\n",
    "            h1 = self.eval_transfer(h, time_transfer, r_transfer)\n",
    "            t1 = self.eval_transfer(t, time_transfer, r_transfer)\n",
    "            hr = self.eval_calc(h1, r)\n",
    "            # print('t1.shape: ',t1.shape, ' hr.shape: ',hr.shape)\n",
    "            t1 = t1.view(t1.shape[0],1,t1.shape[1])\n",
    "            score = torch.sum(hr * t1, -1)\n",
    "            return score\n",
    "        elif len(r.shape) == 3:\n",
    "            assert (len(h.shape) == 2) & (len(t.shape) == 2)\n",
    "            # this is the relation prediction case\n",
    "            r, r_transfer, time_transfer = torch.chunk(r, 3, dim=-1)\n",
    "            h1 = self.eval_transfer(h, time_transfer, r_transfer)\n",
    "            t1 = self.eval_transfer(t, time_transfer, r_transfer)\n",
    "            hr = self.eval_calc(h1, r)\n",
    "            t1 = t1.view(t1.shape[0],1,hr.shape[1])# needs to be addapted\n",
    "            score = torch.sum(hr * t1, -1)# needs to be addapted\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx,start_time_idx, end_time_idx, entities=True):\n",
    "        b_size = h_idx.shape[0]\n",
    "\n",
    "        h = self.ent_emb(h_idx)\n",
    "        t = self.ent_emb(t_idx)\n",
    "        r = self.rel_emb(r_idx)\n",
    "        \n",
    "        time_transfer = self.time_transfer(start_time_idx)\n",
    "        r_transfer = self.rel_transfer(r_idx)\n",
    "\n",
    "        if entities:\n",
    "            all_ents = torch.tensor(range(self.n_ent)).to(device)\n",
    "            candidates = self.ent_emb(all_ents)\n",
    "            candidates = candidates.expand(b_size, self.n_ent, candidates.shape[-1])\n",
    "        else:\n",
    "            all_rels = torch.tensor(range(self.n_rel)).to(device)\n",
    "            candidates = self.rel_emb(all_rels)\n",
    "            candidates = candidates.expand(b_size, self.n_rel, candidates.shape[-1])\n",
    "        return h, t, torch.cat([r, r_transfer, time_transfer], dim = -1), candidates    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a72bb9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, args):\n",
    "    if model_name == 'TDYCE_Quat':\n",
    "        return TDYCE_Quat(args)\n",
    "    elif model_name == 'TDYCE_Oct':\n",
    "        return TDYCE_Oct(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc450d42",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f3a0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class temporal_train_loop():\n",
    "    def __init__(self, kg_train, kg_val, kg_test, args):\n",
    "        self.args = args\n",
    "\n",
    "        self.kg_train = kg_train\n",
    "        self.kg_val = kg_val\n",
    "        self.kg_test = kg_test\n",
    "        self.n_epochs = args.n_epochs\n",
    "        self.model_name = args.model_name\n",
    "        self.emb_dim = args.emb_dim\n",
    "        self.lr = args.lr\n",
    "        self.n_epochs = args.n_epochs\n",
    "        self.b_size = args.b_size\n",
    "        self.margin = args.margin\n",
    "        self.model_save_name = args.model_save_name\n",
    "        self.device = device\n",
    "        self.model_path = args.model_path\n",
    "        self.n_neg = args.n_neg\n",
    "\n",
    "#         self.model = TransEModel(self.emb_dim, self.kg_train.n_ent, self.kg_train.n_rel, dissimilarity_type='L2')\n",
    "        self.model = get_model(args.model_name, args)\n",
    "     \n",
    "        self.criterion = get_loss(args.loss_name, args)\n",
    "\n",
    "        # Move everything to CUDA if available\n",
    "        if cuda.is_available():\n",
    "            cuda.empty_cache()\n",
    "            self.model = self.model.to(self.device)\n",
    "            if hasattr(self.criterion, 'to'):\n",
    "                self.criterion = self.criterion.to(self.device)\n",
    "\n",
    "        # Define the torch optimizer to be used\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
    "        # self.sampler = BernoulliNegativeSampler(self.kg_train, n_neg = self.n_neg)\n",
    "        self.sampler = UniformNegativeSampler(self.kg_train, n_neg = self.n_neg)\n",
    "        self.dataloader = TempDataLoader(self.kg_train, batch_size=self.b_size, use_cuda=None)\n",
    "\n",
    "    def save_model(self,):\n",
    "        torch.save(self.model.state_dict(), join(self.model_path,self.model_save_name))\n",
    "\n",
    "    def evaluation(self, ):\n",
    "        with torch.no_grad():\n",
    "            evaluator = TemporalLinkPredictionEvaluator(self.model, self.kg_val)\n",
    "            evaluator.evaluate(b_size=100, verbose=False)\n",
    "            val_res = evaluator.get_results()\n",
    "            val_filt_mrr = val_res['filt_mrr']\n",
    "            # print(val_filt_mrr)\n",
    "            return val_filt_mrr, val_res\n",
    "    \n",
    "    def test(self,):\n",
    "        with torch.no_grad():\n",
    "            self.model = self.model.to('cpu')\n",
    "            del self.model\n",
    "            torch.cuda.empty_cache()\n",
    "            self.model = get_model(model_name = self.model_name, args = self.args)\n",
    "            self.model.load_state_dict(torch.load(join(self.model_path,self.model_save_name)))\n",
    "            os.remove(join(self.model_path,self.model_save_name))\n",
    "            self.model = self.model.to(device=device)\n",
    "            evaluator = TemporalLinkPredictionEvaluator(self.model, self.kg_test)\n",
    "            evaluator.evaluate(b_size=32)\n",
    "            test_res = evaluator.get_results()\n",
    "            return test_res\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        iterator = tqdm(range(self.n_epochs), unit='epoch')\n",
    "        eval_epoch = 1\n",
    "        best_val_mrr = -100\n",
    "        best_epoch = 0\n",
    "        patience_max = 9\n",
    "        patience_counter = 0\n",
    "        train_evolution = []\n",
    "\n",
    "        for epoch in iterator:\n",
    "            running_loss = 0.0\n",
    "            epoch_log = {}\n",
    "            for i, batch in enumerate(self.dataloader):\n",
    "                h, t, r, start_time, end_time = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n",
    "                # print('h.shape ooooo ',h.shape)\n",
    "                n_h, n_t = self.sampler.corrupt_batch(h, t, r)                \n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                pos, neg = self.model(h, t, r, start_time, end_time, n_h, n_t)\n",
    "                loss = self.criterion(pos, neg)\n",
    "#                 loss = self.criterion(pos, neg) + self.model.get_reg_loss(h, t, r, start_time, end_time)\n",
    "                \n",
    "                if hasattr(self.args, 'add_sin_constaint'):\n",
    "                    if self.args.add_sin_constaint:\n",
    "                        loss =loss + 20*self.model.get_constraints(h, start_time, end_time)\n",
    "                        loss =loss + 20*self.model.get_constraints(t, start_time, end_time)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "            epoch_log['avg_train_loss'] = running_loss / len(self.dataloader)\n",
    "            if epoch % eval_epoch == 0:\n",
    "                val_mrr,  val_res = self.evaluation()\n",
    "                epoch_log['val_res'] = val_res\n",
    "                if (best_val_mrr < val_mrr) & (val_mrr<float('inf')):\n",
    "                    best_val_mrr = val_mrr\n",
    "                    best_epoch = epoch + 1\n",
    "                    # save the model\n",
    "                    self.save_model()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter == patience_max:\n",
    "                        break\n",
    "            train_evolution.append(epoch_log)\n",
    "            iterator.set_description(\n",
    "                'Epoch {} | mean loss: {:.5f}| best mrr: {:.5f} | Best Epoch {} '.format(epoch + 1,\n",
    "                                                      running_loss / len(self.dataloader), best_val_mrr, best_epoch))\n",
    "\n",
    "        # self.model.normalize_parameters()\n",
    "        return train_evolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e613d6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9ddc68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:500$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "True\n",
      "dataset:gdelt$batch_size:1000$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n",
      "False\n",
      "dataset:gdelt$batch_size:500$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62c7284a4f84cb7810f5a5da064d172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2f2ebae6ba4f93bff6305114c6ba58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Link prediction evaluation:   0%|          | 0/10687 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_1': 0.07013372704386711, 'filt_hit_1': 0.08510619401931763, 'hit_2': 0.1282324567437172, 'filt_hit_2': 0.1480812132358551, 'hit_3': 0.1694842427968979, 'filt_hit_3': 0.18957863748073578, 'hit_4': 0.20179640501737595, 'filt_hit_4': 0.22137758880853653, 'hit_5': 0.2287994772195816, 'filt_hit_5': 0.24795664101839066, 'hit_6': 0.2524571567773819, 'filt_hit_6': 0.2709270864725113, 'hit_7': 0.2733805924654007, 'filt_hit_7': 0.2911267578601837, 'hit_8': 0.2920347601175308, 'filt_hit_8': 0.3092428594827652, 'hit_9': 0.309162437915802, 'filt_hit_9': 0.32582223415374756, 'hit_10': 0.3247402459383011, 'filt_hit_10': 0.34128306806087494, 'mr': 67.00736236572266, 'filt_mr': 65.19158935546875, 'head_mrr': 0.15534041821956635, 'tail_mrr': 0.15721067786216736, 'filt_head_mrr': 0.17211297154426575, 'filt_tail_mrr': 0.17342403531074524, 'mrr': 0.15627554059028625, 'filt_mrr': 0.1727685034275055}\n",
      "False\n",
      "dataset:gdelt$batch_size:500$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9cdad3d4f04fbfa63a5ad332b9b8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f8d34a68624f43a3c4b3f25087a51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Link prediction evaluation:   0%|          | 0/10687 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_1': 0.06756911054253578, 'filt_hit_1': 0.08110573887825012, 'hit_2': 0.12263533100485802, 'filt_hit_2': 0.14163603633642197, 'hit_3': 0.16328030824661255, 'filt_hit_3': 0.18254128843545914, 'hit_4': 0.19601942598819733, 'filt_hit_4': 0.21471454203128815, 'hit_5': 0.22294501215219498, 'filt_hit_5': 0.24121464043855667, 'hit_6': 0.24595202505588531, 'filt_hit_6': 0.26345986127853394, 'hit_7': 0.2661575376987457, 'filt_hit_7': 0.28322674334049225, 'hit_8': 0.28464357554912567, 'filt_hit_8': 0.3009231984615326, 'hit_9': 0.3012448698282242, 'filt_hit_9': 0.3170390725135803, 'hit_10': 0.3166808485984802, 'filt_hit_10': 0.3318609446287155, 'mr': 69.09821319580078, 'filt_mr': 67.28962707519531, 'head_mrr': 0.15038815140724182, 'tail_mrr': 0.15290677547454834, 'filt_head_mrr': 0.1659112274646759, 'filt_tail_mrr': 0.16812165081501007, 'mrr': 0.15164746344089508, 'filt_mrr': 0.16701644659042358}\n",
      "False\n",
      "dataset:gdelt$batch_size:1000$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2400eda7eb6e4bb1a46f18765af1b2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4896250ce1624c68b0d8ac7f22ac4b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Link prediction evaluation:   0%|          | 0/10687 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_1': 0.07182690501213074, 'filt_hit_1': 0.08734621480107307, 'hit_2': 0.12903665006160736, 'filt_hit_2': 0.14902430027723312, 'hit_3': 0.17061448097229004, 'filt_hit_3': 0.19067379087209702, 'hit_4': 0.20256111025810242, 'filt_hit_4': 0.22252537310123444, 'hit_5': 0.22938726842403412, 'filt_hit_5': 0.24856343120336533, 'hit_6': 0.25291626155376434, 'filt_hit_6': 0.27137450873851776, 'hit_7': 0.2736525535583496, 'filt_hit_7': 0.2915741801261902, 'hit_8': 0.29225847125053406, 'filt_hit_8': 0.3091990053653717, 'hit_9': 0.3090732544660568, 'filt_hit_9': 0.3257725238800049, 'hit_10': 0.3246861547231674, 'filt_hit_10': 0.3409014493227005, 'mr': 67.32084655761719, 'filt_mr': 65.50654602050781, 'head_mrr': 0.15645036101341248, 'tail_mrr': 0.15829221904277802, 'filt_head_mrr': 0.17348314821720123, 'filt_tail_mrr': 0.1748523712158203, 'mrr': 0.15737128257751465, 'filt_mrr': 0.17416775226593018}\n",
      "False\n",
      "dataset:gdelt$batch_size:1000$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c98262b85c49cc8140ffef19b11c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518e520fe99049c68148555d6416941b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Link prediction evaluation:   0%|          | 0/10687 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_1': 0.06606016680598259, 'filt_hit_1': 0.08015680313110352, 'hit_2': 0.12376557663083076, 'filt_hit_2': 0.14305578917264938, 'hit_3': 0.1649237722158432, 'filt_hit_3': 0.18472720682621002, 'hit_4': 0.1978924497961998, 'filt_hit_4': 0.2171490341424942, 'hit_5': 0.22522743791341782, 'filt_hit_5': 0.24400443583726883, 'hit_6': 0.2491307482123375, 'filt_hit_6': 0.2672029882669449, 'hit_7': 0.27015215158462524, 'filt_hit_7': 0.2876804620027542, 'hit_8': 0.2889963984489441, 'filt_hit_8': 0.30612555146217346, 'hit_9': 0.3063009977340698, 'filt_hit_9': 0.32272832095623016, 'hit_10': 0.3219928592443466, 'filt_hit_10': 0.3380487859249115, 'mr': 66.73566436767578, 'filt_mr': 64.9295654296875, 'head_mrr': 0.15105731785297394, 'tail_mrr': 0.1537083387374878, 'filt_head_mrr': 0.16719938814640045, 'filt_tail_mrr': 0.1693025827407837, 'mrr': 0.15238282084465027, 'filt_mrr': 0.16825097799301147}\n",
      "False\n",
      "dataset:gdelt$batch_size:500$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b95b34677d41b6a408d01f913459aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ba79c4c6ee4cd1b8dc05176adc15ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Link prediction evaluation:   0%|          | 0/10687 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_1': 0.06959126889705658, 'filt_hit_1': 0.08419527485966682, 'hit_2': 0.1271972581744194, 'filt_hit_2': 0.14749488979578018, 'hit_3': 0.168726846575737, 'filt_hit_3': 0.1890566498041153, 'hit_4': 0.20132704824209213, 'filt_hit_4': 0.22121528536081314, 'hit_5': 0.2289164513349533, 'filt_hit_5': 0.24816864728927612, 'hit_6': 0.25271742045879364, 'filt_hit_6': 0.2711668908596039, 'hit_7': 0.2732899338006973, 'filt_hit_7': 0.2912159562110901, 'hit_8': 0.2916940748691559, 'filt_hit_8': 0.30910980701446533, 'hit_9': 0.30874134600162506, 'filt_hit_9': 0.3254581689834595, 'hit_10': 0.324459508061409, 'filt_hit_10': 0.3405826985836029, 'mr': 67.0080795288086, 'filt_mr': 65.19364929199219, 'head_mrr': 0.15436570346355438, 'tail_mrr': 0.15704669058322906, 'filt_head_mrr': 0.17108747363090515, 'filt_tail_mrr': 0.173162043094635, 'mrr': 0.15570619702339172, 'filt_mrr': 0.17212475836277008}\n",
      "False\n",
      "dataset:gdelt$batch_size:500$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2887e1a3aa94028a6c1d4fbd6e49d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77eea17782ac4c418cde6a23b533930b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Link prediction evaluation:   0%|          | 0/10687 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_1': 0.0706908106803894, 'filt_hit_1': 0.08508426323533058, 'hit_2': 0.12372755631804466, 'filt_hit_2': 0.14243437349796295, 'hit_3': 0.16370433568954468, 'filt_hit_3': 0.18221083283424377, 'hit_4': 0.19449293613433838, 'filt_hit_4': 0.2129511833190918, 'hit_5': 0.22120943665504456, 'filt_hit_5': 0.2382698580622673, 'hit_6': 0.24359503388404846, 'filt_hit_6': 0.25985127687454224, 'hit_7': 0.2630724012851715, 'filt_hit_7': 0.2788783013820648, 'hit_8': 0.2808975279331207, 'filt_hit_8': 0.2963188737630844, 'hit_9': 0.29689057171344757, 'filt_hit_9': 0.31211893260478973, 'hit_10': 0.31183235347270966, 'filt_hit_10': 0.3265094608068466, 'mr': 71.15974426269531, 'filt_mr': 69.35005187988281, 'head_mrr': 0.15103180706501007, 'tail_mrr': 0.15394246578216553, 'filt_head_mrr': 0.16671222448349, 'filt_tail_mrr': 0.16934765875339508, 'mrr': 0.1524871289730072, 'filt_mrr': 0.16802993416786194}\n",
      "False\n",
      "dataset:gdelt$batch_size:1000$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342e9ca81afe4bb8b82a29e3f2658d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748d532f285b4f668614bed8987f447b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Link prediction evaluation:   0%|          | 0/10687 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_1': 0.07439737021923065, 'filt_hit_1': 0.09023835882544518, 'hit_2': 0.13002213835716248, 'filt_hit_2': 0.14980655908584595, 'hit_3': 0.17038054764270782, 'filt_hit_3': 0.19041059911251068, 'hit_4': 0.2019323855638504, 'filt_hit_4': 0.22150187194347382, 'hit_5': 0.22837838530540466, 'filt_hit_5': 0.2472606524825096, 'hit_6': 0.25140586495399475, 'filt_hit_6': 0.26934942603111267, 'hit_7': 0.27201932668685913, 'filt_hit_7': 0.289187952876091, 'hit_8': 0.2898561656475067, 'filt_hit_8': 0.30648963153362274, 'hit_9': 0.3060012608766556, 'filt_hit_9': 0.322228267788887, 'hit_10': 0.32131148874759674, 'filt_hit_10': 0.33707059919834137, 'mr': 69.29116821289062, 'filt_mr': 67.47924041748047, 'head_mrr': 0.1573161631822586, 'tail_mrr': 0.1588321030139923, 'filt_head_mrr': 0.17435231804847717, 'filt_tail_mrr': 0.1754358559846878, 'mrr': 0.15807414054870605, 'filt_mrr': 0.17489409446716309}\n",
      "False\n",
      "dataset:gdelt$batch_size:1000$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d026e0b88ebe46c1bd64bdb32e0da803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21f265a53404b8995d2b6cb05669e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Link prediction evaluation:   0%|          | 0/10687 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_1': 0.06194419786334038, 'filt_hit_1': 0.07454358786344528, 'hit_2': 0.11555849760770798, 'filt_hit_2': 0.13236597180366516, 'hit_3': 0.15508200973272324, 'filt_hit_3': 0.1723676100373268, 'hit_4': 0.18607238680124283, 'filt_hit_4': 0.2038346454501152, 'hit_5': 0.21285176277160645, 'filt_hit_5': 0.22989170998334885, 'hit_6': 0.23545082658529282, 'filt_hit_6': 0.2519015222787857, 'hit_7': 0.2555364519357681, 'filt_hit_7': 0.27157337963581085, 'hit_8': 0.2738455533981323, 'filt_hit_8': 0.28960759937763214, 'hit_9': 0.29056091606616974, 'filt_hit_9': 0.30593253672122955, 'hit_10': 0.30602027475833893, 'filt_hit_10': 0.32090793550014496, 'mr': 69.86566925048828, 'filt_mr': 68.05122375488281, 'head_mrr': 0.14379990100860596, 'tail_mrr': 0.1454724818468094, 'filt_head_mrr': 0.15826843678951263, 'filt_tail_mrr': 0.15944348275661469, 'mrr': 0.14463618397712708, 'filt_mrr': 0.15885595977306366}\n",
      "False\n",
      "dataset:gdelt$batch_size:500$lr:0.001$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a40cac1fd7c45b683022495dd8100ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8700a08edbf044c9bbfa092c5664c94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Link prediction evaluation:   0%|          | 0/10687 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_1': 0.07171870395541191, 'filt_hit_1': 0.08642798662185669, 'hit_2': 0.12860091775655746, 'filt_hit_2': 0.14817771315574646, 'hit_3': 0.16941697895526886, 'filt_hit_3': 0.18915022164583206, 'hit_4': 0.20126856863498688, 'filt_hit_4': 0.2209111526608467, 'hit_5': 0.22813566029071808, 'filt_hit_5': 0.24720655381679535, 'hit_6': 0.2518152743577957, 'filt_hit_6': 0.2700074017047882, 'hit_7': 0.27241410315036774, 'filt_hit_7': 0.2900681644678116, 'hit_8': 0.2909673899412155, 'filt_hit_8': 0.3082837015390396, 'hit_9': 0.3079225420951843, 'filt_hit_9': 0.3248031288385391, 'hit_10': 0.32352666556835175, 'filt_hit_10': 0.3399510383605957, 'mr': 67.87479400634766, 'filt_mr': 66.06599426269531, 'head_mrr': 0.15553192794322968, 'tail_mrr': 0.15803104639053345, 'filt_head_mrr': 0.1719973236322403, 'filt_tail_mrr': 0.1740785837173462, 'mrr': 0.15678149461746216, 'filt_mrr': 0.17303794622421265}\n",
      "False\n",
      "dataset:gdelt$batch_size:500$lr:0.01$emb_dim:100$model_name:TDYCE_Quat$dissimilarity_type:.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8b924dcb0d447282a0e1f943d0a131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2ccf4889084a2ea8d9f48d44d60671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Link prediction evaluation:   0%|          | 0/10687 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_1': 0.05980798974633217, 'filt_hit_1': 0.07190147414803505, 'hit_2': 0.11098195612430573, 'filt_hit_2': 0.1264413222670555, 'hit_3': 0.14790721237659454, 'filt_hit_3': 0.16406110674142838, 'hit_4': 0.17832297086715698, 'filt_hit_4': 0.19465523958206177, 'hit_5': 0.20444583147764206, 'filt_hit_5': 0.22060702741146088, 'hit_6': 0.22683434188365936, 'filt_hit_6': 0.2425057217478752, 'hit_7': 0.24668894708156586, 'filt_hit_7': 0.2619713842868805, 'hit_8': 0.2646617591381073, 'filt_hit_8': 0.2794061303138733, 'hit_9': 0.28096917271614075, 'filt_hit_9': 0.29530707001686096, 'hit_10': 0.29586707055568695, 'filt_hit_10': 0.31009677052497864, 'mr': 72.37879943847656, 'filt_mr': 70.55870056152344, 'head_mrr': 0.13890808820724487, 'tail_mrr': 0.14091819524765015, 'filt_head_mrr': 0.15260377526283264, 'filt_tail_mrr': 0.1541258841753006, 'mrr': 0.1399131417274475, 'filt_mrr': 0.15336483716964722}\n"
     ]
    }
   ],
   "source": [
    "res_save_file = 'TDYCE_experiments_log.jsonl'\n",
    "if os.path.exists(res_save_file):\n",
    "    all_exp_df = check_experiment(res_save_file)\n",
    "else:\n",
    "    all_exp_df = None\n",
    "args = Params()\n",
    "\n",
    "args.n_epochs = 1000\n",
    "# args.margin = 200\n",
    "args.model_save_name = 'best_de_model_.bt'\n",
    "args.loss_name = 'MarginLoss'\n",
    "# args.loss_name = 'BinaryCrossEntropyLoss'\n",
    "args.model_path = 'best_models'\n",
    "args.time_mode = 'simple'\n",
    "# args.n_neg = 10\n",
    "# args.flag_hamilton_mul_norm = False\n",
    "# args.input_dropout = .3\n",
    "# args.hidden_dropout = .3\n",
    "# 0.36\n",
    "# args.time_proj_method = 'tero'\n",
    "def get_model_name(dataset,batch_size , lr , emb_dim , model_name , dissimilarity_type):\n",
    "    vals = [dataset,batch_size , lr , emb_dim , model_name , dissimilarity_type]\n",
    "    keys = ['dataset','batch_size' , 'lr' , 'emb_dim' , 'model_name' , 'dissimilarity_type']\n",
    "    kv = []\n",
    "    for k,  v in zip(keys,vals):\n",
    "        kv.append(':'.join([str(k),  str(v)]))\n",
    "    kv = '$'.join(kv)\n",
    "    kv = kv+'.pt'\n",
    "    return kv\n",
    "\n",
    "args.time_proj_method = 'ent'#ent_rel \n",
    "for model_name in ['TDYCE_Quat']: # TDYCE_Oct TDYCE_Quat\n",
    "    args.model_name = model_name\n",
    "    for dataset in ['gdelt']:# icews14 gdelt wikidata12k icews15\n",
    "        args.dataset = dataset\n",
    "        kg_train, kg_val, kg_test = get_data(args.dataset, args.time_mode)\n",
    "        args.n_entities = kg_train.n_ent\n",
    "        args.n_relations = kg_train.n_rel\n",
    "        args.tem_total = kg_train.n_time\n",
    "        for n_neg in [5,2,10,1]:\n",
    "            args.n_neg = n_neg\n",
    "            for margin in [200,100,10]:\n",
    "                args.margin = margin\n",
    "                for b_size in [1000, 500]:\n",
    "                    # 500, 1000, 2000\n",
    "                    args.b_size = b_size\n",
    "                    for lr in [0.001, 0.01]:\n",
    "                        args.lr = lr\n",
    "                        for emb_dim in [100]:\n",
    "                            args.emb_dim = emb_dim\n",
    "                            args.num_features = args.emb_dim\n",
    "\n",
    "                            if args.model_name in ['TeRoTransEModel']:\n",
    "                                dissimilarity_type_list = ['L1']\n",
    "                                # 'L2', 'L1'\n",
    "                            elif args.model_name in ['toruse']:\n",
    "                                dissimilarity_type_list = ['L1', 'torus_L1', 'torus_L2', 'torus_eL2']\n",
    "                            else:\n",
    "                                dissimilarity_type_list = ['']\n",
    "\n",
    "                            for dissimilarity_type in dissimilarity_type_list:\n",
    "                                if all_exp_df is not None:\n",
    "                                    exp_already_done = experiment_exists(all_exp_data_df = all_exp_df, exp_data_dict = args.__dict__)\n",
    "                                else:\n",
    "                                    exp_already_done = False\n",
    "                                model_save_name = get_model_name(dataset, args.b_size , lr , emb_dim , args.model_name , dissimilarity_type)\n",
    "                                print(exp_already_done)\n",
    "                                print(model_save_name)\n",
    "#                                 exp_already_done = False\n",
    "                                if not exp_already_done:\n",
    "                                    args.dissimilarity_type = dissimilarity_type\n",
    "                                    tmp_trainlp = temporal_train_loop(kg_train, kg_val, kg_test, args)\n",
    "                                    train_evolution = tmp_trainlp.run()\n",
    "                                    test_res = tmp_trainlp.test()\n",
    "                                    train_log = {}\n",
    "                                    print(test_res)\n",
    "                                    train_log['train_params'] = args.__dict__\n",
    "                                    train_log['train_evolution'] = train_evolution\n",
    "                                    train_log['test_results'] = test_res\n",
    "                                    write_json_lines(file_name =res_save_file, dict_data = train_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694c729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7632013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd31998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
