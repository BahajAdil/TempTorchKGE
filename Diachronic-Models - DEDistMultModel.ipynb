{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d5cff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pctp/anaconda3/envs/py38_3/lib/python3.8/site-packages/torchkge/utils/data_redundancy.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import empty, zeros, cat\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchkge.data_structures import SmallKG\n",
    "from torchkge.exceptions import NotYetEvaluatedError\n",
    "from torchkge.sampling import PositionalNegativeSampler\n",
    "from torchkge.utils import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch\n",
    "import os\n",
    "from os.path import join\n",
    "# from evaluation import TemporalLinkPredictionEvaluator\n",
    "\n",
    "from torchkge.utils import datasets\n",
    "import torch\n",
    "from pandas import read_csv, concat\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from torchkge.utils import datasets\n",
    "import torch\n",
    "from pandas import read_csv, concat\n",
    "# from data_utils import TemporalKnowledgeGraph\n",
    "from numpy.random import RandomState\n",
    "\n",
    "from torch import tensor, bernoulli, randint, ones, rand, cat\n",
    "from torchkge.utils import init_embedding\n",
    "from torch.nn.functional import normalize\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d95459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from os import environ, makedirs\n",
    "from os.path import exists, expanduser, join\n",
    "from torch.utils.data import Dataset\n",
    "from torchkge.exceptions import SizeMismatchError, WrongArgumentsError, SanityError\n",
    "from torchkge.utils.operations import get_dictionaries\n",
    "from torch import cat, eq, int64, long, randperm, tensor, Tensor, zeros_like\n",
    "from collections import defaultdict\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "# from utils import get_temporal_dictionaries\n",
    "from pandas import read_csv, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c8a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.optim import Adam\n",
    "from torchkge.sampling import BernoulliNegativeSampler, UniformNegativeSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b089c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkge.utils import MarginLoss, BinaryCrossEntropyLoss, LogisticLoss\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from abc import ABC, abstractmethod\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a94ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "from torchkge.utils.dissimilarities import l1_dissimilarity, l2_dissimilarity, \\\n",
    "    l1_torus_dissimilarity, l2_torus_dissimilarity, el2_torus_dissimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "721167df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from torch import tensor, bernoulli, randint, ones, rand, cat\n",
    "\n",
    "from torchkge.exceptions import NotYetImplementedError\n",
    "from torchkge.utils.data import DataLoader\n",
    "from torchkge.utils.operations import get_bernoulli_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e2b79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b3ffc",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5e58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tem_dict = {\n",
    "    '0y': 0, '1y': 1, '2y': 2, '3y': 3, '4y': 4, '5y': 5, '6y': 6, '7y': 7, '8y': 8, '9y': 9,\n",
    "    '0m': 10, '1m': 11, '2m': 12, '3m': 13, '4m': 14, '5m': 15, '6m': 16, '7m': 17, '8m': 18, '9m': 19,\n",
    "    '0d': 20, '1d': 21, '2d': 22, '3d': 23, '4d': 24, '5d': 25, '6d': 26, '7d': 27, '8d': 28, '9d': 29\n",
    "}\n",
    "\n",
    "class Params():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_expriment_tests(results):\n",
    "    res = []\n",
    "    for exp in results:\n",
    "        train_params = exp['train_params']\n",
    "        test_results = exp['test_results']\n",
    "        train_params.update(test_results)\n",
    "        res.append(train_params)\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "def read_json_lines(file_name):\n",
    "    lines = []\n",
    "    with open(file_name) as file_in:\n",
    "        for line in file_in:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "def write_json_lines(file_name,dict_data):\n",
    "    json_string = json.dumps(dict_data)\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(json_string+\"\\n\")\n",
    "\n",
    "def experiment_exists(all_exp_data_df, exp_data_dict, important_fields = None):\n",
    "    if important_fields is None:\n",
    "        search_keys = exp_data_dict\n",
    "    else:\n",
    "        search_keys = important_fields\n",
    "    for k in search_keys:\n",
    "        all_exp_data_df = all_exp_data_df[all_exp_data_df[k]==exp_data_dict[k]]\n",
    "        if all_exp_data_df.shape[0]==0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def check_experiment(exp_file_name):\n",
    "    res = read_json_lines(exp_file_name)\n",
    "    res = get_expriment_tests(res)\n",
    "    return res\n",
    "\n",
    "def transform_time_V2(years, months, days):\n",
    "    all_data = []\n",
    "    for year, month, day in zip(years, months, days):\n",
    "        tem_id_list = []\n",
    "        for j in range(len(year)):\n",
    "            token = year[j:j+1]+'y'\n",
    "            tem_id_list.append(tem_dict[token])\n",
    "        # print(tem_id_list)\n",
    "        # exit()\n",
    "\n",
    "        for j in range(1):\n",
    "            # print(month[1])\n",
    "            # exit()\n",
    "            token1 = month[0]+'m'\n",
    "            tem_id_list.append(tem_dict[token1])\n",
    "            token2 = month[0]+'m'\n",
    "            tem_id_list.append(tem_dict[token2])\n",
    "\n",
    "\n",
    "        for j in range(len(day)):\n",
    "            token = day[j:j+1]+'d'\n",
    "            tem_id_list.append(tem_dict[token])\n",
    "            \n",
    "        all_data.append(torch.tensor(tem_id_list))\n",
    "    return all_data\n",
    "\n",
    "def transform_time(raw_time):\n",
    "    year, month, day = raw_time.split(\"-\")\n",
    "    tem_id_list = []\n",
    "    for j in range(len(year)):\n",
    "        token = year[j:j+1]+'y'\n",
    "        tem_id_list.append(tem_dict[token])\n",
    "    # print(tem_id_list)\n",
    "    # exit()\n",
    "\n",
    "    for j in range(1):\n",
    "        # print(month[1])\n",
    "        # exit()\n",
    "        token1 = month[0]+'m'\n",
    "        tem_id_list.append(tem_dict[token1])\n",
    "        token2 = month[0]+'m'\n",
    "        tem_id_list.append(tem_dict[token2])\n",
    "\n",
    "\n",
    "    for j in range(len(day)):\n",
    "        token = day[j:j+1]+'d'\n",
    "        tem_id_list.append(tem_dict[token])\n",
    "    return tem_id_list\n",
    "\n",
    "def transform_time_v3(raw_time):\n",
    "    date = list(map(float, raw_time.split(\"-\")))\n",
    "    return date\n",
    "\n",
    "def transform_time_v4(raw_time):\n",
    "    year, month, day = raw_time.split(\"-\")\n",
    "    year, month, day = int(year), int(month), int(day)\n",
    "    return month + day\n",
    "\n",
    "def transform_time_v5(raw_time):\n",
    "    year, month, day = raw_time.split(\"-\")\n",
    "    return [int(year), int(month), int(day)]\n",
    "\n",
    "def get_temporal_dictionaries(df, mode='simple'):\n",
    "\n",
    "    tmp = list(set(df['start_time'].unique()).union(set(df['end_time'].unique())))\n",
    "    if mode == 'simple_time':\n",
    "        # return {timee: i for i, timee in enumerate(sorted(tmp))}\n",
    "        idx = {timee: i for i, timee in enumerate(sorted([datetime.strptime(dt, \"%Y-%m-%d\") for dt in tmp]))}\n",
    "        time_trans = torch.tensor(list(idx.values()))\n",
    "        return idx, time_trans\n",
    "    elif mode == 'simple':\n",
    "        # return {timee: i for i, timee in enumerate(sorted(tmp))}\n",
    "        idx = {timee: i for i, timee in enumerate(sorted([dt for dt in tmp]))}\n",
    "        time_trans = torch.tensor(list(idx.values()))\n",
    "        return idx, time_trans\n",
    "    elif mode == 'seq':\n",
    "        idx = {timee: i for i, timee in enumerate(sorted(tmp))}\n",
    "        time_trans = torch.vstack([torch.tensor(transform_time(timee)) for timee in sorted(tmp)])\n",
    "        return idx, time_trans\n",
    "        # return {timee: torch.tensor(transform_time(timee)) for i, timee in enumerate(sorted([datetime.strptime(dt, \"%Y-%m-%d\") for dt in tmp]))}\n",
    "    elif mode == 'ymd':\n",
    "        idx = {timee: transform_time_v4(timee) for i, timee in enumerate(sorted(tmp))}\n",
    "        time_trans = torch.vstack([transform_time_v4(timee) for timee in sorted(tmp)])\n",
    "        return \n",
    "    elif mode == 'ymd_':\n",
    "#         idx = {timee: torch.tensor(transform_time_v5(timee)) for i, timee in enumerate(sorted(tmp))}\n",
    "        idx = {timee: i for i, timee in enumerate(sorted(tmp))}\n",
    "        time_trans = torch.vstack([torch.tensor(transform_time_v5(timee)) for timee in sorted(tmp)])\n",
    "        return idx, time_trans\n",
    "\n",
    "def cconv(a, b):\n",
    "    return torch.fft.ifft(torch.fft.fft(a) * torch.fft.fft(b)).real\n",
    "\n",
    "\n",
    "def ccorr(a, b):\n",
    "    return torch.fft.ifft(torch.conj(torch.fft.fft(a)) * torch.fft.fft(b)).real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61935a69",
   "metadata": {},
   "source": [
    "##  Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ca6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalKnowledgeGraph(Dataset):\n",
    "\n",
    "    def __init__(self, df=None, time_mode = None,kg=None, ent2ix=None, rel2ix=None, time2ix = None,\n",
    "                 time_trans = None,\n",
    "                 dict_of_heads=None, dict_of_tails=None, dict_of_rels=None,\n",
    "                 temp_dict_of_heads=None, temp_dict_of_tails=None, temp_dict_of_rels=None):\n",
    "\n",
    "        if df is None:\n",
    "            if kg is None:\n",
    "                raise WrongArgumentsError(\"Please provide at least one \"\n",
    "                                          \"argument of `df` and kg`\")\n",
    "            else:\n",
    "                try:\n",
    "                    assert (type(kg) == dict) & ('heads' in kg.keys()) & \\\n",
    "                           ('tails' in kg.keys()) & \\\n",
    "                           ('relations' in kg.keys())& \\\n",
    "                            ('start_time' in kg.keys())& \\\n",
    "                            ('end_time' in kg.keys())\n",
    "                    \n",
    "                except AssertionError:\n",
    "                    raise WrongArgumentsError(\"Keys in the `kg` dict should \"\n",
    "                                              \"contain `heads`, `tails`, \"\n",
    "                                              \"`relations`.\")\n",
    "                try:\n",
    "                    assert (rel2ix is not None) & (ent2ix is not None)\n",
    "                except AssertionError:\n",
    "                    raise WrongArgumentsError(\"Please provide the two \"\n",
    "                                              \"dictionaries ent2ix and rel2ix \"\n",
    "                                              \"if building from `kg`.\")\n",
    "        else:\n",
    "            if kg is not None:\n",
    "                raise WrongArgumentsError(\"`df` and kg` arguments should not \"\n",
    "                                          \"both be provided.\")\n",
    "\n",
    "        if ent2ix is None:\n",
    "            self.ent2ix = get_dictionaries(df, ent=True)\n",
    "        else:\n",
    "            self.ent2ix = ent2ix\n",
    "\n",
    "        if rel2ix is None:\n",
    "            self.rel2ix = get_dictionaries(df, ent=False)\n",
    "        else:\n",
    "            self.rel2ix = rel2ix\n",
    "            \n",
    "        if time_mode is not None:\n",
    "            self.time_mode = time_mode\n",
    "            \n",
    "        if time2ix is None:\n",
    "            self.time2ix, self.time_trans = get_temporal_dictionaries(df, mode = self.time_mode)\n",
    "        else:\n",
    "            self.time2ix, self.time_trans = time2ix, time_trans\n",
    "        \n",
    "        self.n_ent = max(self.ent2ix.values()) + 1\n",
    "        self.n_rel = max(self.rel2ix.values()) + 1\n",
    "        time_val = list(self.time2ix.values())\n",
    "        \n",
    "        if isinstance(time_val[0], torch.Tensor):\n",
    "            self.n_time = int(torch.cat(time_val).max()) + 1\n",
    "        else: \n",
    "            self.n_time = max(time_val) + 1\n",
    "            \n",
    "#         print('self.n_time: ',self.n_time)\n",
    "        if df is not None:\n",
    "            # build kg from a pandas dataframe\n",
    "            self.n_facts = len(df)\n",
    "            self.head_idx = tensor(df['from'].map(self.ent2ix).values).long()\n",
    "            self.tail_idx = tensor(df['to'].map(self.ent2ix).values).long()\n",
    "            self.relations = tensor(df['rel'].map(self.rel2ix).values).long()\n",
    "#             self.start_time = tensor(df['start_time'].map(self.rel2ix).values).long()\n",
    "#             self.end_time = tensor(df['end_time'].map(self.rel2ix).values).long()\n",
    "#             print(self.time2ix)\n",
    "            self.start_time = list(df['start_time'].map(self.time2ix).values)\n",
    "#             print(self.start_time)\n",
    "#             print('-'*22)\n",
    "#             print(\"type(self.start_time[0]): \",type(self.start_time[0]))\n",
    "#             print('type(self.start_time): ',type(self.start_time))\n",
    "#             if isinstance(self.start_time[0], torch.Tensor):\n",
    "#                 print('llllll')\n",
    "            if isinstance(self.start_time, list) & isinstance(self.start_time[0], torch.Tensor):\n",
    "                self.start_time = torch.stack(self.start_time)\n",
    "            else:\n",
    "                self.start_time = torch.tensor(self.start_time)\n",
    "            \n",
    "            self.start_time = self.start_time.long()\n",
    "            \n",
    "            self.end_time = list(df['end_time'].map(self.time2ix).values)\n",
    "            if isinstance(self.end_time, list) & isinstance(self.end_time[0], torch.Tensor):\n",
    "                self.end_time = torch.stack(self.end_time)\n",
    "            else:\n",
    "                self.end_time = torch.tensor(self.end_time)   \n",
    "            self.end_time = self.end_time.long()\n",
    "            \n",
    "        else:\n",
    "            # build kg from another kg\n",
    "            self.n_facts = kg['heads'].shape[0]\n",
    "            self.head_idx = kg['heads']\n",
    "            self.tail_idx = kg['tails']\n",
    "            self.relations = kg['relations']\n",
    "            self.start_time = kg['start_time']\n",
    "            self.end_time = kg['end_time']\n",
    "\n",
    "        if dict_of_heads is None or dict_of_tails is None or dict_of_rels is None:\n",
    "            self.dict_of_heads = defaultdict(set)\n",
    "            self.dict_of_tails = defaultdict(set)\n",
    "            self.dict_of_rels = defaultdict(set)\n",
    "            self.temp_dict_of_heads = defaultdict(set)\n",
    "            self.temp_dict_of_tails = defaultdict(set)\n",
    "            self.temp_dict_of_rels = defaultdict(set)\n",
    "#             self.dict_of_start_time = defaultdict(set)\n",
    "#             self.dict_of_end_time = defaultdict(set)\n",
    "            self.evaluate_dicts()\n",
    "\n",
    "        else:\n",
    "            self.dict_of_heads = dict_of_heads\n",
    "            self.dict_of_tails = dict_of_tails\n",
    "            self.dict_of_rels = dict_of_rels\n",
    "            self.temp_dict_of_heads = temp_dict_of_heads\n",
    "            self.temp_dict_of_tails = temp_dict_of_tails\n",
    "            self.temp_dict_of_rels = temp_dict_of_rels\n",
    "#             self.dict_of_start_time = dict_of_start_time\n",
    "#             self.dict_of_end_time = dict_of_end_time\n",
    "        try:\n",
    "            self.sanity_check()\n",
    "        except AssertionError:\n",
    "            raise SanityError(\"Please check the sanity of arguments.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_facts\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (self.head_idx[item].item(),\n",
    "                self.tail_idx[item].item(),\n",
    "                self.relations[item].item())\n",
    "\n",
    "    def sanity_check(self):\n",
    "        assert (type(self.dict_of_heads) == defaultdict) & \\\n",
    "               (type(self.dict_of_tails) == defaultdict) & \\\n",
    "               (type(self.dict_of_rels) == defaultdict) & \\\n",
    "                (type(self.temp_dict_of_heads) == defaultdict) & \\\n",
    "               (type(self.temp_dict_of_tails) == defaultdict) & \\\n",
    "               (type(self.temp_dict_of_rels) == defaultdict)\n",
    "        assert (type(self.ent2ix) == dict) & (type(self.rel2ix) == dict)\n",
    "        assert (len(self.ent2ix) == self.n_ent) & \\\n",
    "               (len(self.rel2ix) == self.n_rel)\n",
    "        assert (type(self.head_idx) == Tensor) & \\\n",
    "               (type(self.tail_idx) == Tensor) & \\\n",
    "               (type(self.relations) == Tensor)\n",
    "        assert (self.head_idx.dtype == int64) & \\\n",
    "               (self.tail_idx.dtype == int64) & (self.relations.dtype == int64)\n",
    "        assert (len(self.head_idx) == len(self.tail_idx) == len(self.relations))\n",
    "\n",
    "    def split_kg(self, share=0.8, sizes=None, validation=False):\n",
    "        if sizes is not None:\n",
    "            try:\n",
    "                if len(sizes) == 3:\n",
    "                    try:\n",
    "                        assert (sizes[0] + sizes[1] + sizes[2] == self.n_facts)\n",
    "                    except AssertionError:\n",
    "                        raise WrongArgumentsError('Sizes should sum to the '\n",
    "                                                  'number of facts.')\n",
    "                elif len(sizes) == 2:\n",
    "                    try:\n",
    "                        assert (sizes[0] + sizes[1] == self.n_facts)\n",
    "                    except AssertionError:\n",
    "                        raise WrongArgumentsError('Sizes should sum to the '\n",
    "                                                  'number of facts.')\n",
    "                else:\n",
    "                    raise SizeMismatchError('Tuple `sizes` should be of '\n",
    "                                            'length 2 or 3.')\n",
    "            except AssertionError:\n",
    "                raise SizeMismatchError('Tuple `sizes` should sum up to the '\n",
    "                                        'number of facts in the knowledge '\n",
    "                                        'graph.')\n",
    "        else:\n",
    "            assert share < 1\n",
    "\n",
    "        if ((sizes is not None) and (len(sizes) == 3)) or \\\n",
    "                ((sizes is None) and validation):\n",
    "            # return training, validation and a testing graphs\n",
    "\n",
    "            if (sizes is None) and validation:\n",
    "                mask_tr, mask_val, mask_te = self.get_mask(share,\n",
    "                                                           validation=True)\n",
    "            else:\n",
    "                mask_tr = cat([tensor([1 for _ in range(sizes[0])]),\n",
    "                               tensor([0 for _ in range(sizes[1] + sizes[2])])]).bool()\n",
    "                mask_val = cat([tensor([0 for _ in range(sizes[0])]),\n",
    "                                tensor([1 for _ in range(sizes[1])]),\n",
    "                                tensor([0 for _ in range(sizes[2])])]).bool()\n",
    "                mask_te = ~(mask_tr | mask_val)\n",
    "\n",
    "            return (TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_tr],\n",
    "                            'tails': self.tail_idx[mask_tr],\n",
    "                            'relations': self.relations[mask_tr],\n",
    "                           'start_time': self.start_time[mask_tr],\n",
    "                           'end_time': self.end_time[mask_tr]},\n",
    "                            ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                            time_trans = self.time_trans,\n",
    "                            dict_of_heads=self.dict_of_heads,\n",
    "                            dict_of_tails=self.dict_of_tails,\n",
    "                            dict_of_rels=self.dict_of_rels,\n",
    "                            temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                            temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                            temp_dict_of_rels = self.temp_dict_of_rels\n",
    "                            ),\n",
    "                    TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_val],\n",
    "                            'tails': self.tail_idx[mask_val],\n",
    "                            'relations': self.relations[mask_val],\n",
    "                           'start_time':  self.start_time[mask_val],\n",
    "                           'end_time': self.end_time[mask_val]},\n",
    "                        ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                        time_trans = self.time_trans,\n",
    "                        dict_of_heads=self.dict_of_heads,\n",
    "                        dict_of_tails=self.dict_of_tails,\n",
    "                        dict_of_rels=self.dict_of_rels,\n",
    "                        temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                        temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                        temp_dict_of_rels = self.temp_dict_of_rels),\n",
    "                    TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_te],\n",
    "                            'tails': self.tail_idx[mask_te],\n",
    "                            'relations': self.relations[mask_te],\n",
    "                           'start_time': self.start_time[mask_te],\n",
    "                           'end_time': self.end_time[mask_te]},\n",
    "                        ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                        time_trans = self.time_trans,\n",
    "                        dict_of_heads=self.dict_of_heads,\n",
    "                        dict_of_tails=self.dict_of_tails,\n",
    "                        dict_of_rels=self.dict_of_rels,\n",
    "                        temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                        temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                        temp_dict_of_rels = self.temp_dict_of_rels\n",
    "                        ))\n",
    "        else:\n",
    "            # return training and testing graphs\n",
    "\n",
    "            assert (((sizes is not None) and len(sizes) == 2) or\n",
    "                    ((sizes is None) and not validation))\n",
    "            if sizes is None:\n",
    "                mask_tr, mask_te = self.get_mask(share, validation=False)\n",
    "            else:\n",
    "                mask_tr = cat([tensor([1 for _ in range(sizes[0])]),\n",
    "                               tensor([0 for _ in range(sizes[1])])]).bool()\n",
    "                mask_te = ~mask_tr\n",
    "            return (TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_tr],\n",
    "                            'tails': self.tail_idx[mask_tr],\n",
    "                            'relations': self.relations[mask_tr],\n",
    "                           'start_time': self.start_time[mask_tr],\n",
    "                           'end_time': self.end_time[mask_tr]},\n",
    "                        ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                        time_trans = self.time_trans,\n",
    "                        dict_of_heads=self.dict_of_heads,\n",
    "                        dict_of_tails=self.dict_of_tails,\n",
    "                        dict_of_rels=self.dict_of_rels,\n",
    "                        temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                        temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                        temp_dict_of_rels = self.temp_dict_of_rels\n",
    "                        ),\n",
    "                    TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_te],\n",
    "                            'tails': self.tail_idx[mask_te],\n",
    "                            'relations': self.relations[mask_te],\n",
    "                           'start_time': self.start_time[mask_te],\n",
    "                           'end_time': self.end_time[mask_te]},\n",
    "                        ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                        time_trans = self.time_trans,\n",
    "                        dict_of_heads=self.dict_of_heads,\n",
    "                        dict_of_tails=self.dict_of_tails,\n",
    "                        dict_of_rels=self.dict_of_rels,\n",
    "                        temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                        temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                        temp_dict_of_rels = self.temp_dict_of_rels\n",
    "                        ))\n",
    "\n",
    "    def get_mask(self, share, validation=False):\n",
    "\n",
    "        uniques_r, counts_r = self.relations.unique(return_counts=True)\n",
    "        uniques_e, _ = cat((self.head_idx,\n",
    "                            self.tail_idx)).unique(return_counts=True)\n",
    "\n",
    "        mask = zeros_like(self.relations).bool()\n",
    "        if validation:\n",
    "            mask_val = zeros_like(self.relations).bool()\n",
    "\n",
    "        # splitting relations among subsets\n",
    "        for i, r in enumerate(uniques_r):\n",
    "            rand = randperm(counts_r[i].item())\n",
    "\n",
    "            # list of indices k such that relations[k] == r\n",
    "            sub_mask = eq(self.relations, r).nonzero(as_tuple=False)[:, 0]\n",
    "\n",
    "            assert len(sub_mask) == counts_r[i].item()\n",
    "\n",
    "            if validation:\n",
    "                train_size, val_size, test_size = self.get_sizes(counts_r[i].item(),\n",
    "                                                                 share=share,\n",
    "                                                                 validation=True)\n",
    "                mask[sub_mask[rand[:train_size]]] = True\n",
    "                mask_val[sub_mask[rand[train_size:train_size + val_size]]] = True\n",
    "\n",
    "            else:\n",
    "                train_size, test_size = self.get_sizes(counts_r[i].item(),\n",
    "                                                       share=share,\n",
    "                                                       validation=False)\n",
    "                mask[sub_mask[rand[:train_size]]] = True\n",
    "\n",
    "        # adding missing entities to the train set\n",
    "        u = cat((self.head_idx[mask], self.tail_idx[mask])).unique()\n",
    "        if len(u) < self.n_ent:\n",
    "            missing_entities = tensor(list(set(uniques_e.tolist()) -\n",
    "                                           set(u.tolist())), dtype=long)\n",
    "            for e in missing_entities:\n",
    "                sub_mask = ((self.head_idx == e) |\n",
    "                            (self.tail_idx == e)).nonzero(as_tuple=False)[:, 0]\n",
    "                rand = randperm(len(sub_mask))\n",
    "                sizes = self.get_sizes(mask.shape[0],\n",
    "                                       share=share,\n",
    "                                       validation=validation)\n",
    "                mask[sub_mask[rand[:sizes[0]]]] = True\n",
    "                if validation:\n",
    "                    mask_val[sub_mask[rand[:sizes[0]]]] = False\n",
    "\n",
    "        if validation:\n",
    "            assert not (mask & mask_val).any().item()\n",
    "            return mask, mask_val, ~(mask | mask_val)\n",
    "        else:\n",
    "            return mask, ~mask\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sizes(count, share, validation=False):\n",
    "        if count == 1:\n",
    "            if validation:\n",
    "                return 1, 0, 0\n",
    "            else:\n",
    "                return 1, 0\n",
    "        if count == 2:\n",
    "            if validation:\n",
    "                return 1, 1, 0\n",
    "            else:\n",
    "                return 1, 1\n",
    "\n",
    "        n_train = int(count * share)\n",
    "        assert n_train < count\n",
    "        if n_train == 0:\n",
    "            n_train += 1\n",
    "\n",
    "        if not validation:\n",
    "            return n_train, count - n_train\n",
    "        else:\n",
    "            if count - n_train == 1:\n",
    "                n_train -= 1\n",
    "                return n_train, 1, 1\n",
    "            else:\n",
    "                n_val = int(int(count - n_train) / 2)\n",
    "                return n_train, n_val, count - n_train - n_val\n",
    "\n",
    "    def evaluate_dicts(self):\n",
    "        for i in range(self.n_facts):\n",
    "            self.dict_of_heads[(self.tail_idx[i].item(),\n",
    "                                self.relations[i].item())].add(self.head_idx[i].item())\n",
    "            self.dict_of_tails[(self.head_idx[i].item(),\n",
    "                                self.relations[i].item())].add(self.tail_idx[i].item())\n",
    "            self.dict_of_rels[(self.head_idx[i].item(),\n",
    "                               self.tail_idx[i].item())].add(self.relations[i].item())\n",
    "            \n",
    "            self.temp_dict_of_heads[(self.tail_idx[i].item(),\n",
    "                                    self.relations[i].item(),\n",
    "                                     self.start_time[i].item(),\n",
    "                                     self.end_time[i].item())].add(self.head_idx[i].item())\n",
    "            self.temp_dict_of_tails[(self.head_idx[i].item(),\n",
    "                                self.relations[i].item(),\n",
    "                                     self.start_time[i].item(),\n",
    "                                     self.end_time[i].item())].add(self.tail_idx[i].item())\n",
    "            self.temp_dict_of_rels[(self.head_idx[i].item(),\n",
    "                                   self.tail_idx[i].item(),\n",
    "                                    self.start_time[i].item(),\n",
    "                                     self.end_time[i].item())].add(self.relations[i].item())\n",
    "\n",
    "    def get_df(self):\n",
    "        ix2ent = {v: k for k, v in self.ent2ix.items()}\n",
    "        ix2rel = {v: k for k, v in self.rel2ix.items()}\n",
    "\n",
    "        df = DataFrame(cat((self.head_idx.view(1, -1),\n",
    "                            self.tail_idx.view(1, -1),\n",
    "                            self.relations.view(1, -1))).transpose(0, 1).numpy(),\n",
    "                       columns=['from', 'to', 'rel'])\n",
    "\n",
    "        df['from'] = df['from'].apply(lambda x: ix2ent[x])\n",
    "        df['to'] = df['to'].apply(lambda x: ix2ent[x])\n",
    "        df['rel'] = df['rel'].apply(lambda x: ix2rel[x])\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class SmallKG(Dataset):\n",
    "    def __init__(self, heads, tails, relations):\n",
    "        assert heads.shape == tails.shape == relations.shape\n",
    "        self.head_idx = heads\n",
    "        self.tail_idx = tails\n",
    "        self.relations = relations\n",
    "        self.length = heads.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.head_idx[item].item(), self.tail_idx[item].item(), self.relations[item].item()\n",
    "\n",
    "\n",
    "\n",
    "def get_data_home(data_home=None):\n",
    "    if data_home is None:\n",
    "        data_home = environ.get('TORCHKGE_DATA',\n",
    "                                join('~', 'torchkge_data'))\n",
    "    data_home = expanduser(data_home)\n",
    "    if not exists(data_home):\n",
    "        makedirs(data_home)\n",
    "    return data_home\n",
    "\n",
    "\n",
    "def clear_data_home(data_home=None):\n",
    "    data_home = get_data_home(data_home)\n",
    "    shutil.rmtree(data_home)\n",
    "\n",
    "\n",
    "def get_n_batches(n, b_size):\n",
    "    n_batch = n // b_size\n",
    "    if n % b_size > 0:\n",
    "        n_batch += 1\n",
    "    return n_batch\n",
    "\n",
    "\n",
    "class TempDataLoader:\n",
    "    \"\"\"This class is inspired from :class:`torch.utils.dataloader.DataLoader`.\n",
    "    It is however way simpler.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, kg, batch_size, use_cuda=None):\n",
    "\n",
    "        self.h = kg.head_idx\n",
    "        self.t = kg.tail_idx\n",
    "        self.r = kg.relations\n",
    "        self.start_time = kg.start_time\n",
    "        self.end_time = kg.end_time\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.h = self.h.to(device)\n",
    "        self.t = self.t.to(device)\n",
    "        self.r = self.r.to(device)\n",
    "        self.start_time = self.start_time.to(device)\n",
    "        self.end_time = self.end_time.to(device)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return get_n_batches(len(self.h), self.batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return _TempDataLoaderIter(self)\n",
    "\n",
    "\n",
    "class _TempDataLoaderIter:\n",
    "    def __init__(self, loader):\n",
    "        self.h = loader.h\n",
    "        self.t = loader.t\n",
    "        self.r = loader.r\n",
    "        self.start_time = loader.start_time\n",
    "        self.end_time = loader.end_time\n",
    "        \n",
    "        self.use_cuda = loader.use_cuda\n",
    "        self.batch_size = loader.batch_size\n",
    "\n",
    "        self.n_batches = get_n_batches(len(self.h), self.batch_size)\n",
    "        self.current_batch = 0\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_batch == self.n_batches:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            i = self.current_batch\n",
    "            self.current_batch += 1\n",
    "\n",
    "            tmp_h = self.h[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            tmp_t = self.t[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            tmp_r = self.r[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            tmp_start_time = self.start_time[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            tmp_end_time = self.end_time[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            return tmp_h.to(device), tmp_t.to(device), tmp_r.to(device), tmp_start_time.to(device), tmp_end_time.to(device)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0cc0c",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d2881cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_icews14(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/icews14'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "\n",
    "def load_edukg(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/edukg'\n",
    "    ent_df = pd.read_csv(data_path + '/entity2id.txt', sep='\\t',\n",
    "                         header=None, names=['ent', 'id'])\n",
    "    # print('ent_df.dtypes: ',ent_df.dtypes)\n",
    "    rel_df = pd.read_csv(data_path + '/relation2id.txt', sep='\\t',\n",
    "                         header=None, names=['rel', 'id'])\n",
    "    # print('rel_df.dtypes: ',rel_df.dtypes)\n",
    "    ent_df = dict(zip(ent_df['ent'], ent_df['id']))\n",
    "    rel_df = dict(zip(rel_df['rel'], rel_df['id']))\n",
    "    \n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df1['end_time'] = df1['start_time']\n",
    "    # print(df1)\n",
    "    # print(df1.isna().any())\n",
    "    # print('df1.dtypes: ',df1.dtypes)\n",
    "    df2 = read_csv(data_path + '/val.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2['end_time'] = df2['start_time']\n",
    "    # print('df2.dtypes: ',df2.dtypes)\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3['end_time'] = df3['start_time']\n",
    "    # print('df3.dtypes: ',df3.dtypes)\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode, ent2ix=ent_df, rel2ix=rel_df)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "def load_gdelt(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/gdelt'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time'])\n",
    "    df1['end_time'] = '1111-11-11'\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time'])\n",
    "    df2['end_time'] = '1111-11-11'\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time'])\n",
    "    df3['end_time'] = '1111-11-11'\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "def load_icews15(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/icews05-15'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "def load_yago11k(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/YAGO11k'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    df['start_time'] = df['start_time'].str.replace('##','01')\n",
    "    df['end_time'] = df['end_time'].str.replace('##','01')\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "def load_wikidata12k(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/WIKIDATA12k'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    df['start_time'] = df['start_time'].str.replace('##','01')\n",
    "    df['end_time'] = df['end_time'].str.replace('##','01')\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "\n",
    "def get_data(data_name, time_mode):\n",
    "    if data_name == 'icews14':\n",
    "        return load_icews14(data_home='data', time_mode = time_mode)\n",
    "    elif data_name == 'edukg':\n",
    "        return load_edukg(data_home='data', time_mode=time_mode)\n",
    "    elif data_name == 'gdelt':\n",
    "        return load_gdelt(data_home='data', time_mode = time_mode)\n",
    "    elif data_name == 'icews15':\n",
    "        return load_icews15(data_home='data', time_mode = time_mode)\n",
    "    elif data_name == 'yago11k':\n",
    "        return load_yago11k(data_home='data', time_mode = time_mode)\n",
    "    elif data_name == 'wikidata12k':\n",
    "        return load_wikidata12k(data_home='data', time_mode = time_mode)\n",
    "    else:\n",
    "        datas = ['icews14']\n",
    "        \n",
    "        print('Choose One of the Following Datasets: ',datas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f62e3b",
   "metadata": {},
   "source": [
    "## Eval Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a3ba6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_targets(dictionary, key1, key2, key3, key4, true_idx, i):\n",
    "    try:\n",
    "        if key4 is not None:\n",
    "            true_targets = dictionary[key1[i].item(), key2[i].item(), key3[i].item(), key4[i].item()].copy()\n",
    "        else:\n",
    "            true_targets = dictionary[key1[i].item(), key2[i].item(), key3[i].item()].copy()\n",
    "            \n",
    "        if true_idx is not None:\n",
    "            true_targets.remove(true_idx[i].item())\n",
    "            if len(true_targets) > 0:\n",
    "                return tensor(list(true_targets)).long()\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return tensor(list(true_targets)).long()\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c38f999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_scores(scores, self.kg.dict_of_tails, h_idx, r_idx, t_idx, start_time, end_time)\n",
    "def filter_scores(scores, dictionary, key1, key2, key3, key4, true_idx):\n",
    "    # filter out the true negative samples by assigning - inf score.\n",
    "    b_size = scores.shape[0]\n",
    "    filt_scores = scores.clone()\n",
    "\n",
    "    for i in range(b_size):\n",
    "        true_targets = get_true_targets(dictionary, key1, key2, key3, key4, true_idx, i)\n",
    "        if true_targets is None:\n",
    "            continue\n",
    "        filt_scores[i][true_targets] = - float('Inf')\n",
    "\n",
    "    return filt_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0447b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(data, true, low_values=False):\n",
    "    true_data = data.gather(1, true.long().view(-1, 1))\n",
    "\n",
    "    if low_values:\n",
    "        return (data <= true_data).sum(dim=1)\n",
    "    else:\n",
    "        return (data >= true_data).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c67061",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b416309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalLinkPredictionEvaluator(object):\n",
    "\n",
    "    def __init__(self, model, knowledge_graph):\n",
    "        self.model = model\n",
    "        self.kg = knowledge_graph\n",
    "\n",
    "        self.rank_true_heads = empty(size=(knowledge_graph.n_facts,)).long()\n",
    "        self.rank_true_tails = empty(size=(knowledge_graph.n_facts,)).long()\n",
    "        self.filt_rank_true_heads = empty(size=(knowledge_graph.n_facts,)).long()\n",
    "        self.filt_rank_true_tails = empty(size=(knowledge_graph.n_facts,)).long()\n",
    "\n",
    "        self.evaluated = False\n",
    "\n",
    "    def evaluate(self, b_size, verbose=True):\n",
    "        use_cuda = next(self.model.parameters()).is_cuda\n",
    "\n",
    "        if use_cuda:\n",
    "            dataloader = TempDataLoader(self.kg, batch_size=b_size, use_cuda='batch')\n",
    "            self.rank_true_heads = self.rank_true_heads.to(device)\n",
    "            self.rank_true_tails = self.rank_true_tails.to(device)\n",
    "            self.filt_rank_true_heads = self.filt_rank_true_heads.to(device)\n",
    "            self.filt_rank_true_tails = self.filt_rank_true_tails.to(device)\n",
    "        else:\n",
    "            dataloader = TempDataLoader(self.kg, batch_size=b_size)\n",
    "\n",
    "        for i, batch in tqdm(enumerate(dataloader), total=len(dataloader),\n",
    "                             unit='batch', disable=(not verbose),\n",
    "                             desc='Link prediction evaluation'):\n",
    "            h_idx, t_idx, r_idx, start_time, end_time = batch[0], batch[1], batch[2], batch[3], batch[4]\n",
    "                        \n",
    "            h_emb, t_emb, r_emb, candidates = self.model.inference_prepare_candidates(h_idx, t_idx, r_idx, start_time, end_time, entities=True)\n",
    "\n",
    "            scores = self.model.inference_scoring_function(h_emb, candidates, r_emb, start_time, end_time)\n",
    "            filt_scores = filter_scores(scores, self.kg.temp_dict_of_tails, h_idx, r_idx, start_time, end_time, t_idx)\n",
    "            self.rank_true_tails[i * b_size: (i + 1) * b_size] = get_rank(scores, t_idx).detach()\n",
    "            self.filt_rank_true_tails[i * b_size: (i + 1) * b_size] = get_rank(filt_scores, t_idx).detach()\n",
    "\n",
    "            scores = self.model.inference_scoring_function(candidates, t_emb, r_emb, start_time, end_time)\n",
    "            filt_scores = filter_scores(scores, self.kg.temp_dict_of_heads, t_idx, r_idx, start_time, end_time, h_idx)\n",
    "            self.rank_true_heads[i * b_size: (i + 1) * b_size] = get_rank(scores, h_idx).detach()\n",
    "            self.filt_rank_true_heads[i * b_size: (i + 1) * b_size] = get_rank(filt_scores, h_idx).detach()\n",
    "\n",
    "        self.evaluated = True\n",
    "\n",
    "        if use_cuda:\n",
    "            self.rank_true_heads = self.rank_true_heads.cpu()\n",
    "            self.rank_true_tails = self.rank_true_tails.cpu()\n",
    "            self.filt_rank_true_heads = self.filt_rank_true_heads.cpu()\n",
    "            self.filt_rank_true_tails = self.filt_rank_true_tails.cpu()\n",
    "\n",
    "    def mean_rank(self):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "        sum_ = (self.rank_true_heads.float().mean() +\n",
    "                self.rank_true_tails.float().mean()).item()\n",
    "        filt_sum = (self.filt_rank_true_heads.float().mean() +\n",
    "                    self.filt_rank_true_tails.float().mean()).item()\n",
    "        # return sum_ / 2, filt_sum / 2\n",
    "        return {'mr':sum_ / 2, 'filt_mr':filt_sum / 2}\n",
    "\n",
    "    def hit_at_k_heads(self, k=10):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "        head_hit = (self.rank_true_heads <= k).float().mean()\n",
    "        filt_head_hit = (self.filt_rank_true_heads <= k).float().mean()\n",
    "\n",
    "        # return head_hit.item(), filt_head_hit.item()\n",
    "        return {'head_hit_'+str(k):head_hit.item(), 'filt_head_hit_'+str(k): filt_head_hit.item()}\n",
    "\n",
    "    def hit_at_k_tails(self, k=10):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "        tail_hit = (self.rank_true_tails <= k).float().mean()\n",
    "        filt_tail_hit = (self.filt_rank_true_tails <= k).float().mean()\n",
    "\n",
    "        # return tail_hit.item(), filt_tail_hit.item()\n",
    "        return {'tail_hit_'+str(k):tail_hit.item(), 'filt_tail_hit_'+str(k):filt_tail_hit.item()}\n",
    "\n",
    "    def hit_at_k(self, k=10):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "\n",
    "        head_hit = self.hit_at_k_heads(k=k)\n",
    "        head_hit, filt_head_hit = head_hit['head_hit_'+str(k)], head_hit['filt_head_hit_'+str(k)]\n",
    "\n",
    "        tail_hit = self.hit_at_k_tails(k=k)\n",
    "        tail_hit, filt_tail_hit = tail_hit['tail_hit_'+str(k)], tail_hit['filt_tail_hit_'+str(k)]\n",
    "\n",
    "        # return (head_hit + tail_hit) / 2, (filt_head_hit + filt_tail_hit) / 2\n",
    "        return {'hit_'+str(k): (head_hit + tail_hit) / 2, 'filt_hit_'+str(k): (filt_head_hit + filt_tail_hit) / 2}\n",
    "\n",
    "    def mrr(self):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "        res = {}\n",
    "        head_mrr = (self.rank_true_heads.float()**(-1)).mean()\n",
    "        res['head_mrr'] = head_mrr.item()\n",
    "        tail_mrr = (self.rank_true_tails.float()**(-1)).mean()\n",
    "        res['tail_mrr'] = tail_mrr.item()\n",
    "        filt_head_mrr = (self.filt_rank_true_heads.float()**(-1)).mean()\n",
    "        res['filt_head_mrr'] = filt_head_mrr.item()\n",
    "        filt_tail_mrr = (self.filt_rank_true_tails.float()**(-1)).mean()    \n",
    "        res['filt_tail_mrr'] = filt_tail_mrr.item()\n",
    "        res['mrr'] = (head_mrr + tail_mrr).item() / 2\n",
    "        res['filt_mrr'] = (filt_head_mrr + filt_tail_mrr).item() / 2\n",
    "        return res\n",
    "\n",
    "    def get_results(self, k=None):\n",
    "        res = {}\n",
    "        if k is None:\n",
    "            k = 10\n",
    "\n",
    "        for i in range(1, k + 1):\n",
    "            hits_res = self.hit_at_k(k=i)\n",
    "            res.update(hits_res)\n",
    "\n",
    "        mr_res = self.mean_rank()\n",
    "        res.update(mr_res)\n",
    "        mrr_res = self.mrr()\n",
    "        res.update(mrr_res)\n",
    "        return res\n",
    "\n",
    "    def print_results(self, k=None, n_digits=3):\n",
    "        if k is None:\n",
    "            k = 10\n",
    "\n",
    "        if k is not None and type(k) == int:\n",
    "            print('Hit@{} : {} \\t\\t Filt. Hit@{} : {}'.format(\n",
    "                k, round(self.hit_at_k(k=k)[0], n_digits),\n",
    "                k, round(self.hit_at_k(k=k)[1], n_digits)))\n",
    "        if k is not None and type(k) == list:\n",
    "            for i in k:\n",
    "                print('Hit@{} : {} \\t\\t Filt. Hit@{} : {}'.format(\n",
    "                    i, round(self.hit_at_k(k=i)[0], n_digits),\n",
    "                    i, round(self.hit_at_k(k=i)[1], n_digits)))\n",
    "\n",
    "        print('Mean Rank : {} \\t Filt. Mean Rank : {}'.format(\n",
    "            int(self.mean_rank()[0]), int(self.mean_rank()[1])))\n",
    "        print('MRR : {} \\t\\t Filt. MRR : {}'.format(\n",
    "            round(self.mrr()[0], n_digits), round(self.mrr()[1], n_digits)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24164d4e",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef6a5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRankLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, y_pos, y_neg, temp=0):\n",
    "        M = y_pos.size(0)\n",
    "        N = y_neg.size(0)\n",
    "        y_pos = self.gamma-y_pos\n",
    "        y_neg = self.gamma-y_neg\n",
    "        C = int(N / M)\n",
    "        y_neg = y_neg.view(C, -1).transpose(0, 1)\n",
    "        p = F.softmax(temp * y_neg)\n",
    "        loss_pos = torch.sum(F.softplus(-1 * y_pos))\n",
    "        loss_neg = torch.sum(p * F.softplus(y_neg))\n",
    "        loss = (loss_pos + loss_neg) / 2 / M\n",
    "#         if self.gpu:\n",
    "#             loss = loss.cuda()\n",
    "        return loss\n",
    "class CrossEntropy(torch.nn.Module):\n",
    "    def __init__(self, n_neg, b_size):\n",
    "        super().__init__()\n",
    "        self.loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "        self.n_neg = n_neg\n",
    "        self.b_size = b_size\n",
    "    def forward(self, positive_triplets, negative_triplets):\n",
    "        negative_triplets = negative_triplets.view(-1, self.n_neg)\n",
    "        positive_triplets = positive_triplets.view(-1, self.n_neg)[:,0].unsqueeze(1)\n",
    "        b_size = negative_triplets.shape[0]\n",
    "#         print('positive_triplets.shape: ',positive_triplets.shape)\n",
    "#         print('negative_triplets.shape: ',negative_triplets.shape)\n",
    "        all_scores = torch.cat([positive_triplets, negative_triplets], dim =-1)\n",
    "#         print('all_scores.shape: ',all_scores.shape)\n",
    "        l = torch.zeros(b_size).long().to(device)\n",
    "        loss = self.loss(all_scores, l)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b3a7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss_name, args):\n",
    "    if loss_name == 'MarginLoss':\n",
    "        return MarginLoss(args.margin)\n",
    "    elif loss_name == 'LogisticLoss':\n",
    "        return LogisticLoss()\n",
    "    elif loss_name == 'LogRankLoss':\n",
    "        return LogRankLoss(args.gamma)\n",
    "    elif loss_name == 'CrossEntropy':\n",
    "        return CrossEntropy(args.n_neg, args.b_size)\n",
    "    elif loss_name == 'BinaryCrossEntropyLoss':\n",
    "        return BinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017960ed",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a99e3",
   "metadata": {},
   "source": [
    "## Interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f838ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempModel(Module):\n",
    "    def __init__(self, n_entities, n_relations):\n",
    "        super().__init__()\n",
    "        self.n_ent = n_entities\n",
    "        self.n_rel = n_relations\n",
    "\n",
    "    def forward(self, heads, tails, relations, start_time, end_time, negative_heads, negative_tails, negative_relations=None):\n",
    "        pos = self.scoring_function(heads, tails, relations, start_time, end_time)\n",
    "\n",
    "        if negative_relations is None:\n",
    "            negative_relations = relations\n",
    "\n",
    "        if negative_heads.shape[0] > negative_relations.shape[0]:\n",
    "            # in that case, several negative samples are sampled from each fact\n",
    "            n_neg = int(negative_heads.shape[0] / negative_relations.shape[0])\n",
    "            # print('pos.shape: ', pos.shape)\n",
    "            pos = pos.repeat(n_neg)\n",
    "#             neg = self.scoring_function(negative_heads,\n",
    "#                                         negative_tails,\n",
    "#                                         negative_relations.repeat(n_neg),\n",
    "#                                         start_time.repeat(n_neg, 1) if start_time.dim() ==2 & start_time.shape[-1]>1 else start_time.repeat(n_neg),\n",
    "#                                         end_time.repeat(n_neg, 1) if end_time.dim() ==2 & end_time.shape[-1]>1 else end_time.repeat(n_neg))\n",
    "            neg = self.scoring_function(negative_heads,\n",
    "                            negative_tails,\n",
    "                            negative_relations.repeat(n_neg),\n",
    "                            start_time.repeat(n_neg),\n",
    "                            end_time.repeat(n_neg))\n",
    "\n",
    "        else:\n",
    "            neg = self.scoring_function(negative_heads,\n",
    "                                        negative_tails,\n",
    "                                        negative_relations, start_time, end_time)\n",
    "\n",
    "        return pos, neg\n",
    "\n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_scoring_function(self, h, t, r, time):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx, entities=True):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class TempTranslationModel(TempModel):\n",
    "    def __init__(self, n_entities, n_relations, dissimilarity_type):\n",
    "        super().__init__(n_entities, n_relations)\n",
    "\n",
    "        assert dissimilarity_type in ['L1', 'L2', 'torus_L1', 'torus_L2',\n",
    "                                      'torus_eL2']\n",
    "\n",
    "        if dissimilarity_type == 'L1':\n",
    "            self.dissimilarity = l1_dissimilarity\n",
    "        elif dissimilarity_type == 'L2':\n",
    "            self.dissimilarity = l2_dissimilarity\n",
    "        elif dissimilarity_type == 'torus_L1':\n",
    "            self.dissimilarity = l1_torus_dissimilarity\n",
    "        elif dissimilarity_type == 'torus_L2':\n",
    "            self.dissimilarity = l2_torus_dissimilarity\n",
    "        else:\n",
    "            self.dissimilarity = el2_torus_dissimilarity\n",
    "\n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx, entities=True):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_scoring_function(self, proj_h, proj_t, r, start_time, end_time):\n",
    "        b_size = proj_h.shape[0]\n",
    "\n",
    "        if len(r.shape) == 2:\n",
    "            if len(proj_t.shape) == 3:\n",
    "                assert (len(proj_h.shape) == 2)\n",
    "                # this is the tail completion case in link prediction\n",
    "                hr = (proj_h + r).view(b_size, 1, r.shape[1])\n",
    "                return - self.dissimilarity(hr, proj_t)\n",
    "            else:\n",
    "                assert (len(proj_h.shape) == 3) & (len(proj_t.shape) == 2)\n",
    "                # this is the head completion case in link prediction\n",
    "                r_ = r.view(b_size, 1, r.shape[1])\n",
    "                t_ = proj_t.view(b_size, 1, r.shape[1])\n",
    "                return - self.dissimilarity(proj_h + r_, t_)\n",
    "        elif len(r.shape) == 3:\n",
    "            # this is the relation prediction case\n",
    "            # Two cases possible:\n",
    "            # * proj_ent.shape == (b_size, self.n_rel, self.emb_dim) -> projection depending on relations\n",
    "            # * proj_ent.shape == (b_size, self.emb_dim) -> no projection\n",
    "            proj_h = proj_h.view(b_size, -1, self.emb_dim)\n",
    "            proj_t = proj_t.view(b_size, -1, self.emb_dim)\n",
    "            return - self.dissimilarity(proj_h + r, proj_t)\n",
    "\n",
    "\n",
    "class TempBilinearModel(TempModel):\n",
    "\n",
    "    def __init__(self, emb_dim, n_entities, n_relations):\n",
    "        super().__init__(n_entities, n_relations)\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_scoring_function(self, h, t, r, start_time, end_time):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx, start_time, end_time, entities=True):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571444a8",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "580ae799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETransEModel(TempTranslationModel):\n",
    "\n",
    "    def __init__(self, args):\n",
    "\n",
    "        super().__init__(args.n_entities, args.n_relations, args.dissimilarity_type)\n",
    "\n",
    "        self.emb_dim = args.emb_dim\n",
    "        self.s_rep_ratio = args.s_rep_ratio\n",
    "        self.ent_emb_dim = int(self.emb_dim * self.s_rep_ratio)\n",
    "        self.time_emb_dim = args.emb_dim - self.ent_emb_dim\n",
    "        self.time_trans = None\n",
    "        self.ent_emb = init_embedding(self.n_ent, self.ent_emb_dim)\n",
    "        self.rel_emb = init_embedding(self.n_rel, self.ent_emb_dim + self.time_emb_dim)\n",
    "        self.device = device\n",
    "\n",
    "        self.create_time_embedds()        \n",
    "        self.normalize_parameters()\n",
    "        self.rel_emb.weight.data = normalize(self.rel_emb.weight.data, p=2, dim=1)\n",
    "\n",
    "    def create_time_embedds(self):\n",
    "        self.m_freq = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        self.d_freq = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        self.y_freq = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        nn.init.xavier_uniform_(self.m_freq.weight)\n",
    "        nn.init.xavier_uniform_(self.d_freq.weight)\n",
    "        nn.init.xavier_uniform_(self.y_freq.weight)\n",
    "        \n",
    "        \n",
    "        self.m_phi = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        self.d_phi = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        self.y_phi = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        nn.init.xavier_uniform_(self.m_phi.weight)\n",
    "        nn.init.xavier_uniform_(self.d_phi.weight)\n",
    "        nn.init.xavier_uniform_(self.y_phi.weight)\n",
    "\n",
    "        self.m_amp = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        self.d_amp = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        self.y_amp = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        nn.init.xavier_uniform_(self.m_amp.weight)\n",
    "        nn.init.xavier_uniform_(self.d_amp.weight)\n",
    "        nn.init.xavier_uniform_(self.y_amp.weight)\n",
    "        \n",
    "    def get_time_embedd(self, entities, start_time_idx, end_time_idx):\n",
    "        year, month, day = start_time_idx[:,0].unsqueeze(dim=1), start_time_idx[:,1].unsqueeze(dim=1), start_time_idx[:,2].unsqueeze(dim=1)\n",
    "        pi = 3.14159265359\n",
    "        y = self.y_amp(entities)*torch.sin(self.y_freq(entities)*year + self.y_phi(entities))\n",
    "        m = self.m_amp(entities)*torch.sin(self.m_freq(entities)*month + self.m_phi(entities))\n",
    "        d = self.d_amp(entities)*torch.sin(self.d_freq(entities)*day + self.d_phi(entities))\n",
    "        return y+m+d\n",
    "\n",
    "    def getEmbeddings(self, heads_idx, rels_idx, tails_idx, start_time_idx, end_time_idx, intervals = None):\n",
    "        h,r,t = self.ent_emb(heads_idx), self.rel_emb(rels_idx), self.ent_emb(tails_idx)\n",
    "        h_t = self.get_time_embedd(heads_idx, start_time_idx, end_time_idx)\n",
    "        t_t = self.get_time_embedd(tails_idx, start_time_idx, end_time_idx)\n",
    "\n",
    "        h = torch.cat((h,h_t), 1)\n",
    "        t = torch.cat((t,t_t), 1)\n",
    "        return h,r,t\n",
    "\n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        start_time_idx = self.time_trans[start_time_idx]\n",
    "        h,r,t = self.getEmbeddings(heads_idx = h_idx, rels_idx = r_idx, tails_idx= t_idx, start_time_idx=start_time_idx, end_time_idx=end_time_idx, intervals = None)\n",
    "        h = normalize(h, p=2, dim=1)\n",
    "        t = normalize(t, p=2, dim=1)\n",
    "\n",
    "        return - self.dissimilarity(h + r, t)\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        self.ent_emb.weight.data = normalize(self.ent_emb.weight.data,\n",
    "                                             p=2, dim=1)\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        self.normalize_parameters()\n",
    "        return self.ent_emb.weight.data, self.rel_emb.weight.data\n",
    "\n",
    "    def inference_scoring_function(self, proj_h, proj_t, r, start_time, end_time):\n",
    "        b_size = proj_h.shape[0]\n",
    "\n",
    "        if len(r.shape) == 2:\n",
    "            if len(proj_t.shape) == 3:\n",
    "                assert (len(proj_h.shape) == 2)\n",
    "                # this is the tail completion case in link prediction\n",
    "                hr = (proj_h + r).view(b_size, 1, r.shape[1])\n",
    "                return - self.dissimilarity(hr, proj_t)\n",
    "            else:\n",
    "                assert (len(proj_h.shape) == 3) & (len(proj_t.shape) == 2)\n",
    "                # this is the head completion case in link prediction\n",
    "                r_ = r.view(b_size, 1, r.shape[1])\n",
    "                t_ = proj_t.view(b_size, 1, r.shape[1])\n",
    "                return - self.dissimilarity(proj_h + r_, t_)\n",
    "        elif len(r.shape) == 3:\n",
    "            # this is the relation prediction case\n",
    "            # Two cases possible:\n",
    "            proj_h = proj_h.view(b_size, -1, self.emb_dim)\n",
    "            proj_t = proj_t.view(b_size, -1, self.emb_dim)\n",
    "            return - self.dissimilarity(proj_h + r, proj_t)\n",
    "        \n",
    "    def get_candidate_time_emb(self, time_amp, time_freq, time_phi, time):\n",
    "        b_size = time.shape[0]\n",
    "        time_amp_ = time_amp.weight.data.view(1, self.n_ent, self.time_emb_dim)\n",
    "        time_amp_ = time_amp_.expand(b_size, self.n_ent, self.time_emb_dim)\n",
    "        time_freq_ = time_freq.weight.data.view(1, self.n_ent, self.time_emb_dim)\n",
    "        time_freq_ = time_freq_.expand(b_size, self.n_ent, self.time_emb_dim)\n",
    "        time_phi_ = time_phi.weight.data.view(1, self.n_ent, self.time_emb_dim)\n",
    "        time_phi_ = time_phi_.expand(b_size, self.n_ent, self.time_emb_dim)\n",
    "        return time_amp_*torch.sin(time_freq_*time + time_phi_)\n",
    "    \n",
    "    def get_candidate_embedding(self, start_time_idx):\n",
    "        b_size = start_time_idx.shape[0]\n",
    "        year, month, day = start_time_idx[:,0].unsqueeze(dim=1), start_time_idx[:,1].unsqueeze(dim=1), start_time_idx[:,2].unsqueeze(dim=1)\n",
    "        year = year.view(b_size, 1, year.shape[1])\n",
    "        month = month.view(b_size, 1, month.shape[1])\n",
    "        day = day.view(b_size, 1, day.shape[1])\n",
    "        c = self.ent_emb.weight.data.view(1, self.n_ent, self.ent_emb_dim)\n",
    "        c = c.expand(b_size, self.n_ent, self.ent_emb_dim)\n",
    "        c_y = self.get_candidate_time_emb(self.y_amp, self.y_freq, self.y_phi, year)\n",
    "        c_m = self.get_candidate_time_emb(self.m_amp, self.m_freq, self.m_phi, month)\n",
    "        c_d = self.get_candidate_time_emb(self.d_amp, self.d_freq, self.d_phi, day)\n",
    "        c = torch.cat((c,c_y + c_m + c_d), -1)\n",
    "        return c\n",
    "    \n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx, entities=True):\n",
    "        start_time_idx = self.time_trans[start_time_idx]\n",
    "        b_size = h_idx.shape[0]\n",
    "        h,r,t = self.getEmbeddings(heads_idx = h_idx, rels_idx = r_idx, tails_idx= t_idx, start_time_idx=start_time_idx, end_time_idx=end_time_idx, intervals = None)\n",
    "        if entities:\n",
    "            candidates = self.get_candidate_embedding(start_time_idx)\n",
    "        else:\n",
    "            candidates = self.rel_emb.weight.data.view(1, self.n_rel, self.emb_dim)\n",
    "            candidates = candidates.expand(b_size, self.n_rel, self.emb_dim + self.time_emb_dim)\n",
    "\n",
    "        return h, t, r, candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "001fd657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEDistMultModel(TempTranslationModel):\n",
    "\n",
    "    def __init__(self, args):\n",
    "\n",
    "        super().__init__(args.n_entities, args.n_relations, args.dissimilarity_type)\n",
    "\n",
    "        self.emb_dim = args.emb_dim\n",
    "        self.s_rep_ratio = args.s_rep_ratio\n",
    "        self.ent_emb_dim = int(self.emb_dim * self.s_rep_ratio)\n",
    "        self.time_emb_dim = args.emb_dim - self.ent_emb_dim\n",
    "        self.time_trans = None\n",
    "        self.ent_emb = init_embedding(self.n_ent, self.ent_emb_dim)\n",
    "        self.rel_emb = init_embedding(self.n_rel, self.ent_emb_dim + self.time_emb_dim)\n",
    "        self.device = device\n",
    "        self.args = args\n",
    "        self.create_time_embedds()        \n",
    "        self.normalize_parameters()\n",
    "        self.rel_emb.weight.data = normalize(self.rel_emb.weight.data, p=2, dim=1)\n",
    "\n",
    "    def create_time_embedds(self):\n",
    "        self.m_freq = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        self.d_freq = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        self.y_freq = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        nn.init.xavier_uniform_(self.m_freq.weight)\n",
    "        nn.init.xavier_uniform_(self.d_freq.weight)\n",
    "        nn.init.xavier_uniform_(self.y_freq.weight)\n",
    "        \n",
    "        \n",
    "        self.m_phi = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        self.d_phi = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        self.y_phi = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        nn.init.xavier_uniform_(self.m_phi.weight)\n",
    "        nn.init.xavier_uniform_(self.d_phi.weight)\n",
    "        nn.init.xavier_uniform_(self.y_phi.weight)\n",
    "\n",
    "        self.m_amp = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        self.d_amp = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        self.y_amp = nn.Embedding(self.n_ent, self.time_emb_dim).to(device)\n",
    "        nn.init.xavier_uniform_(self.m_amp.weight)\n",
    "        nn.init.xavier_uniform_(self.d_amp.weight)\n",
    "        nn.init.xavier_uniform_(self.y_amp.weight)\n",
    "        \n",
    "    def get_time_embedd(self, entities, start_time_idx, end_time_idx):\n",
    "        year, month, day = start_time_idx[:,0].unsqueeze(dim=1), start_time_idx[:,1].unsqueeze(dim=1), start_time_idx[:,2].unsqueeze(dim=1)\n",
    "        pi = 3.14159265359\n",
    "        y = self.y_amp(entities)*torch.sin(self.y_freq(entities)*year + self.y_phi(entities))\n",
    "        m = self.m_amp(entities)*torch.sin(self.m_freq(entities)*month + self.m_phi(entities))\n",
    "        d = self.d_amp(entities)*torch.sin(self.d_freq(entities)*day + self.d_phi(entities))\n",
    "        return y+m+d\n",
    "\n",
    "    def getEmbeddings(self, heads_idx, rels_idx, tails_idx, start_time_idx, end_time_idx, intervals = None):\n",
    "        h,r,t = self.ent_emb(heads_idx), self.rel_emb(rels_idx), self.ent_emb(tails_idx)\n",
    "        h_t = self.get_time_embedd(heads_idx, start_time_idx, end_time_idx)\n",
    "        t_t = self.get_time_embedd(tails_idx, start_time_idx, end_time_idx)\n",
    "\n",
    "        h = torch.cat((h,h_t), 1)\n",
    "        t = torch.cat((t,t_t), 1)\n",
    "        return h,r,t\n",
    "\n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        start_time_idx = self.time_trans[start_time_idx]\n",
    "        h,r,t = self.getEmbeddings(heads_idx = h_idx, rels_idx = r_idx, tails_idx= t_idx, start_time_idx=start_time_idx, end_time_idx=end_time_idx, intervals = None)\n",
    "        # h = normalize(h, p=2, dim=1)\n",
    "        # t = normalize(t, p=2, dim=1)\n",
    "        scores = (h * r * t)\n",
    "        scores = F.dropout(scores, p=self.args.dropout, training=self.training)\n",
    "\n",
    "        return scores.sum(dim=1)\n",
    "    \n",
    "    def normalize_parameters(self):\n",
    "        self.ent_emb.weight.data = normalize(self.ent_emb.weight.data,\n",
    "                                             p=2, dim=1)\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        self.normalize_parameters()\n",
    "        return self.ent_emb.weight.data, self.rel_emb.weight.data\n",
    "\n",
    "    def inference_scoring_function(self, h, t, r, start_time, end_time):\n",
    "        b_size = h.shape[0]\n",
    "\n",
    "        if len(t.shape) == 3:\n",
    "            assert (len(h.shape) == 2) & (len(r.shape) == 2)\n",
    "            # this is the tail completion case in link prediction\n",
    "            hr = (h * r).view(b_size, 1, self.emb_dim)\n",
    "            return (hr * t).sum(dim=2)\n",
    "        elif len(h.shape) == 3:\n",
    "            assert (len(t.shape) == 2) & (len(r.shape) == 2)\n",
    "            # this is the head completion case in link prediction\n",
    "            rt = (r * t).view(b_size, 1, self.emb_dim)\n",
    "            return (h * rt).sum(dim=2)\n",
    "        elif len(r.shape) == 3:\n",
    "            assert (len(h.shape) == 2) & (len(t.shape) == 2)\n",
    "            # this is the relation prediction case\n",
    "            hr = (h.view(b_size, 1, self.emb_dim) * r)  # hr has shape (b_size, self.n_rel, self.emb_dim)\n",
    "            return (hr * t.view(b_size, 1, self.emb_dim)).sum(dim=2)\n",
    "        \n",
    "    def get_candidate_time_emb(self, time_amp, time_freq, time_phi, time):\n",
    "        b_size = time.shape[0]\n",
    "        time_amp_ = time_amp.weight.data.view(1, self.n_ent, self.time_emb_dim)\n",
    "        time_amp_ = time_amp_.expand(b_size, self.n_ent, self.time_emb_dim)\n",
    "        time_freq_ = time_freq.weight.data.view(1, self.n_ent, self.time_emb_dim)\n",
    "        time_freq_ = time_freq_.expand(b_size, self.n_ent, self.time_emb_dim)\n",
    "        time_phi_ = time_phi.weight.data.view(1, self.n_ent, self.time_emb_dim)\n",
    "        time_phi_ = time_phi_.expand(b_size, self.n_ent, self.time_emb_dim)\n",
    "        return time_amp_*torch.sin(time_freq_*time + time_phi_)\n",
    "    \n",
    "    def get_candidate_embedding(self, start_time_idx):\n",
    "        b_size = start_time_idx.shape[0]\n",
    "        year, month, day = start_time_idx[:,0].unsqueeze(dim=1), start_time_idx[:,1].unsqueeze(dim=1), start_time_idx[:,2].unsqueeze(dim=1)\n",
    "        year = year.view(b_size, 1, year.shape[1])\n",
    "        month = month.view(b_size, 1, month.shape[1])\n",
    "        day = day.view(b_size, 1, day.shape[1])\n",
    "        c = self.ent_emb.weight.data.view(1, self.n_ent, self.ent_emb_dim)\n",
    "        c = c.expand(b_size, self.n_ent, self.ent_emb_dim)\n",
    "        c_y = self.get_candidate_time_emb(self.y_amp, self.y_freq, self.y_phi, year)\n",
    "        c_m = self.get_candidate_time_emb(self.m_amp, self.m_freq, self.m_phi, month)\n",
    "        c_d = self.get_candidate_time_emb(self.d_amp, self.d_freq, self.d_phi, day)\n",
    "        c = torch.cat((c,c_y + c_m + c_d), -1)\n",
    "        return c\n",
    "    \n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx, entities=True):\n",
    "        start_time_idx = self.time_trans[start_time_idx]\n",
    "        b_size = h_idx.shape[0]\n",
    "        h,r,t = self.getEmbeddings(heads_idx = h_idx, rels_idx = r_idx, tails_idx= t_idx, start_time_idx=start_time_idx, end_time_idx=end_time_idx, intervals = None)\n",
    "        if entities:\n",
    "            candidates = self.get_candidate_embedding(start_time_idx)\n",
    "        else:\n",
    "            candidates = self.rel_emb.weight.data.view(1, self.n_rel, self.emb_dim)\n",
    "            candidates = candidates.expand(b_size, self.n_rel, self.emb_dim + self.time_emb_dim)\n",
    "\n",
    "        return h, t, r, candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a72bb9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, args):\n",
    "    if model_name == 'DETransEModel':\n",
    "        return DETransEModel(args)\n",
    "    elif model_name == 'DEDistMultModel':\n",
    "        return DEDistMultModel(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc450d42",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f3a0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class temporal_train_loop():\n",
    "    def __init__(self, kg_train, kg_val, kg_test, args):\n",
    "        self.args = args\n",
    "\n",
    "        self.kg_train = kg_train\n",
    "        self.kg_val = kg_val\n",
    "        self.kg_test = kg_test\n",
    "        self.n_epochs = args.n_epochs\n",
    "        self.model_name = args.model_name\n",
    "        self.emb_dim = args.emb_dim\n",
    "        self.lr = args.lr\n",
    "        self.n_epochs = args.n_epochs\n",
    "        self.b_size = args.b_size\n",
    "        self.margin = args.margin\n",
    "        self.model_save_name = args.model_save_name\n",
    "        self.device = device\n",
    "        self.model_path = args.model_path\n",
    "        self.n_neg = args.n_neg\n",
    "\n",
    "#         self.model = TransEModel(self.emb_dim, self.kg_train.n_ent, self.kg_train.n_rel, dissimilarity_type='L2')\n",
    "        self.model = get_model(args.model_name, args)\n",
    "        self.model.time_trans = kg_train.time_trans.to(device)\n",
    "        self.criterion = get_loss(args.loss_name, args)\n",
    "\n",
    "        # Move everything to CUDA if available\n",
    "        if cuda.is_available():\n",
    "            cuda.empty_cache()\n",
    "            self.model = self.model.to(self.device)\n",
    "            if hasattr(self.criterion, 'to'):\n",
    "                self.criterion = self.criterion.to(self.device)\n",
    "\n",
    "        # Define the torch optimizer to be used\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=self.lr, weight_decay=0.0)\n",
    "#         self.sampler = BernoulliNegativeSampler(self.kg_train, n_neg = self.n_neg)\n",
    "        self.sampler = UniformNegativeSampler(self.kg_train, n_neg = self.n_neg)\n",
    "        self.dataloader = TempDataLoader(self.kg_train, batch_size=self.b_size, use_cuda=None)\n",
    "\n",
    "    def save_model(self,):\n",
    "        torch.save(self.model.state_dict(), join(self.model_path,self.model_save_name))\n",
    "\n",
    "    def evaluation(self, ):\n",
    "        with torch.no_grad():\n",
    "            evaluator = TemporalLinkPredictionEvaluator(self.model, self.kg_val)\n",
    "            evaluator.evaluate(b_size=50, verbose=False)\n",
    "            val_res = evaluator.get_results()\n",
    "            val_filt_mrr = val_res['filt_mrr']\n",
    "            # print(val_filt_mrr)\n",
    "            return val_filt_mrr, val_res\n",
    "    \n",
    "    def test(self,):\n",
    "        with torch.no_grad():\n",
    "            self.model = self.model.to('cpu')\n",
    "            del self.model\n",
    "            torch.cuda.empty_cache()\n",
    "            self.model = get_model(model_name = self.model_name, args = self.args)\n",
    "            self.model.load_state_dict(torch.load(join(self.model_path,self.model_save_name)))\n",
    "            self.model.time_trans = self.kg_train.time_trans.to(device)\n",
    "            \n",
    "            os.remove(join(self.model_path,self.model_save_name))\n",
    "            self.model = self.model.to(device=device)\n",
    "            evaluator = TemporalLinkPredictionEvaluator(self.model, self.kg_test)\n",
    "            evaluator.evaluate(b_size=32)\n",
    "            test_res = evaluator.get_results()\n",
    "            return test_res\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        iterator = tqdm(range(self.n_epochs), unit='epoch')\n",
    "        eval_epoch = 1\n",
    "        best_val_mrr = -100\n",
    "        best_epoch = 0\n",
    "        patience_max = 50\n",
    "        patience_counter = 0\n",
    "        train_evolution = []\n",
    "\n",
    "        for epoch in iterator:\n",
    "            running_loss = 0.0\n",
    "            epoch_log = {}\n",
    "            for i, batch in enumerate(self.dataloader):\n",
    "                h, t, r, start_time, end_time = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n",
    "                # print('h.shape ooooo ',h.shape)\n",
    "                n_h, n_t = self.sampler.corrupt_batch(h, t, r)                \n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                pos, neg = self.model(h, t, r, start_time, end_time, n_h, n_t)\n",
    "                loss = self.criterion(pos, neg)                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "            epoch_log['avg_train_loss'] = running_loss / len(self.dataloader)\n",
    "            if epoch % eval_epoch == 0:\n",
    "                val_mrr,  val_res = self.evaluation()\n",
    "                epoch_log['val_res'] = val_res\n",
    "                if (best_val_mrr < val_mrr) & (val_mrr<float('inf')):\n",
    "                    best_val_mrr = val_mrr\n",
    "                    best_epoch = epoch + 1\n",
    "                    # save the model\n",
    "                    self.save_model()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter == patience_max:\n",
    "                        break\n",
    "            train_evolution.append(epoch_log)\n",
    "            iterator.set_description(\n",
    "                'Epoch {} | mean loss: {:.5f}| best mrr: {:.5f} | Best Epoch {} '.format(epoch + 1,\n",
    "                                                      running_loss / len(self.dataloader), best_val_mrr, best_epoch))\n",
    "\n",
    "        # self.model.normalize_parameters()\n",
    "        return train_evolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e613d6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fe961eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = Params()\n",
    "\n",
    "# args.n_epochs = 1000\n",
    "# args.margin = 200\n",
    "# args.data_home = '/home/vortex/Desktop/Projects/TempTorchKGE_V2/datasets'\n",
    "# args.model_save_name = 'best_de_model.bt'\n",
    "# args.loss_name = 'MarginLoss' # LogisticLoss MarginLoss CrossEntropy BinaryCrossEntropyLoss\n",
    "# args.model_path = 'best_models'\n",
    "# args.time_mode = 'ymd_'\n",
    "# args.n_neg = 500\n",
    "# args.model_name = 'DEDistMultModel' # 'DETransEModel','DEDistMultModel'\n",
    "# args.dataset = \"edukg\"\n",
    "# kg_train, kg_val, kg_test = get_data(args.dataset, args.time_mode)\n",
    "# args.n_entities = kg_train.n_ent\n",
    "# args.n_relations = kg_train.n_rel\n",
    "# args.tem_total = kg_train.n_time\n",
    "# args.b_size = 1000\n",
    "# args.lr = 0.001\n",
    "# args.emb_dim = 100\n",
    "# args.s_rep_ratio = 0.2\n",
    "# args.dissimilarity_type = 'L2'\n",
    "# args.device = device\n",
    "# args.dropout = 0.2\n",
    "\n",
    "# tmp_trainlp = temporal_train_loop(kg_train, kg_val, kg_test, args)\n",
    "# train_evolution = tmp_trainlp.run()\n",
    "# test_res = tmp_trainlp.test()\n",
    "# train_log = {}\n",
    "\n",
    "# train_log['train_params'] = args.__dict__\n",
    "# train_log['train_evolution'] = train_evolution\n",
    "# train_log['test_results'] = test_res\n",
    "# # write_json_lines(file_name = 'TA_experiments_log.jsonl',dict_data = train_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0af452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyper_params(file_path):\n",
    "    all_res = read_json_lines(file_name = file_path)\n",
    "    all_res_df = []\n",
    "    for res_ in  all_res:\n",
    "        train_params = res_['train_params']\n",
    "        train_params.update(res_['test_results'])\n",
    "        all_res_df.append(train_params)\n",
    "    return pd.DataFrame(all_res_df)\n",
    "\n",
    "def check_results_existence(param_df, query_params):\n",
    "    param_df_c = param_df.copy()\n",
    "    for k in query_params:\n",
    "        param_df_c = param_df_c[param_df_c[k] == query_params[k]]\n",
    "    if param_df_c.shape[0]==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_save_file = 'edukg_experiments.jsonl'\n",
    "pra_df = get_hyper_params(res_save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2694c729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs:1000|data_home:/home/vortex/Desktop/Projects/TempTorchKGE_V2/datasets|model_save_name:DEDistMultModel.bt|loss_name:MarginLoss|model_path:best_models|time_mode:ymd_|dissimilarity_type:L2|dropout:0.2|model_name:DEDistMultModel|dataset:edukg|n_entities:13346|n_relations:12|tem_total:2012|s_rep_ratio:0.1|b_size:2000|lr:0.01|emb_dim:100|margin:1|n_neg:10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa605ba35d504791b474642ca14b5043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(k)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m     32\u001b[0m tmp_trainlp \u001b[38;5;241m=\u001b[39m temporal_train_loop(kg_train, kg_val, kg_test, args)\n\u001b[0;32m---> 33\u001b[0m train_evolution \u001b[38;5;241m=\u001b[39m \u001b[43mtmp_trainlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m test_res \u001b[38;5;241m=\u001b[39m tmp_trainlp\u001b[38;5;241m.\u001b[39mtest()\n\u001b[1;32m     35\u001b[0m train_log \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[21], line 93\u001b[0m, in \u001b[0;36mtemporal_train_loop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m epoch_log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_train_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m eval_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     val_mrr,  val_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     epoch_log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_res\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_res\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (best_val_mrr \u001b[38;5;241m<\u001b[39m val_mrr) \u001b[38;5;241m&\u001b[39m (val_mrr\u001b[38;5;241m<\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n",
      "Cell \u001b[0;32mIn[21], line 44\u001b[0m, in \u001b[0;36mtemporal_train_loop.evaluation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     43\u001b[0m     evaluator \u001b[38;5;241m=\u001b[39m TemporalLinkPredictionEvaluator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkg_val)\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     val_res \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mget_results()\n\u001b[1;32m     46\u001b[0m     val_filt_mrr \u001b[38;5;241m=\u001b[39m val_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilt_mrr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[14], line 39\u001b[0m, in \u001b[0;36mTemporalLinkPredictionEvaluator.evaluate\u001b[0;34m(self, b_size, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilt_rank_true_tails[i \u001b[38;5;241m*\u001b[39m b_size: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m b_size] \u001b[38;5;241m=\u001b[39m get_rank(filt_scores, t_idx)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     38\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minference_scoring_function(candidates, t_emb, r_emb, start_time, end_time)\n\u001b[0;32m---> 39\u001b[0m filt_scores \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_dict_of_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank_true_heads[i \u001b[38;5;241m*\u001b[39m b_size: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m b_size] \u001b[38;5;241m=\u001b[39m get_rank(scores, h_idx)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilt_rank_true_heads[i \u001b[38;5;241m*\u001b[39m b_size: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m b_size] \u001b[38;5;241m=\u001b[39m get_rank(filt_scores, h_idx)\u001b[38;5;241m.\u001b[39mdetach()\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mfilter_scores\u001b[0;34m(scores, dictionary, key1, key2, key3, key4, true_idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m true_targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     filt_scores[i][true_targets] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filt_scores\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dataset in ['edukg']:\n",
    "    for model_name in ['DEDistMultModel']:\n",
    "        args = Params()\n",
    "        args.n_epochs = 1000\n",
    "        args.data_home = '/home/vortex/Desktop/Projects/TempTorchKGE_V2/datasets'\n",
    "        args.model_save_name = model_name + '.bt'\n",
    "        args.loss_name = 'MarginLoss'\n",
    "        args.model_path = 'best_models'\n",
    "        args.time_mode = 'ymd_'\n",
    "        args.dissimilarity_type = 'L2'\n",
    "        args.dropout = 0.2\n",
    "        args.model_name = model_name # 'TTransEModel','TDistMultModel'\n",
    "        args.dataset = dataset\n",
    "        kg_train, kg_val, kg_test = get_data(args.dataset, args.time_mode)\n",
    "        args.n_entities = kg_train.n_ent\n",
    "        args.n_relations = kg_train.n_rel\n",
    "        args.tem_total = kg_train.n_time\n",
    "        for s_rep_ratio in [0.1, 0.2, 0.3, 0.4]:\n",
    "            for lr in [0.01, 0.0001]:\n",
    "                for margin in [1, 5, 20, 50, 100]:\n",
    "                    for n_neg in [10, 30, 40, 100]:\n",
    "                        for b_size in [2000, 4000]:\n",
    "                            for emb_dim in [100, 50, 150]:\n",
    "\n",
    "                                args.s_rep_ratio = s_rep_ratio\n",
    "                                args.b_size = b_size\n",
    "                                args.lr = lr\n",
    "                                args.emb_dim = emb_dim\n",
    "                                args.margin = margin\n",
    "                                args.n_neg = n_neg\n",
    "                                print('|'.join(str(k)+':'+str(v) for k, v in args.__dict__.items()))\n",
    "                                if not check_results_existence(param_df = pra_df, query_params = args.__dict__):\n",
    "                                    tmp_trainlp = temporal_train_loop(kg_train, kg_val, kg_test, args)\n",
    "                                    train_evolution = tmp_trainlp.run()\n",
    "                                    test_res = tmp_trainlp.test()\n",
    "                                    train_log = {}\n",
    "\n",
    "                                    train_log['train_params'] = args.__dict__\n",
    "                                    train_log['train_evolution'] = train_evolution\n",
    "                                    train_log['test_results'] = test_res\n",
    "\n",
    "                                    write_json_lines(file_name = res_save_file,dict_data = train_log)\n",
    "                                else:\n",
    "                                    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9637598e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
