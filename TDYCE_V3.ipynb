{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5dd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import empty, zeros, cat\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from torchkge.data_structures import SmallKG\n",
    "from torchkge.exceptions import NotYetEvaluatedError\n",
    "from torchkge.sampling import PositionalNegativeSampler\n",
    "from torchkge.utils import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch\n",
    "import os\n",
    "from os.path import join\n",
    "# from evaluation import TemporalLinkPredictionEvaluator\n",
    "\n",
    "from torchkge.utils import datasets\n",
    "import torch\n",
    "from pandas import read_csv, concat\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from torchkge.utils import datasets\n",
    "import torch\n",
    "from pandas import read_csv, concat\n",
    "# from data_utils import TemporalKnowledgeGraph\n",
    "from numpy.random import RandomState\n",
    "\n",
    "from torch import tensor, bernoulli, randint, ones, rand, cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73d71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from os import environ, makedirs\n",
    "from os.path import exists, expanduser, join\n",
    "from torch.utils.data import Dataset\n",
    "from torchkge.exceptions import SizeMismatchError, WrongArgumentsError, SanityError\n",
    "from torchkge.utils.operations import get_dictionaries\n",
    "from torch import cat, eq, int64, long, randperm, tensor, Tensor, zeros_like\n",
    "from collections import defaultdict\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "# from utils import get_temporal_dictionaries\n",
    "from pandas import read_csv, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3813bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.optim import Adam\n",
    "from torchkge.sampling import BernoulliNegativeSampler, UniformNegativeSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe95697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkge.utils import MarginLoss, BinaryCrossEntropyLoss\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from abc import ABC, abstractmethod\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869f41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "from torchkge.utils.dissimilarities import l1_dissimilarity, l2_dissimilarity, \\\n",
    "    l1_torus_dissimilarity, l2_torus_dissimilarity, el2_torus_dissimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583a6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from torch import tensor, bernoulli, randint, ones, rand, cat\n",
    "\n",
    "from torchkge.exceptions import NotYetImplementedError\n",
    "from torchkge.utils.data import DataLoader\n",
    "from torchkge.utils.operations import get_bernoulli_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa4bab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affeb98c",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "359b2e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tem_dict = {\n",
    "    '0y': 0, '1y': 1, '2y': 2, '3y': 3, '4y': 4, '5y': 5, '6y': 6, '7y': 7, '8y': 8, '9y': 9,\n",
    "    '0m': 10, '1m': 11, '2m': 12, '3m': 13, '4m': 14, '5m': 15, '6m': 16, '7m': 17, '8m': 18, '9m': 19,\n",
    "    '0d': 20, '1d': 21, '2d': 22, '3d': 23, '4d': 24, '5d': 25, '6d': 26, '7d': 27, '8d': 28, '9d': 29\n",
    "}\n",
    "\n",
    "class Params():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "def read_json_lines(file_name):\n",
    "    lines = []\n",
    "    with open(file_name) as file_in:\n",
    "        for line in file_in:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "def get_expriment_tests(results):\n",
    "    res = []\n",
    "    for exp in results:\n",
    "        train_params = exp['train_params']\n",
    "        test_results = exp['test_results']\n",
    "        train_params.update(test_results)\n",
    "        res.append(train_params)\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "\n",
    "def write_json_lines(file_name,dict_data):\n",
    "    json_string = json.dumps(dict_data)\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(json_string+\"\\n\")\n",
    "\n",
    "def experiment_exists(all_exp_data_df, exp_data_dict):\n",
    "    for k in exp_data_dict:\n",
    "        all_exp_data_df = all_exp_data_df[all_exp_data_df[k]==exp_data_dict[k]]\n",
    "        if all_exp_data_df.shape[0]==0:\n",
    "            return False\n",
    "    return True\n",
    "def check_experiment(exp_file_name):\n",
    "    res = read_json_lines(exp_file_name)\n",
    "    res = get_expriment_tests(res)\n",
    "    return res\n",
    "    # experiment_exists(all_exp_data_df, exp_data_dict)\n",
    "def transform_time_V2(years, months, days):\n",
    "    all_data = []\n",
    "    for year, month, day in zip(years, months, days):\n",
    "        tem_id_list = []\n",
    "        for j in range(len(year)):\n",
    "            token = year[j:j+1]+'y'\n",
    "            tem_id_list.append(tem_dict[token])\n",
    "        # print(tem_id_list)\n",
    "        # exit()\n",
    "\n",
    "        for j in range(1):\n",
    "            # print(month[1])\n",
    "            # exit()\n",
    "            token1 = month[0]+'m'\n",
    "            tem_id_list.append(tem_dict[token1])\n",
    "            token2 = month[0]+'m'\n",
    "            tem_id_list.append(tem_dict[token2])\n",
    "\n",
    "\n",
    "        for j in range(len(day)):\n",
    "            token = day[j:j+1]+'d'\n",
    "            tem_id_list.append(tem_dict[token])\n",
    "            \n",
    "        all_data.append(torch.tensor(tem_id_list))\n",
    "    return all_data\n",
    "\n",
    "def transform_time(raw_time):\n",
    "    year, month, day = raw_time.split(\"-\")\n",
    "    tem_id_list = []\n",
    "    for j in range(len(year)):\n",
    "        token = year[j:j+1]+'y'\n",
    "        tem_id_list.append(tem_dict[token])\n",
    "    # print(tem_id_list)\n",
    "    # exit()\n",
    "\n",
    "    for j in range(1):\n",
    "        # print(month[1])\n",
    "        # exit()\n",
    "        token1 = month[0]+'m'\n",
    "        tem_id_list.append(tem_dict[token1])\n",
    "        token2 = month[0]+'m'\n",
    "        tem_id_list.append(tem_dict[token2])\n",
    "\n",
    "\n",
    "    for j in range(len(day)):\n",
    "        token = day[j:j+1]+'d'\n",
    "        tem_id_list.append(tem_dict[token])\n",
    "    return tem_id_list\n",
    "\n",
    "def transform_time_v3(raw_time):\n",
    "    date = list(map(float, raw_time.split(\"-\")))\n",
    "    return date\n",
    "\n",
    "def transform_time_v4(raw_time):\n",
    "    year, month, day = raw_time.split(\"-\")\n",
    "    year, month, day = int(year), int(month), int(day)\n",
    "    return month + day\n",
    "\n",
    "def transform_time_v5(raw_time):\n",
    "    year, month, day = raw_time.split(\"-\")\n",
    "    return [int(year), int(month), int(day)]\n",
    "\n",
    "def get_temporal_dictionaries(df, mode='simple'):\n",
    "\n",
    "    tmp = list(set(df['start_time'].unique()).union(set(df['end_time'].unique())))\n",
    "    if mode == 'simple':\n",
    "        # return {timee: i for i, timee in enumerate(sorted(tmp))}\n",
    "        return {timee: i for i, timee in enumerate(sorted([datetime.strptime(dt, \"%Y-%m-%d\") for dt in tmp]))}\n",
    "    elif mode == 'seq':\n",
    "        return {timee: torch.tensor(transform_time(timee)) for i, timee in enumerate(sorted(tmp))}\n",
    "        # return {timee: torch.tensor(transform_time(timee)) for i, timee in enumerate(sorted([datetime.strptime(dt, \"%Y-%m-%d\") for dt in tmp]))}\n",
    "    elif mode == 'ymd':\n",
    "        return {timee: transform_time_v4(timee) for i, timee in enumerate(sorted(tmp))}\n",
    "    elif mode == 'ymd_':\n",
    "        return {timee: torch.tensor(transform_time_v5(timee)) for i, timee in enumerate(sorted(tmp))}\n",
    "\n",
    "\n",
    "def cconv(a, b):\n",
    "    return torch.fft.ifft(torch.fft.fft(a) * torch.fft.fft(b)).real\n",
    "\n",
    "\n",
    "def ccorr(a, b):\n",
    "    return torch.fft.ifft(torch.conj(torch.fft.fft(a)) * torch.fft.fft(b)).real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf90d2c",
   "metadata": {},
   "source": [
    "##  Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "722f96ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalKnowledgeGraph(Dataset):\n",
    "\n",
    "    def __init__(self, df=None, time_mode = None,kg=None, ent2ix=None, rel2ix=None, time2ix = None,\n",
    "                 dict_of_heads=None, dict_of_tails=None, dict_of_rels=None,\n",
    "                 temp_dict_of_heads=None, temp_dict_of_tails=None, temp_dict_of_rels=None):\n",
    "\n",
    "        if df is None:\n",
    "            if kg is None:\n",
    "                raise WrongArgumentsError(\"Please provide at least one \"\n",
    "                                          \"argument of `df` and kg`\")\n",
    "            else:\n",
    "                try:\n",
    "                    assert (type(kg) == dict) & ('heads' in kg.keys()) & \\\n",
    "                           ('tails' in kg.keys()) & \\\n",
    "                           ('relations' in kg.keys())& \\\n",
    "                            ('start_time' in kg.keys())& \\\n",
    "                            ('end_time' in kg.keys())\n",
    "                    \n",
    "                except AssertionError:\n",
    "                    raise WrongArgumentsError(\"Keys in the `kg` dict should \"\n",
    "                                              \"contain `heads`, `tails`, \"\n",
    "                                              \"`relations`.\")\n",
    "                try:\n",
    "                    assert (rel2ix is not None) & (ent2ix is not None)\n",
    "                except AssertionError:\n",
    "                    raise WrongArgumentsError(\"Please provide the two \"\n",
    "                                              \"dictionaries ent2ix and rel2ix \"\n",
    "                                              \"if building from `kg`.\")\n",
    "        else:\n",
    "            if kg is not None:\n",
    "                raise WrongArgumentsError(\"`df` and kg` arguments should not \"\n",
    "                                          \"both be provided.\")\n",
    "\n",
    "        if ent2ix is None:\n",
    "            self.ent2ix = get_dictionaries(df, ent=True)\n",
    "        else:\n",
    "            self.ent2ix = ent2ix\n",
    "\n",
    "        if rel2ix is None:\n",
    "            self.rel2ix = get_dictionaries(df, ent=False)\n",
    "        else:\n",
    "            self.rel2ix = rel2ix\n",
    "        \n",
    "        if time_mode is not None:\n",
    "            self.time_mode = time_mode\n",
    "            \n",
    "        if time2ix is None:\n",
    "            self.time2ix = get_temporal_dictionaries(df, mode = self.time_mode)\n",
    "        else:\n",
    "            self.time2ix = time2ix\n",
    "        \n",
    "        self.n_ent = max(self.ent2ix.values()) + 1\n",
    "        self.n_rel = max(self.rel2ix.values()) + 1\n",
    "        time_val = list(self.time2ix.values())\n",
    "        \n",
    "        if isinstance(time_val[0], torch.Tensor):\n",
    "            self.n_time = int(torch.cat(time_val).max()) + 1\n",
    "        else: \n",
    "            self.n_time = max(time_val) + 1\n",
    "            \n",
    "#         print('self.n_time: ',self.n_time)\n",
    "        if df is not None:\n",
    "            # build kg from a pandas dataframe\n",
    "            self.n_facts = len(df)\n",
    "            self.head_idx = tensor(df['from'].map(self.ent2ix).values).long()\n",
    "            self.tail_idx = tensor(df['to'].map(self.ent2ix).values).long()\n",
    "            self.relations = tensor(df['rel'].map(self.rel2ix).values).long()\n",
    "#             self.start_time = tensor(df['start_time'].map(self.rel2ix).values).long()\n",
    "#             self.end_time = tensor(df['end_time'].map(self.rel2ix).values).long()\n",
    "#             print(self.time2ix)\n",
    "            self.start_time = list(df['start_time'].map(self.time2ix).values)\n",
    "#             print(self.start_time)\n",
    "#             print('-'*22)\n",
    "#             print(\"type(self.start_time[0]): \",type(self.start_time[0]))\n",
    "#             print('type(self.start_time): ',type(self.start_time))\n",
    "#             if isinstance(self.start_time[0], torch.Tensor):\n",
    "#                 print('llllll')\n",
    "            if isinstance(self.start_time, list) & isinstance(self.start_time[0], torch.Tensor):\n",
    "                self.start_time = torch.stack(self.start_time)\n",
    "            else:\n",
    "                self.start_time = torch.tensor(self.start_time)\n",
    "            \n",
    "            self.start_time = self.start_time.long()\n",
    "            \n",
    "            self.end_time = list(df['end_time'].map(self.time2ix).values)\n",
    "            if isinstance(self.end_time, list) & isinstance(self.end_time[0], torch.Tensor):\n",
    "                self.end_time = torch.stack(self.end_time)\n",
    "            else:\n",
    "                self.end_time = torch.tensor(self.end_time)   \n",
    "            self.end_time = self.end_time.long()\n",
    "            \n",
    "        else:\n",
    "            # build kg from another kg\n",
    "            self.n_facts = kg['heads'].shape[0]\n",
    "            self.head_idx = kg['heads']\n",
    "            self.tail_idx = kg['tails']\n",
    "            self.relations = kg['relations']\n",
    "            self.start_time = kg['start_time']\n",
    "            self.end_time = kg['end_time']\n",
    "\n",
    "        if dict_of_heads is None or dict_of_tails is None or dict_of_rels is None:\n",
    "            self.dict_of_heads = defaultdict(set)\n",
    "            self.dict_of_tails = defaultdict(set)\n",
    "            self.dict_of_rels = defaultdict(set)\n",
    "            self.temp_dict_of_heads = defaultdict(set)\n",
    "            self.temp_dict_of_tails = defaultdict(set)\n",
    "            self.temp_dict_of_rels = defaultdict(set)\n",
    "#             self.dict_of_start_time = defaultdict(set)\n",
    "#             self.dict_of_end_time = defaultdict(set)\n",
    "            self.evaluate_dicts()\n",
    "\n",
    "        else:\n",
    "            self.dict_of_heads = dict_of_heads\n",
    "            self.dict_of_tails = dict_of_tails\n",
    "            self.dict_of_rels = dict_of_rels\n",
    "            self.temp_dict_of_heads = temp_dict_of_heads\n",
    "            self.temp_dict_of_tails = temp_dict_of_tails\n",
    "            self.temp_dict_of_rels = temp_dict_of_rels\n",
    "#             self.dict_of_start_time = dict_of_start_time\n",
    "#             self.dict_of_end_time = dict_of_end_time\n",
    "        try:\n",
    "            self.sanity_check()\n",
    "        except AssertionError:\n",
    "            raise SanityError(\"Please check the sanity of arguments.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_facts\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (self.head_idx[item].item(),\n",
    "                self.tail_idx[item].item(),\n",
    "                self.relations[item].item())\n",
    "\n",
    "    def sanity_check(self):\n",
    "        assert (type(self.dict_of_heads) == defaultdict) & \\\n",
    "               (type(self.dict_of_tails) == defaultdict) & \\\n",
    "               (type(self.dict_of_rels) == defaultdict) & \\\n",
    "                (type(self.temp_dict_of_heads) == defaultdict) & \\\n",
    "               (type(self.temp_dict_of_tails) == defaultdict) & \\\n",
    "               (type(self.temp_dict_of_rels) == defaultdict)\n",
    "        assert (type(self.ent2ix) == dict) & (type(self.rel2ix) == dict)\n",
    "        assert (len(self.ent2ix) == self.n_ent) & \\\n",
    "               (len(self.rel2ix) == self.n_rel)\n",
    "        assert (type(self.head_idx) == Tensor) & \\\n",
    "               (type(self.tail_idx) == Tensor) & \\\n",
    "               (type(self.relations) == Tensor)\n",
    "        assert (self.head_idx.dtype == int64) & \\\n",
    "               (self.tail_idx.dtype == int64) & (self.relations.dtype == int64)\n",
    "        assert (len(self.head_idx) == len(self.tail_idx) == len(self.relations))\n",
    "\n",
    "    def split_kg(self, share=0.8, sizes=None, validation=False):\n",
    "        if sizes is not None:\n",
    "            try:\n",
    "                if len(sizes) == 3:\n",
    "                    try:\n",
    "                        assert (sizes[0] + sizes[1] + sizes[2] == self.n_facts)\n",
    "                    except AssertionError:\n",
    "                        raise WrongArgumentsError('Sizes should sum to the '\n",
    "                                                  'number of facts.')\n",
    "                elif len(sizes) == 2:\n",
    "                    try:\n",
    "                        assert (sizes[0] + sizes[1] == self.n_facts)\n",
    "                    except AssertionError:\n",
    "                        raise WrongArgumentsError('Sizes should sum to the '\n",
    "                                                  'number of facts.')\n",
    "                else:\n",
    "                    raise SizeMismatchError('Tuple `sizes` should be of '\n",
    "                                            'length 2 or 3.')\n",
    "            except AssertionError:\n",
    "                raise SizeMismatchError('Tuple `sizes` should sum up to the '\n",
    "                                        'number of facts in the knowledge '\n",
    "                                        'graph.')\n",
    "        else:\n",
    "            assert share < 1\n",
    "\n",
    "        if ((sizes is not None) and (len(sizes) == 3)) or \\\n",
    "                ((sizes is None) and validation):\n",
    "            # return training, validation and a testing graphs\n",
    "\n",
    "            if (sizes is None) and validation:\n",
    "                mask_tr, mask_val, mask_te = self.get_mask(share,\n",
    "                                                           validation=True)\n",
    "            else:\n",
    "                mask_tr = cat([tensor([1 for _ in range(sizes[0])]),\n",
    "                               tensor([0 for _ in range(sizes[1] + sizes[2])])]).bool()\n",
    "                mask_val = cat([tensor([0 for _ in range(sizes[0])]),\n",
    "                                tensor([1 for _ in range(sizes[1])]),\n",
    "                                tensor([0 for _ in range(sizes[2])])]).bool()\n",
    "                mask_te = ~(mask_tr | mask_val)\n",
    "\n",
    "            return (TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_tr],\n",
    "                            'tails': self.tail_idx[mask_tr],\n",
    "                            'relations': self.relations[mask_tr],\n",
    "                           'start_time': self.start_time[mask_tr],\n",
    "                           'end_time': self.end_time[mask_tr]},\n",
    "                            ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                            dict_of_heads=self.dict_of_heads,\n",
    "                            dict_of_tails=self.dict_of_tails,\n",
    "                            dict_of_rels=self.dict_of_rels,\n",
    "                            temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                            temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                            temp_dict_of_rels = self.temp_dict_of_rels\n",
    "                            ),\n",
    "                    TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_val],\n",
    "                            'tails': self.tail_idx[mask_val],\n",
    "                            'relations': self.relations[mask_val],\n",
    "                           'start_time':  self.start_time[mask_val],\n",
    "                           'end_time': self.end_time[mask_val]},\n",
    "                        ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                        dict_of_heads=self.dict_of_heads,\n",
    "                        dict_of_tails=self.dict_of_tails,\n",
    "                        dict_of_rels=self.dict_of_rels,\n",
    "                        temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                        temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                        temp_dict_of_rels = self.temp_dict_of_rels),\n",
    "                    TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_te],\n",
    "                            'tails': self.tail_idx[mask_te],\n",
    "                            'relations': self.relations[mask_te],\n",
    "                           'start_time': self.start_time[mask_te],\n",
    "                           'end_time': self.end_time[mask_te]},\n",
    "                        ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                        dict_of_heads=self.dict_of_heads,\n",
    "                        dict_of_tails=self.dict_of_tails,\n",
    "                        dict_of_rels=self.dict_of_rels,\n",
    "                        temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                        temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                        temp_dict_of_rels = self.temp_dict_of_rels\n",
    "                        ))\n",
    "        else:\n",
    "            # return training and testing graphs\n",
    "\n",
    "            assert (((sizes is not None) and len(sizes) == 2) or\n",
    "                    ((sizes is None) and not validation))\n",
    "            if sizes is None:\n",
    "                mask_tr, mask_te = self.get_mask(share, validation=False)\n",
    "            else:\n",
    "                mask_tr = cat([tensor([1 for _ in range(sizes[0])]),\n",
    "                               tensor([0 for _ in range(sizes[1])])]).bool()\n",
    "                mask_te = ~mask_tr\n",
    "            return (TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_tr],\n",
    "                            'tails': self.tail_idx[mask_tr],\n",
    "                            'relations': self.relations[mask_tr],\n",
    "                           'start_time': self.start_time[mask_tr],\n",
    "                           'end_time': self.end_time[mask_tr]},\n",
    "                        ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                        dict_of_heads=self.dict_of_heads,\n",
    "                        dict_of_tails=self.dict_of_tails,\n",
    "                        dict_of_rels=self.dict_of_rels,\n",
    "                        temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                        temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                        temp_dict_of_rels = self.temp_dict_of_rels\n",
    "                        ),\n",
    "                    TemporalKnowledgeGraph(\n",
    "                        kg={'heads': self.head_idx[mask_te],\n",
    "                            'tails': self.tail_idx[mask_te],\n",
    "                            'relations': self.relations[mask_te],\n",
    "                           'start_time': self.start_time[mask_te],\n",
    "                           'end_time': self.end_time[mask_te]},\n",
    "                        ent2ix=self.ent2ix, rel2ix=self.rel2ix, time2ix = self.time2ix,\n",
    "                        dict_of_heads=self.dict_of_heads,\n",
    "                        dict_of_tails=self.dict_of_tails,\n",
    "                        dict_of_rels=self.dict_of_rels,\n",
    "                        temp_dict_of_heads = self.temp_dict_of_heads,\n",
    "                        temp_dict_of_tails = self.temp_dict_of_tails,\n",
    "                        temp_dict_of_rels = self.temp_dict_of_rels\n",
    "                        ))\n",
    "\n",
    "    def get_mask(self, share, validation=False):\n",
    "\n",
    "        uniques_r, counts_r = self.relations.unique(return_counts=True)\n",
    "        uniques_e, _ = cat((self.head_idx,\n",
    "                            self.tail_idx)).unique(return_counts=True)\n",
    "\n",
    "        mask = zeros_like(self.relations).bool()\n",
    "        if validation:\n",
    "            mask_val = zeros_like(self.relations).bool()\n",
    "\n",
    "        # splitting relations among subsets\n",
    "        for i, r in enumerate(uniques_r):\n",
    "            rand = randperm(counts_r[i].item())\n",
    "\n",
    "            # list of indices k such that relations[k] == r\n",
    "            sub_mask = eq(self.relations, r).nonzero(as_tuple=False)[:, 0]\n",
    "\n",
    "            assert len(sub_mask) == counts_r[i].item()\n",
    "\n",
    "            if validation:\n",
    "                train_size, val_size, test_size = self.get_sizes(counts_r[i].item(),\n",
    "                                                                 share=share,\n",
    "                                                                 validation=True)\n",
    "                mask[sub_mask[rand[:train_size]]] = True\n",
    "                mask_val[sub_mask[rand[train_size:train_size + val_size]]] = True\n",
    "\n",
    "            else:\n",
    "                train_size, test_size = self.get_sizes(counts_r[i].item(),\n",
    "                                                       share=share,\n",
    "                                                       validation=False)\n",
    "                mask[sub_mask[rand[:train_size]]] = True\n",
    "\n",
    "        # adding missing entities to the train set\n",
    "        u = cat((self.head_idx[mask], self.tail_idx[mask])).unique()\n",
    "        if len(u) < self.n_ent:\n",
    "            missing_entities = tensor(list(set(uniques_e.tolist()) -\n",
    "                                           set(u.tolist())), dtype=long)\n",
    "            for e in missing_entities:\n",
    "                sub_mask = ((self.head_idx == e) |\n",
    "                            (self.tail_idx == e)).nonzero(as_tuple=False)[:, 0]\n",
    "                rand = randperm(len(sub_mask))\n",
    "                sizes = self.get_sizes(mask.shape[0],\n",
    "                                       share=share,\n",
    "                                       validation=validation)\n",
    "                mask[sub_mask[rand[:sizes[0]]]] = True\n",
    "                if validation:\n",
    "                    mask_val[sub_mask[rand[:sizes[0]]]] = False\n",
    "\n",
    "        if validation:\n",
    "            assert not (mask & mask_val).any().item()\n",
    "            return mask, mask_val, ~(mask | mask_val)\n",
    "        else:\n",
    "            return mask, ~mask\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sizes(count, share, validation=False):\n",
    "        if count == 1:\n",
    "            if validation:\n",
    "                return 1, 0, 0\n",
    "            else:\n",
    "                return 1, 0\n",
    "        if count == 2:\n",
    "            if validation:\n",
    "                return 1, 1, 0\n",
    "            else:\n",
    "                return 1, 1\n",
    "\n",
    "        n_train = int(count * share)\n",
    "        assert n_train < count\n",
    "        if n_train == 0:\n",
    "            n_train += 1\n",
    "\n",
    "        if not validation:\n",
    "            return n_train, count - n_train\n",
    "        else:\n",
    "            if count - n_train == 1:\n",
    "                n_train -= 1\n",
    "                return n_train, 1, 1\n",
    "            else:\n",
    "                n_val = int(int(count - n_train) / 2)\n",
    "                return n_train, n_val, count - n_train - n_val\n",
    "\n",
    "    def evaluate_dicts(self):\n",
    "        for i in range(self.n_facts):\n",
    "            self.dict_of_heads[(self.tail_idx[i].item(),\n",
    "                                self.relations[i].item())].add(self.head_idx[i].item())\n",
    "            self.dict_of_tails[(self.head_idx[i].item(),\n",
    "                                self.relations[i].item())].add(self.tail_idx[i].item())\n",
    "            self.dict_of_rels[(self.head_idx[i].item(),\n",
    "                               self.tail_idx[i].item())].add(self.relations[i].item())\n",
    "            \n",
    "            self.temp_dict_of_heads[(self.tail_idx[i].item(),\n",
    "                                    self.relations[i].item(),\n",
    "                                     self.start_time[i].item(),\n",
    "                                     self.end_time[i].item())].add(self.head_idx[i].item())\n",
    "            self.temp_dict_of_tails[(self.head_idx[i].item(),\n",
    "                                self.relations[i].item(),\n",
    "                                     self.start_time[i].item(),\n",
    "                                     self.end_time[i].item())].add(self.tail_idx[i].item())\n",
    "            self.temp_dict_of_rels[(self.head_idx[i].item(),\n",
    "                                   self.tail_idx[i].item(),\n",
    "                                    self.start_time[i].item(),\n",
    "                                     self.end_time[i].item())].add(self.relations[i].item())\n",
    "\n",
    "    def get_df(self):\n",
    "        ix2ent = {v: k for k, v in self.ent2ix.items()}\n",
    "        ix2rel = {v: k for k, v in self.rel2ix.items()}\n",
    "\n",
    "        df = DataFrame(cat((self.head_idx.view(1, -1),\n",
    "                            self.tail_idx.view(1, -1),\n",
    "                            self.relations.view(1, -1))).transpose(0, 1).numpy(),\n",
    "                       columns=['from', 'to', 'rel'])\n",
    "\n",
    "        df['from'] = df['from'].apply(lambda x: ix2ent[x])\n",
    "        df['to'] = df['to'].apply(lambda x: ix2ent[x])\n",
    "        df['rel'] = df['rel'].apply(lambda x: ix2rel[x])\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class SmallKG(Dataset):\n",
    "    def __init__(self, heads, tails, relations):\n",
    "        assert heads.shape == tails.shape == relations.shape\n",
    "        self.head_idx = heads\n",
    "        self.tail_idx = tails\n",
    "        self.relations = relations\n",
    "        self.length = heads.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.head_idx[item].item(), self.tail_idx[item].item(), self.relations[item].item()\n",
    "\n",
    "\n",
    "\n",
    "def get_data_home(data_home=None):\n",
    "    if data_home is None:\n",
    "        data_home = environ.get('TORCHKGE_DATA',\n",
    "                                join('~', 'torchkge_data'))\n",
    "    data_home = expanduser(data_home)\n",
    "    if not exists(data_home):\n",
    "        makedirs(data_home)\n",
    "    return data_home\n",
    "\n",
    "\n",
    "def clear_data_home(data_home=None):\n",
    "    data_home = get_data_home(data_home)\n",
    "    shutil.rmtree(data_home)\n",
    "\n",
    "\n",
    "def get_n_batches(n, b_size):\n",
    "    n_batch = n // b_size\n",
    "    if n % b_size > 0:\n",
    "        n_batch += 1\n",
    "    return n_batch\n",
    "\n",
    "\n",
    "class TempDataLoader:\n",
    "    \"\"\"This class is inspired from :class:`torch.utils.dataloader.DataLoader`.\n",
    "    It is however way simpler.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, kg, batch_size, use_cuda=None):\n",
    "\n",
    "        self.h = kg.head_idx\n",
    "        self.t = kg.tail_idx\n",
    "        self.r = kg.relations\n",
    "        self.start_time = kg.start_time\n",
    "        self.end_time = kg.end_time\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if use_cuda is not None and use_cuda == 'all':\n",
    "            self.h = self.h.cuda()\n",
    "            self.t = self.t.cuda()\n",
    "            self.r = self.r.cuda()\n",
    "            self.start_time = self.start_time.cuda()\n",
    "            self.end_time = self.end_time.cuda()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return get_n_batches(len(self.h), self.batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return _TempDataLoaderIter(self)\n",
    "\n",
    "\n",
    "class _TempDataLoaderIter:\n",
    "    def __init__(self, loader):\n",
    "        self.h = loader.h\n",
    "        self.t = loader.t\n",
    "        self.r = loader.r\n",
    "        self.start_time = loader.start_time\n",
    "        self.end_time = loader.end_time\n",
    "        \n",
    "        self.use_cuda = loader.use_cuda\n",
    "        self.batch_size = loader.batch_size\n",
    "\n",
    "        self.n_batches = get_n_batches(len(self.h), self.batch_size)\n",
    "        self.current_batch = 0\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_batch == self.n_batches:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            i = self.current_batch\n",
    "            self.current_batch += 1\n",
    "\n",
    "            tmp_h = self.h[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            tmp_t = self.t[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            tmp_r = self.r[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            tmp_start_time = self.start_time[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            tmp_end_time = self.end_time[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            \n",
    "            if self.use_cuda is not None and self.use_cuda == 'batch':\n",
    "                return tmp_h.cuda(), tmp_t.cuda(), tmp_r.cuda(), tmp_start_time.cuda(), tmp_end_time.cuda()\n",
    "            else:\n",
    "                return tmp_h, tmp_t, tmp_r, tmp_start_time, tmp_end_time\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "def load_icews14(data_home=None):\n",
    "    data_path = data_home + '/icews14'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode='seq')\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc3cd8",
   "metadata": {},
   "source": [
    "## Quat Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e40ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_mul_with_unit_norm(*, Q_1, Q_2):\n",
    "    a_h, b_h, c_h, d_h = Q_1  # = {a_h + b_h i + c_h j + d_h k : a_r, b_r, c_r, d_r \\in R^k}\n",
    "    a_r, b_r, c_r, d_r = Q_2  # = {a_r + b_r i + c_r j + d_r k : a_r, b_r, c_r, d_r \\in R^k}\n",
    "\n",
    "    # Normalize the relation to eliminate the scaling effect\n",
    "    denominator = torch.sqrt(a_r ** 2 + b_r ** 2 + c_r ** 2 + d_r ** 2)\n",
    "    p = a_r / denominator\n",
    "    q = b_r / denominator\n",
    "    u = c_r / denominator\n",
    "    v = d_r / denominator\n",
    "    #  Q'=E Hamilton product R\n",
    "    r_val = a_h * p - b_h * q - c_h * u - d_h * v\n",
    "    i_val = a_h * q + b_h * p + c_h * v - d_h * u\n",
    "    j_val = a_h * u - b_h * v + c_h * p + d_h * q\n",
    "    k_val = a_h * v + b_h * u - c_h * q + d_h * p\n",
    "    return r_val, i_val, j_val, k_val\n",
    "\n",
    "\n",
    "def quaternion_mul(*, Q_1, Q_2):\n",
    "    a_h, b_h, c_h, d_h = Q_1  # = {a_h + b_h i + c_h j + d_h k : a_r, b_r, c_r, d_r \\in R^k}\n",
    "    a_r, b_r, c_r, d_r = Q_2  # = {a_r + b_r i + c_r j + d_r k : a_r, b_r, c_r, d_r \\in R^k}\n",
    "    r_val = a_h * a_r - b_h * b_r - c_h * c_r - d_h * d_r\n",
    "    i_val = a_h * b_r + b_h * a_r + c_h * d_r - d_h * c_r\n",
    "    j_val = a_h * c_r - b_h * d_r + c_h * a_r + d_h * b_r\n",
    "    k_val = a_h * d_r + b_h * c_r - c_h * b_r + d_h * a_r\n",
    "    return r_val, i_val, j_val, k_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460ef98",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8c36ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_name, time_mode):\n",
    "    if data_name == 'icews14':\n",
    "        return load_icews14(data_home='data', time_mode = time_mode)\n",
    "    elif data_name == 'gdelt':\n",
    "        return load_gdelt(data_home='data', time_mode = time_mode)\n",
    "    elif data_name == 'icews15':\n",
    "        return load_icews15(data_home='data', time_mode = time_mode)\n",
    "    elif data_name == 'yago11k':\n",
    "        return load_yago11k(data_home='data', time_mode = time_mode)\n",
    "    elif data_name == 'wikidata12k':\n",
    "        return load_wikidata12k(data_home='data', time_mode = time_mode)\n",
    "    else:\n",
    "        datas = ['icews14']\n",
    "        \n",
    "        print('Choose One of the Following Datasets: ',datas)\n",
    "\n",
    "def load_icews14(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/icews14'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "def load_gdelt(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/gdelt'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time'])\n",
    "    df1['end_time'] = '1111-11-11'\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time'])\n",
    "    df2['end_time'] = '1111-11-11'\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time'])\n",
    "    df3['end_time'] = '1111-11-11'\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "def load_icews15(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/icews05-15'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "def load_yago11k(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/YAGO11k'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n",
    "\n",
    "def load_wikidata12k(data_home=None, time_mode=None):\n",
    "    data_path = data_home + '/WIKIDATA12k'\n",
    "\n",
    "    df1 = read_csv(data_path + '/train.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df2 = read_csv(data_path + '/valid.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df3 = read_csv(data_path + '/test.txt',\n",
    "                   sep='\\t', header=None, names=['from', 'rel', 'to', 'start_time', 'end_time'])\n",
    "    df = concat([df1, df2, df3])\n",
    "    kg = TemporalKnowledgeGraph(df, time_mode = time_mode)\n",
    "\n",
    "    return kg.split_kg(sizes=(len(df1), len(df2), len(df3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27323c0f",
   "metadata": {},
   "source": [
    "## Eval Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6fd6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_targets(dictionary, key1, key2, key3, key4, true_idx, i):\n",
    "    try:\n",
    "        if key4 is not None:\n",
    "            true_targets = dictionary[key1[i].item(), key2[i].item(), key3[i].item(), key4[i].item()].copy()\n",
    "        else:\n",
    "            true_targets = dictionary[key1[i].item(), key2[i].item(), key3[i].item()].copy()\n",
    "            \n",
    "        if true_idx is not None:\n",
    "            true_targets.remove(true_idx[i].item())\n",
    "            if len(true_targets) > 0:\n",
    "                return tensor(list(true_targets)).long()\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return tensor(list(true_targets)).long()\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d0dd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_scores(scores, self.kg.dict_of_tails, h_idx, r_idx, t_idx, start_time, end_time)\n",
    "def filter_scores(scores, dictionary, key1, key2, key3, key4, true_idx):\n",
    "    # filter out the true negative samples by assigning - inf score.\n",
    "    b_size = scores.shape[0]\n",
    "    filt_scores = scores.clone()\n",
    "\n",
    "    for i in range(b_size):\n",
    "        true_targets = get_true_targets(dictionary, key1, key2, key3, key4, true_idx, i)\n",
    "        if true_targets is None:\n",
    "            continue\n",
    "        filt_scores[i][true_targets] = - float('Inf')\n",
    "\n",
    "    return filt_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd6c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(data, true, low_values=False):\n",
    "    true_data = data.gather(1, true.long().view(-1, 1))\n",
    "\n",
    "    if low_values:\n",
    "        return (data <= true_data).sum(dim=1)\n",
    "    else:\n",
    "        return (data >= true_data).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d847a",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "354405c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalLinkPredictionEvaluator(object):\n",
    "\n",
    "    def __init__(self, model, knowledge_graph):\n",
    "        self.model = model\n",
    "        self.kg = knowledge_graph\n",
    "\n",
    "        self.rank_true_heads = empty(size=(knowledge_graph.n_facts,)).long()\n",
    "        self.rank_true_tails = empty(size=(knowledge_graph.n_facts,)).long()\n",
    "        self.filt_rank_true_heads = empty(size=(knowledge_graph.n_facts,)).long()\n",
    "        self.filt_rank_true_tails = empty(size=(knowledge_graph.n_facts,)).long()\n",
    "\n",
    "        self.evaluated = False\n",
    "\n",
    "    def evaluate(self, b_size, verbose=True):\n",
    "        use_cuda = next(self.model.parameters()).is_cuda\n",
    "\n",
    "        if use_cuda:\n",
    "            dataloader = TempDataLoader(self.kg, batch_size=b_size, use_cuda='batch')\n",
    "            self.rank_true_heads = self.rank_true_heads.cuda()\n",
    "            self.rank_true_tails = self.rank_true_tails.cuda()\n",
    "            self.filt_rank_true_heads = self.filt_rank_true_heads.cuda()\n",
    "            self.filt_rank_true_tails = self.filt_rank_true_tails.cuda()\n",
    "        else:\n",
    "            dataloader = TempDataLoader(self.kg, batch_size=b_size)\n",
    "\n",
    "        for i, batch in tqdm(enumerate(dataloader), total=len(dataloader),\n",
    "                             unit='batch', disable=(not verbose),\n",
    "                             desc='Link prediction evaluation'):\n",
    "            h_idx, t_idx, r_idx, start_time, end_time = batch[0], batch[1], batch[2], batch[3], batch[4]\n",
    "                        \n",
    "            h_emb, t_emb, r_emb, candidates = self.model.inference_prepare_candidates(h_idx, t_idx, r_idx, start_time, end_time, entities=True)\n",
    "\n",
    "            scores = self.model.inference_scoring_function(h_emb, candidates, r_emb, start_time, end_time)\n",
    "            filt_scores = filter_scores(scores, self.kg.temp_dict_of_tails, h_idx, r_idx, start_time, end_time, t_idx)\n",
    "            self.rank_true_tails[i * b_size: (i + 1) * b_size] = get_rank(scores, t_idx).detach()\n",
    "            self.filt_rank_true_tails[i * b_size: (i + 1) * b_size] = get_rank(filt_scores, t_idx).detach()\n",
    "\n",
    "            scores = self.model.inference_scoring_function(candidates, t_emb, r_emb, start_time, end_time)\n",
    "            filt_scores = filter_scores(scores, self.kg.temp_dict_of_heads, t_idx, r_idx, start_time, end_time, h_idx)\n",
    "            self.rank_true_heads[i * b_size: (i + 1) * b_size] = get_rank(scores, h_idx).detach()\n",
    "            self.filt_rank_true_heads[i * b_size: (i + 1) * b_size] = get_rank(filt_scores, h_idx).detach()\n",
    "\n",
    "        self.evaluated = True\n",
    "\n",
    "        if use_cuda:\n",
    "            self.rank_true_heads = self.rank_true_heads.cpu()\n",
    "            self.rank_true_tails = self.rank_true_tails.cpu()\n",
    "            self.filt_rank_true_heads = self.filt_rank_true_heads.cpu()\n",
    "            self.filt_rank_true_tails = self.filt_rank_true_tails.cpu()\n",
    "\n",
    "    def mean_rank(self):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "        sum_ = (self.rank_true_heads.float().mean() +\n",
    "                self.rank_true_tails.float().mean()).item()\n",
    "        filt_sum = (self.filt_rank_true_heads.float().mean() +\n",
    "                    self.filt_rank_true_tails.float().mean()).item()\n",
    "        # return sum_ / 2, filt_sum / 2\n",
    "        return {'mr':sum_ / 2, 'filt_mr':filt_sum / 2}\n",
    "\n",
    "    def hit_at_k_heads(self, k=10):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "        head_hit = (self.rank_true_heads <= k).float().mean()\n",
    "        filt_head_hit = (self.filt_rank_true_heads <= k).float().mean()\n",
    "\n",
    "        # return head_hit.item(), filt_head_hit.item()\n",
    "        return {'head_hit_'+str(k):head_hit.item(), 'filt_head_hit_'+str(k): filt_head_hit.item()}\n",
    "\n",
    "    def hit_at_k_tails(self, k=10):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "        tail_hit = (self.rank_true_tails <= k).float().mean()\n",
    "        filt_tail_hit = (self.filt_rank_true_tails <= k).float().mean()\n",
    "\n",
    "        # return tail_hit.item(), filt_tail_hit.item()\n",
    "        return {'tail_hit_'+str(k):tail_hit.item(), 'filt_tail_hit_'+str(k):filt_tail_hit.item()}\n",
    "\n",
    "    def hit_at_k(self, k=10):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "\n",
    "        head_hit = self.hit_at_k_heads(k=k)\n",
    "        head_hit, filt_head_hit = head_hit['head_hit_'+str(k)], head_hit['filt_head_hit_'+str(k)]\n",
    "\n",
    "        tail_hit = self.hit_at_k_tails(k=k)\n",
    "        tail_hit, filt_tail_hit = tail_hit['tail_hit_'+str(k)], tail_hit['filt_tail_hit_'+str(k)]\n",
    "\n",
    "        # return (head_hit + tail_hit) / 2, (filt_head_hit + filt_tail_hit) / 2\n",
    "        return {'hit_'+str(k): (head_hit + tail_hit) / 2, 'filt_hit_'+str(k): (filt_head_hit + filt_tail_hit) / 2}\n",
    "\n",
    "    def mrr(self):\n",
    "        if not self.evaluated:\n",
    "            raise NotYetEvaluatedError('Evaluator not evaluated call '\n",
    "                                       'LinkPredictionEvaluator.evaluate')\n",
    "        res = {}\n",
    "        head_mrr = (self.rank_true_heads.float()**(-1)).mean()\n",
    "        res['head_mrr'] = head_mrr.item()\n",
    "        tail_mrr = (self.rank_true_tails.float()**(-1)).mean()\n",
    "        res['tail_mrr'] = tail_mrr.item()\n",
    "        filt_head_mrr = (self.filt_rank_true_heads.float()**(-1)).mean()\n",
    "        res['filt_head_mrr'] = filt_head_mrr.item()\n",
    "        filt_tail_mrr = (self.filt_rank_true_tails.float()**(-1)).mean()    \n",
    "        res['filt_tail_mrr'] = filt_tail_mrr.item()\n",
    "        res['mrr'] = (head_mrr + tail_mrr).item() / 2\n",
    "        res['filt_mrr'] = (filt_head_mrr + filt_tail_mrr).item() / 2\n",
    "        return res\n",
    "\n",
    "    def get_results(self, k=None):\n",
    "        res = {}\n",
    "        if k is None:\n",
    "            k = 10\n",
    "\n",
    "        for i in range(1, k + 1):\n",
    "            hits_res = self.hit_at_k(k=i)\n",
    "            res.update(hits_res)\n",
    "\n",
    "        mr_res = self.mean_rank()\n",
    "        res.update(mr_res)\n",
    "        mrr_res = self.mrr()\n",
    "        res.update(mrr_res)\n",
    "        return res\n",
    "\n",
    "    def print_results(self, k=None, n_digits=3):\n",
    "        if k is None:\n",
    "            k = 10\n",
    "\n",
    "        if k is not None and type(k) == int:\n",
    "            print('Hit@{} : {} \\t\\t Filt. Hit@{} : {}'.format(\n",
    "                k, round(self.hit_at_k(k=k)[0], n_digits),\n",
    "                k, round(self.hit_at_k(k=k)[1], n_digits)))\n",
    "        if k is not None and type(k) == list:\n",
    "            for i in k:\n",
    "                print('Hit@{} : {} \\t\\t Filt. Hit@{} : {}'.format(\n",
    "                    i, round(self.hit_at_k(k=i)[0], n_digits),\n",
    "                    i, round(self.hit_at_k(k=i)[1], n_digits)))\n",
    "\n",
    "        print('Mean Rank : {} \\t Filt. Mean Rank : {}'.format(\n",
    "            int(self.mean_rank()[0]), int(self.mean_rank()[1])))\n",
    "        print('MRR : {} \\t\\t Filt. MRR : {}'.format(\n",
    "            round(self.mrr()[0], n_digits), round(self.mrr()[1], n_digits)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9b6474",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cdde9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FullGatherLayer(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gather tensors from all process and support backward propagation\n",
    "    for the gradients across processes.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = [torch.zeros_like(x) for _ in range(dist.get_world_size())]\n",
    "        dist.all_gather(output, x)\n",
    "        return tuple(output)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grads):\n",
    "        all_gradients = torch.stack(grads)\n",
    "        dist.all_reduce(all_gradients)\n",
    "        return all_gradients[dist.get_rank()]\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "def get_class_mean(samples, labels):\n",
    "    M = torch.zeros(labels.max()+1, len(samples)).to(device)\n",
    "    M[labels, torch.arange(len(samples))] = 1\n",
    "    M = torch.nn.functional.normalize(M, p=1, dim=1)\n",
    "    M = torch.mm(M, samples)\n",
    "    return M[labels]\n",
    "\n",
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def compute(self, anchor, sample, pos_mask, neg_mask, *args, **kwargs) -> torch.FloatTensor:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, anchor, sample, pos_mask=None, neg_mask=None, *args, **kwargs) -> torch.FloatTensor:\n",
    "        loss = self.compute(anchor, sample, pos_mask, neg_mask, *args, **kwargs)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "\n",
    "def bt_loss(h1: torch.Tensor, h2: torch.Tensor, lambda_, batch_norm=True, eps=1e-15,loss_type='bt'):\n",
    "    batch_size = h1.size(0)\n",
    "    feature_dim = h1.size(1)\n",
    "\n",
    "    if lambda_ is None:\n",
    "        lambda_ = 1. / feature_dim\n",
    "\n",
    "    if batch_norm:\n",
    "        z1_norm = (h1 - h1.mean(dim=0)) / (h1.std(dim=0) + eps)\n",
    "        z2_norm = (h2 - h2.mean(dim=0)) / (h2.std(dim=0) + eps)\n",
    "        c = (z1_norm.T @ z2_norm) / batch_size\n",
    "    else:\n",
    "        c = h1.T @ h2 / batch_size\n",
    "\n",
    "    off_diagonal_mask = ~torch.eye(feature_dim).bool()\n",
    "    loss = (1 - c.diagonal()).pow(2).sum()\n",
    "    if loss_type == 'bt':\n",
    "        loss += lambda_ * c[off_diagonal_mask].pow(2).sum()\n",
    "    elif loss_type == 'hsic':\n",
    "        loss += lambda_ * c[off_diagonal_mask].add_(1).pow(2).sum()\n",
    "    # return (1 - c.diagonal()).pow(2).sum(), c[off_diagonal_mask].pow(2).sum()\n",
    "    return loss\n",
    "\n",
    "class BarlowTwins(Loss):\n",
    "    def __init__(self, args):\n",
    "        if hasattr(args, 'lambda_'):\n",
    "            self.lambda_ = args.lambda_\n",
    "        else:\n",
    "            self.lambda_ = None\n",
    "\n",
    "        if hasattr(args, 'batch_norm'):\n",
    "            self.batch_norm = args.batch_norm\n",
    "        else:\n",
    "            self.batch_norm = True\n",
    "\n",
    "        if hasattr(args, 'eps'):\n",
    "            self.eps = args.eps\n",
    "        else:\n",
    "            self.eps = 1e-5\n",
    "\n",
    "    def compute(self, anchor, sample, pos_mask, neg_mask, *args, **kwargs) -> torch.FloatTensor:\n",
    "        loss = bt_loss(anchor, sample, self.lambda_, self.batch_norm, self.eps, 'bt')\n",
    "        # cii, cij = bt_loss(anchor, sample, self.lambda_, self.batch_norm, self.eps)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "def get_loss(loss_name, args):\n",
    "    if loss_name == 'MarginLoss':\n",
    "        return MarginLoss(args.margin)\n",
    "    elif loss_name == 'BarlowTwins':\n",
    "        return BarlowTwins(args)\n",
    "    elif loss_name == 'BinaryCrossEntropyLoss':\n",
    "        return BinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1d4ea",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34e061fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NegativeSampler:\n",
    "#     def __init__(self, kg, kg_val=None, kg_test=None, n_neg=1):\n",
    "#         self.kg = kg\n",
    "#         self.n_ent = kg.n_ent\n",
    "#         self.n_facts = kg.n_facts\n",
    "\n",
    "#         self.kg_val = kg_val\n",
    "#         self.kg_test = kg_test\n",
    "\n",
    "#         self.n_neg = n_neg\n",
    "\n",
    "#         if kg_val is None:\n",
    "#             self.n_facts_val = 0\n",
    "#         else:\n",
    "#             self.n_facts_val = kg_val.n_facts\n",
    "\n",
    "#         if kg_test is None:\n",
    "#             self.n_facts_test = 0\n",
    "#         else:\n",
    "#             self.n_facts_test = kg_test.n_facts\n",
    "\n",
    "#     def corrupt_batch(self, heads, tails, relations, n_neg):\n",
    "#         raise NotYetImplementedError('NegativeSampler is just an interface, '\n",
    "#                                      'please consider using a child class '\n",
    "#                                      'where this is implemented.')\n",
    "\n",
    "#     def corrupt_kg(self, batch_size, use_cuda, which='main'):\n",
    "#         assert which in ['main', 'train', 'test', 'val']\n",
    "#         if which == 'val':\n",
    "#             assert self.n_facts_val > 0\n",
    "#         if which == 'test':\n",
    "#             assert self.n_facts_test > 0\n",
    "\n",
    "#         if use_cuda:\n",
    "#             tmp_cuda = 'batch'\n",
    "#         else:\n",
    "#             tmp_cuda = None\n",
    "\n",
    "#         if which == 'val':\n",
    "#             dataloader = DataLoader(self.kg_val, batch_size=batch_size,\n",
    "#                                     use_cuda=tmp_cuda)\n",
    "#         elif which == 'test':\n",
    "#             dataloader = DataLoader(self.kg_test, batch_size=batch_size,\n",
    "#                                     use_cuda=tmp_cuda)\n",
    "#         else:\n",
    "#             dataloader = DataLoader(self.kg, batch_size=batch_size,\n",
    "#                                     use_cuda=tmp_cuda)\n",
    "\n",
    "#         corr_heads, corr_tails = [], []\n",
    "\n",
    "#         for i, batch in enumerate(dataloader):\n",
    "#             heads, tails, rels = batch[0], batch[1], batch[2]\n",
    "#             neg_heads, neg_tails = self.corrupt_batch(heads, tails, rels,\n",
    "#                                                       n_neg=1)\n",
    "\n",
    "#             corr_heads.append(neg_heads)\n",
    "#             corr_tails.append(neg_tails)\n",
    "\n",
    "#         if use_cuda:\n",
    "#             return cat(corr_heads).long().cpu(), cat(corr_tails).long().cpu()\n",
    "#         else:\n",
    "#             return cat(corr_heads).long(), cat(corr_tails).long()\n",
    "\n",
    "\n",
    "# class UniformNegativeSampler(NegativeSampler):\n",
    "\n",
    "#     def __init__(self, kg, kg_val=None, kg_test=None, n_neg=1):\n",
    "#         super().__init__(kg, kg_val, kg_test, n_neg)\n",
    "\n",
    "#     def corrupt_batch(self, heads, tails, relations=None, start_time = None, n_neg=None):\n",
    "#         if n_neg is None:\n",
    "#             n_neg = self.n_neg\n",
    "\n",
    "#         device = heads.device\n",
    "#         assert (device == tails.device)\n",
    "\n",
    "#         batch_size = heads.shape[0]\n",
    "#         neg_heads = heads.repeat(n_neg)\n",
    "#         neg_tails = tails.repeat(n_neg)\n",
    "#         neg_start_time = start_time.repeat(n_neg)\n",
    "        \n",
    "#         # Randomly choose which samples will have head/tail corrupted\n",
    "#         mask = bernoulli(ones(size=(batch_size * n_neg,),\n",
    "#                               device=device) / 2).double()\n",
    "\n",
    "#         n_h_cor = int(mask.sum().item())\n",
    "#         neg_heads[mask == 1] = randint(1, self.n_ent,\n",
    "#                                        (n_h_cor,),\n",
    "#                                        device=device)\n",
    "#         neg_tails[mask == 0] = randint(1, self.n_ent,\n",
    "#                                        (batch_size * n_neg - n_h_cor,),\n",
    "#                                        device=device)\n",
    "#         neg_start_time = randint(1, self.n_ent, (batch_size * n_neg,), device=device)\n",
    "        \n",
    "#         return neg_heads.long(), neg_tails.long(), neg_start_time.long()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ebdb5d",
   "metadata": {},
   "source": [
    "## Interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff36c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempModel(Module):\n",
    "    def __init__(self, n_entities, n_relations):\n",
    "        super().__init__()\n",
    "        self.n_ent = n_entities\n",
    "        self.n_rel = n_relations\n",
    "\n",
    "    def forward(self, heads, tails, relations, start_time, end_time, negative_heads, negative_tails, negative_relations=None):\n",
    "        pos = self.scoring_function(heads, tails, relations, start_time, end_time)\n",
    "\n",
    "        if negative_relations is None:\n",
    "            negative_relations = relations\n",
    "\n",
    "        if negative_heads.shape[0] > negative_relations.shape[0]:\n",
    "            # in that case, several negative samples are sampled from each fact\n",
    "            n_neg = int(negative_heads.shape[0] / negative_relations.shape[0])\n",
    "            # print('pos.shape: ', pos.shape)\n",
    "            pos = pos.repeat(n_neg)\n",
    "            # print('pos.repeat(n_neg).shape: ',pos.repeat(n_neg).shape)\n",
    "            # print(start_time.repeat(n_neg, 1).shape)\n",
    "            # print('start_time.dim: ',start_time.dim())\n",
    "            # print('start_time dim: ',start_time.dim(), 'start_time.shape: ',start_time.shape)\n",
    "            # print(start_time.repeat(n_neg).shape)\n",
    "\n",
    "            # if start_time.dim() ==2 & start_time.shape[-1]>1:\n",
    "            #     start_time = start_time.repeat(n_neg, 1)\n",
    "            # else:\n",
    "            #     start_time = start_time.repeat(n_neg)\n",
    "\n",
    "            neg = self.scoring_function(negative_heads,\n",
    "                                        negative_tails,\n",
    "                                        negative_relations.repeat(n_neg),\n",
    "                                        start_time.repeat(n_neg, 1) if start_time.dim() ==2 & start_time.shape[-1]>1 else start_time.repeat(n_neg),\n",
    "                                        end_time.repeat(n_neg, 1) if end_time.dim() ==2 & end_time.shape[-1]>1 else end_time.repeat(n_neg))\n",
    "        else:\n",
    "            neg = self.scoring_function(negative_heads,\n",
    "                                        negative_tails,\n",
    "                                        negative_relations, start_time, end_time)\n",
    "\n",
    "        return pos, neg\n",
    "\n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_scoring_function(self, h, t, r, time):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx, entities=True):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class TempTranslationModel(TempModel):\n",
    "    def __init__(self, n_entities, n_relations, dissimilarity_type):\n",
    "        super().__init__(n_entities, n_relations)\n",
    "\n",
    "        assert dissimilarity_type in ['L1', 'L2', 'torus_L1', 'torus_L2',\n",
    "                                      'torus_eL2']\n",
    "\n",
    "        if dissimilarity_type == 'L1':\n",
    "            self.dissimilarity = l1_dissimilarity\n",
    "        elif dissimilarity_type == 'L2':\n",
    "            self.dissimilarity = l2_dissimilarity\n",
    "        elif dissimilarity_type == 'torus_L1':\n",
    "            self.dissimilarity = l1_torus_dissimilarity\n",
    "        elif dissimilarity_type == 'torus_L2':\n",
    "            self.dissimilarity = l2_torus_dissimilarity\n",
    "        else:\n",
    "            self.dissimilarity = el2_torus_dissimilarity\n",
    "\n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx, entities=True):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_scoring_function(self, proj_h, proj_t, r, start_time, end_time):\n",
    "        b_size = proj_h.shape[0]\n",
    "\n",
    "        if len(r.shape) == 2:\n",
    "            if len(proj_t.shape) == 3:\n",
    "                assert (len(proj_h.shape) == 2)\n",
    "                # this is the tail completion case in link prediction\n",
    "                hr = (proj_h + r).view(b_size, 1, r.shape[1])\n",
    "                return - self.dissimilarity(hr, proj_t)\n",
    "            else:\n",
    "                assert (len(proj_h.shape) == 3) & (len(proj_t.shape) == 2)\n",
    "                # this is the head completion case in link prediction\n",
    "                r_ = r.view(b_size, 1, r.shape[1])\n",
    "                t_ = proj_t.view(b_size, 1, r.shape[1])\n",
    "                return - self.dissimilarity(proj_h + r_, t_)\n",
    "        elif len(r.shape) == 3:\n",
    "            # this is the relation prediction case\n",
    "            # Two cases possible:\n",
    "            # * proj_ent.shape == (b_size, self.n_rel, self.emb_dim) -> projection depending on relations\n",
    "            # * proj_ent.shape == (b_size, self.emb_dim) -> no projection\n",
    "            proj_h = proj_h.view(b_size, -1, self.emb_dim)\n",
    "            proj_t = proj_t.view(b_size, -1, self.emb_dim)\n",
    "            return - self.dissimilarity(proj_h + r, proj_t)\n",
    "\n",
    "\n",
    "class TempBilinearModel(TempModel):\n",
    "\n",
    "    def __init__(self, emb_dim, n_entities, n_relations):\n",
    "        super().__init__(n_entities, n_relations)\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def normalize_parameters(self):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_scoring_function(self, h, t, r, start_time, end_time):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx, start_time, end_time, entities=True):\n",
    "        \"\"\"See torchkge.models.interfaces.Models.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f60c7",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bceea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_reg(h,t,r):\n",
    "    s_a, x_a, y_a, z_a = torch.chunk(h, 4, dim =-1)\n",
    "    s_c, x_c, y_c, z_c = torch.chunk(t, 4, dim =-1)\n",
    "    s_b, x_b, y_b, z_b = torch.chunk(t, 4, dim =-1)\n",
    "    regul = (torch.mean( torch.abs(s_a) ** 2)\n",
    "                 + torch.mean( torch.abs(x_a) ** 2)\n",
    "                 + torch.mean( torch.abs(y_a) ** 2)\n",
    "                 + torch.mean( torch.abs(z_a) ** 2)\n",
    "                 + torch.mean( torch.abs(s_c) ** 2)\n",
    "                 + torch.mean( torch.abs(x_c) ** 2)\n",
    "                 + torch.mean( torch.abs(y_c) ** 2)\n",
    "                 + torch.mean( torch.abs(z_c) ** 2)\n",
    "                 )\n",
    "    regul2 =  (torch.mean( torch.abs(s_b) ** 2 )\n",
    "             + torch.mean( torch.abs(x_b) ** 2 )\n",
    "             + torch.mean( torch.abs(y_b) ** 2 )\n",
    "             + torch.mean( torch.abs(z_b) ** 2 ))\n",
    "    return 0.2 * (regul + regul2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27c7fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_init(in_features, out_features, criterion='he'):\n",
    "    fan_in = in_features\n",
    "    fan_out = out_features\n",
    "\n",
    "    if criterion == 'glorot':\n",
    "        s = 1. / np.sqrt(2 * (fan_in + fan_out))\n",
    "    elif criterion == 'he':\n",
    "        s = 1. / np.sqrt(2 * fan_in)\n",
    "    else:\n",
    "        raise ValueError('Invalid criterion: ', criterion)\n",
    "    rng = RandomState(123)\n",
    "\n",
    "    # Generating randoms and purely imaginary quaternions :\n",
    "    kernel_shape = (in_features, out_features)\n",
    "\n",
    "    number_of_weights = np.prod(kernel_shape)\n",
    "    v_i = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    v_j = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "    v_k = np.random.uniform(0.0, 1.0, number_of_weights)\n",
    "\n",
    "    # Purely imaginary quaternions unitary\n",
    "    for i in range(0, number_of_weights):\n",
    "        norm = np.sqrt(v_i[i] ** 2 + v_j[i] ** 2 + v_k[i] ** 2) + 0.0001\n",
    "        v_i[i] /= norm\n",
    "        v_j[i] /= norm\n",
    "        v_k[i] /= norm\n",
    "    v_i = v_i.reshape(kernel_shape)\n",
    "    v_j = v_j.reshape(kernel_shape)\n",
    "    v_k = v_k.reshape(kernel_shape)\n",
    "\n",
    "    modulus = rng.uniform(low=-s, high=s, size=kernel_shape)\n",
    "    phase = rng.uniform(low=-np.pi, high=np.pi, size=kernel_shape)\n",
    "\n",
    "    weight_r = modulus * np.cos(phase)\n",
    "    weight_i = modulus * v_i * np.sin(phase)\n",
    "    weight_j = modulus * v_j * np.sin(phase)\n",
    "    weight_k = modulus * v_k * np.sin(phase)\n",
    "\n",
    "    return (weight_r, weight_i, weight_j, weight_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70655d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeQuatDEEnt_Mini(TempBilinearModel):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args.emb_dim, args.n_entities, args.n_relations)\n",
    "        self.emb_dim = args.emb_dim\n",
    "        self.tem_total = args.tem_total\n",
    "        self.time_proj_method = args.time_proj_method\n",
    "\n",
    "        self.quat_emb_dim = self.emb_dim * 4\n",
    "        self.ent_emb = nn.Embedding(self.n_ent, self.quat_emb_dim)\n",
    "        self.rel_emb = nn.Embedding(self.n_rel, self.quat_emb_dim)\n",
    "        self.ent_transfer = nn.Embedding(self.tem_total, self.emb_dim * 4)\n",
    "        self.rel_transfer = nn.Embedding(self.n_rel, self.emb_dim * 4)\n",
    "        # if self.time_proj_method == 'ent':\n",
    "        # elif self.time_proj_method == 'rel':\n",
    "        # elif self.tem_proj_mode == 'ent_rel':\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        r, i, j, k = quaternion_init(self.n_ent, self.emb_dim)\n",
    "        r, i, j, k = torch.from_numpy(r), torch.from_numpy(i), torch.from_numpy(j), torch.from_numpy(k)\n",
    "        vec1 = torch.cat([r, i, j, k], dim=1)\n",
    "        self.ent_emb.weight.data = vec1.type_as(self.ent_emb.weight.data)\n",
    "        self.ent_transfer.weight.data = vec1.type_as(self.ent_transfer.weight.data)\n",
    "\n",
    "        s, x, y, z = quaternion_init(self.n_rel, self.emb_dim)\n",
    "        s, x, y, z = torch.from_numpy(s), torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(z)\n",
    "        vec2 = torch.cat([s, x, y, z], dim=1)\n",
    "        self.rel_emb.data = vec2.type_as(self.rel_emb.weight.data)\n",
    "        self.rel_transfer.data = vec2.type_as(self.rel_transfer.weight.data)\n",
    "    \n",
    "    def _calc(self, h, r):\n",
    "        s_a, x_a, y_a, z_a = torch.chunk(h, 4, dim=1)\n",
    "        s_b, x_b, y_b, z_b = torch.chunk(r, 4, dim=1)\n",
    "\n",
    "        denominator_b = torch.sqrt(s_b ** 2 + x_b ** 2 + y_b ** 2 + z_b ** 2)\n",
    "        s_b = s_b / denominator_b\n",
    "        x_b = x_b / denominator_b\n",
    "        y_b = y_b / denominator_b\n",
    "        z_b = z_b / denominator_b\n",
    "\n",
    "        A = s_a * s_b - x_a * x_b - y_a * y_b - z_a * z_b\n",
    "        B = s_a * x_b + s_b * x_a + y_a * z_b - y_b * z_a\n",
    "        C = s_a * y_b + s_b * y_a + z_a * x_b - z_b * x_a\n",
    "        D = s_a * z_b + s_b * z_a + x_a * y_b - x_b * y_a\n",
    "\n",
    "        return torch.cat([A, B, C, D], dim=1)\n",
    "\n",
    "    def eval_calc(self, h, r):\n",
    "        s_a, x_a, y_a, z_a = torch.chunk(h, 4, dim=-1)\n",
    "        s_b, x_b, y_b, z_b = torch.chunk(r, 4, dim=-1)\n",
    "\n",
    "        denominator_b = torch.sqrt(s_b ** 2 + x_b ** 2 + y_b ** 2 + z_b ** 2)\n",
    "        s_b = s_b / denominator_b\n",
    "        x_b = x_b / denominator_b\n",
    "        y_b = y_b / denominator_b\n",
    "        z_b = z_b / denominator_b            \n",
    "\n",
    "        if (len(h.shape) == 3) & (len(r.shape) == 2):\n",
    "            # print('s_b.shape: ',s_b.shape)\n",
    "            s_b = s_b.view(s_b.shape[0], 1, s_b.shape[1])\n",
    "            x_b = x_b.view(x_b.shape[0], 1, x_b.shape[1])\n",
    "            y_b = y_b.view(y_b.shape[0], 1, y_b.shape[1])\n",
    "            z_b = z_b.view(z_b.shape[0], 1, z_b.shape[1])\n",
    "\n",
    "        elif (len(r.shape) == 3) & (len(h.shape) == 2):\n",
    "            s_a, x_a, y_a, z_a = s_a.view(s_a.shape[0], 1, s_a.shape[1]), x_a.view(x_a.shape[0], 1, x_a.shape[1]), y_a.view(x_a.shape[0], 1, x_a.shape[1]), z_a.view(x_a.shape[0], 1, x_a.shape[1])\n",
    "\n",
    "        A = s_a * s_b - x_a * x_b - y_a * y_b - z_a * z_b\n",
    "        B = s_a * x_b + s_b * x_a + y_a * z_b - y_b * z_a\n",
    "        C = s_a * y_b + s_b * y_a + z_a * x_b - z_b * x_a\n",
    "        D = s_a * z_b + s_b * z_a + x_a * y_b - x_b * y_a\n",
    "\n",
    "        return torch.cat([A, B, C, D], dim=-1)\n",
    "\n",
    "    def _transfer(self, x, x_transfer, r_transfer):\n",
    "        ent_transfer = self._calc(x, x_transfer)\n",
    "        ent_rel_transfer = self._calc(ent_transfer, r_transfer)\n",
    "\n",
    "        return ent_rel_transfer\n",
    "\n",
    "    def eval_transfer(self, x, x_transfer, r_transfer):\n",
    "        ent_transfer = self.eval_calc(x, x_transfer)\n",
    "        # print('ent_transfer.shape: ',ent_transfer.shape)\n",
    "        # print('r_transfer.shape: ',r_transfer.shape)\n",
    "        ent_rel_transfer = self.eval_calc(ent_transfer, r_transfer)\n",
    "\n",
    "        return ent_rel_transfer\n",
    "    \n",
    "    def get_reg_loss(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        h = self.ent_emb(h_idx)\n",
    "        r = self.rel_emb(r_idx)\n",
    "        t = self.ent_emb(t_idx)\n",
    "\n",
    "        # (h, r, t) transfer vector\n",
    "        h_transfer = self.ent_transfer(start_time_idx)\n",
    "        t_transfer = self.ent_transfer(start_time_idx)\n",
    "        r_transfer = self.rel_transfer(r_idx)\n",
    "\n",
    "        h1 = self._transfer(h, h_transfer, r_transfer)\n",
    "        t1 = self._transfer(t, t_transfer, r_transfer)\n",
    "        reg = scale_reg(h,t,r)\n",
    "        reg_trans = scale_reg(h1,t1,r)\n",
    "        return 0.5 * (reg + reg_trans)\n",
    "    \n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        h = self.ent_emb(h_idx)\n",
    "        r = self.rel_emb(r_idx)\n",
    "        t = self.ent_emb(t_idx)\n",
    "\n",
    "        # (h, r, t) transfer vector\n",
    "        h_transfer = self.ent_transfer(start_time_idx)\n",
    "        t_transfer = self.ent_transfer(start_time_idx)\n",
    "        r_transfer = self.rel_transfer(r_idx)\n",
    "\n",
    "        h1 = self._transfer(h, h_transfer, r_transfer)\n",
    "        t1 = self._transfer(t, t_transfer, r_transfer)\n",
    "\n",
    "        # multiplication as QuatE\n",
    "        hr = self._calc(h1, r)\n",
    "        # Inner product as QuatE\n",
    "        score = torch.sum(hr * t1, -1)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        self.normalize_parameters()\n",
    "        return self.ent_emb.weight.data, self.rel_emb.weight.data\n",
    "\n",
    "    def inference_scoring_function(self, h, t, r, start_time, end_time):\n",
    "        b_size = h.shape[0]\n",
    "        emb_dim_ = 4*self.emb_dim\n",
    "        if len(t.shape) == 3:\n",
    "            assert (len(h.shape) == 2) & (len(r.shape) == 2)\n",
    "            r, h_transfer, t_transfer, r_transfer = torch.chunk(r, 4, dim=-1)\n",
    "#             h, h_transfer = torch.chunk(h, 2, dim=-1)\n",
    "#             t, t_transfer = torch.chunk(t, 2, dim=-1)\n",
    "            \n",
    "            h1 = self.eval_transfer(h, h_transfer, r_transfer)\n",
    "            # print('t.shape: ',t.shape)\n",
    "            t1 = self.eval_transfer(t, t_transfer, r_transfer)\n",
    "            hr = self.eval_calc(h1, r)\n",
    "            hr = hr.view(hr.shape[0],1,hr.shape[1])\n",
    "            # print('t1.shape: ',t1.shape, ' hr.shape: ',hr.shape)\n",
    "            score = torch.sum(hr * t1, -1)\n",
    "            return score\n",
    "\n",
    "        elif len(h.shape) == 3:\n",
    "            assert (len(t.shape) == 2) & (len(r.shape) == 2)\n",
    "            # this is the head completion case in link prediction\n",
    "            r, h_transfer, t_transfer, r_transfer = torch.chunk(r, 4, dim=-1)\n",
    "#             h, h_transfer = torch.chunk(h, 2, dim=-1)\n",
    "#             t, t_transfer = torch.chunk(t, 2, dim=-1)\n",
    "            # this is the head completion case in link prediction\n",
    "            h1 = self.eval_transfer(h, h_transfer, r_transfer)\n",
    "            t1 = self.eval_transfer(t, t_transfer, r_transfer)\n",
    "            hr = self.eval_calc(h1, r)\n",
    "            # print('t1.shape: ',t1.shape, ' hr.shape: ',hr.shape)\n",
    "            t1 = t1.view(t1.shape[0],1,t1.shape[1])\n",
    "            score = torch.sum(hr * t1, -1)\n",
    "            return score\n",
    "        elif len(r.shape) == 3:\n",
    "            assert (len(h.shape) == 2) & (len(t.shape) == 2)\n",
    "            # this is the relation prediction case\n",
    "            r, h_transfer, t_transfer, r_transfer = torch.chunk(r, 4, dim=-1)\n",
    "#             h, h_transfer = torch.chunk(h, 2, dim=-1)\n",
    "#             t, t_transfer = torch.chunk(t, 2, dim=-1)\n",
    "            h1 = self.eval_transfer(h, h_transfer, r_transfer)\n",
    "            t1 = self.eval_transfer(t, t_transfer, r_transfer)\n",
    "            hr = self.eval_calc(h1, r)\n",
    "            t1 = t1.view(t1.shape[0],1,hr.shape[1])# needs to be addapted\n",
    "            score = torch.sum(hr * t1, -1)# needs to be addapted\n",
    "\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx,start_time_idx, end_time_idx, entities=True):\n",
    "        b_size = h_idx.shape[0]\n",
    "\n",
    "        h = self.ent_emb(h_idx)\n",
    "        t = self.ent_emb(t_idx)\n",
    "        r = self.rel_emb(r_idx)\n",
    "        \n",
    "        h_transfer = self.ent_transfer(start_time_idx)\n",
    "        t_transfer = self.ent_transfer(start_time_idx)\n",
    "        r_transfer = self.rel_transfer(r_idx)\n",
    "\n",
    "        if entities:\n",
    "            all_ents = torch.tensor(range(self.n_ent)).to(device)\n",
    "            candidates = self.ent_emb(all_ents)\n",
    "            candidates = candidates.expand(b_size, self.n_ent, candidates.shape[-1])\n",
    "        else:\n",
    "            all_rels = torch.tensor(range(self.n_rel)).to(device)\n",
    "            candidates = self.rel_emb(all_rels)\n",
    "            candidates = candidates.expand(b_size, self.n_rel, candidates.shape[-1])\n",
    "        return h, t, torch.cat([r, h_transfer, t_transfer, r_transfer], dim = -1), candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94a93d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeQuatDEEnt(TempBilinearModel):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args.emb_dim, args.n_entities, args.n_relations)\n",
    "        self.emb_dim = args.emb_dim\n",
    "        self.tem_total = args.tem_total\n",
    "        self.time_proj_method = args.time_proj_method\n",
    "\n",
    "        self.quat_emb_dim = self.emb_dim * 4\n",
    "        self.ent_emb = nn.Embedding(self.n_ent, self.quat_emb_dim)\n",
    "        self.rel_emb = nn.Embedding(self.n_rel, self.quat_emb_dim)\n",
    "        self.time_transfer = nn.Embedding(self.tem_total, self.emb_dim * 4)\n",
    "        self.ent_transfer = nn.Embedding(self.n_ent, self.emb_dim * 4)\n",
    "        self.rel_transfer = nn.Embedding(self.n_rel, self.emb_dim * 4)\n",
    "        # if self.time_proj_method == 'ent':\n",
    "        # elif self.time_proj_method == 'rel':\n",
    "        # elif self.tem_proj_mode == 'ent_rel':\n",
    "        self.init_weights()\n",
    "# time_transfer, ent_transfer, rel_transfer, ent_emb, rel_emb\n",
    "    def init_weights(self):\n",
    "        # time_transfer, ent_transfer, rel_transfer, ent_emb, rel_emb\n",
    "\n",
    "        r, i, j, k = quaternion_init(self.n_ent, self.emb_dim)\n",
    "        r, i, j, k = torch.from_numpy(r), torch.from_numpy(i), torch.from_numpy(j), torch.from_numpy(k)\n",
    "        vec1 = torch.cat([r, i, j, k], dim=1)\n",
    "        self.ent_emb.weight.data = vec1.type_as(self.ent_emb.weight.data)\n",
    "        self.ent_transfer.weight.data = vec1.type_as(self.ent_transfer.weight.data)\n",
    "\n",
    "        s, x, y, z = quaternion_init(self.n_rel, self.emb_dim)\n",
    "        s, x, y, z = torch.from_numpy(s), torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(z)\n",
    "        vec2 = torch.cat([s, x, y, z], dim=1)\n",
    "        self.rel_emb.data = vec2.type_as(self.rel_emb.weight.data)\n",
    "        self.rel_transfer.data = vec2.type_as(self.rel_transfer.weight.data)\n",
    "        \n",
    "        r, i, j, k = quaternion_init(self.n_ent, self.emb_dim)\n",
    "        r, i, j, k = torch.from_numpy(r), torch.from_numpy(i), torch.from_numpy(j), torch.from_numpy(k)\n",
    "        vec1 = torch.cat([r, i, j, k], dim=1)\n",
    "        self.time_transfer.weight.data = vec1.type_as(self.time_transfer.weight.data)\n",
    "\n",
    "    def _calc(self, h, r):\n",
    "        s_a, x_a, y_a, z_a = torch.chunk(h, 4, dim=1)\n",
    "        s_b, x_b, y_b, z_b = torch.chunk(r, 4, dim=1)\n",
    "\n",
    "        denominator_b = torch.sqrt(s_b ** 2 + x_b ** 2 + y_b ** 2 + z_b ** 2)\n",
    "        s_b = s_b / denominator_b\n",
    "        x_b = x_b / denominator_b\n",
    "        y_b = y_b / denominator_b\n",
    "        z_b = z_b / denominator_b\n",
    "\n",
    "        A = s_a * s_b - x_a * x_b - y_a * y_b - z_a * z_b\n",
    "        B = s_a * x_b + s_b * x_a + y_a * z_b - y_b * z_a\n",
    "        C = s_a * y_b + s_b * y_a + z_a * x_b - z_b * x_a\n",
    "        D = s_a * z_b + s_b * z_a + x_a * y_b - x_b * y_a\n",
    "\n",
    "        return torch.cat([A, B, C, D], dim=-1)\n",
    "\n",
    "    def eval_calc(self, h, r):\n",
    "        s_a, x_a, y_a, z_a = torch.chunk(h, 4, dim=-1)\n",
    "        s_b, x_b, y_b, z_b = torch.chunk(r, 4, dim=-1)\n",
    "\n",
    "        denominator_b = torch.sqrt(s_b ** 2 + x_b ** 2 + y_b ** 2 + z_b ** 2)\n",
    "        s_b = s_b / denominator_b\n",
    "        x_b = x_b / denominator_b\n",
    "        y_b = y_b / denominator_b\n",
    "        z_b = z_b / denominator_b            \n",
    "\n",
    "        if (len(h.shape) == 3) & (len(r.shape) == 2):\n",
    "            # print('s_b.shape: ',s_b.shape)\n",
    "            s_b = s_b.view(s_b.shape[0], 1, s_b.shape[1])\n",
    "            x_b = x_b.view(x_b.shape[0], 1, x_b.shape[1])\n",
    "            y_b = y_b.view(y_b.shape[0], 1, y_b.shape[1])\n",
    "            z_b = z_b.view(z_b.shape[0], 1, z_b.shape[1])\n",
    "\n",
    "        elif (len(r.shape) == 3) & (len(h.shape) == 2):\n",
    "            s_a, x_a, y_a, z_a = s_a.view(s_a.shape[0], 1, s_a.shape[1]), x_a.view(x_a.shape[0], 1, x_a.shape[1]), y_a.view(x_a.shape[0], 1, x_a.shape[1]), z_a.view(x_a.shape[0], 1, x_a.shape[1])\n",
    "\n",
    "        A = s_a * s_b - x_a * x_b - y_a * y_b - z_a * z_b\n",
    "        B = s_a * x_b + s_b * x_a + y_a * z_b - y_b * z_a\n",
    "        C = s_a * y_b + s_b * y_a + z_a * x_b - z_b * x_a\n",
    "        D = s_a * z_b + s_b * z_a + x_a * y_b - x_b * y_a\n",
    "\n",
    "        return torch.cat([A, B, C, D], dim=-1)\n",
    "\n",
    "    def _transfer(self, x, x_transfer, r_transfer):\n",
    "        ent_transfer = self._calc(x, x_transfer)\n",
    "        ent_rel_transfer = self._calc(ent_transfer, r_transfer)\n",
    "\n",
    "        return ent_rel_transfer\n",
    "\n",
    "    def eval_transfer(self, x, x_transfer, r_transfer):\n",
    "        ent_transfer = self.eval_calc(x, x_transfer)\n",
    "        # print('ent_transfer.shape: ',ent_transfer.shape)\n",
    "        # print('r_transfer.shape: ',r_transfer.shape)\n",
    "        ent_rel_transfer = self.eval_calc(ent_transfer, r_transfer)\n",
    "\n",
    "        return ent_rel_transfer\n",
    "    \n",
    "    def get_reg_loss(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        h = self.ent_emb(h_idx)\n",
    "        r = self.rel_emb(r_idx)\n",
    "        t = self.ent_emb(t_idx)\n",
    "\n",
    "        # (h, r, t) transfer vector\n",
    "        h_transfer = self.time_transfer(start_time_idx)\n",
    "        t_transfer = self.time_transfer(start_time_idx)\n",
    "        r_transfer = self.rel_transfer(r_idx)\n",
    "\n",
    "        h1 = self._transfer(h, h_transfer, r_transfer)\n",
    "        t1 = self._transfer(t, t_transfer, r_transfer)\n",
    "        reg = scale_reg(h,t,r)\n",
    "        reg_trans = scale_reg(h1,t1,r)\n",
    "        return 0.5 * (reg + reg_trans)\n",
    "    \n",
    "    def scoring_function(self, h_idx, t_idx, r_idx, start_time_idx, end_time_idx):\n",
    "        h = self.ent_emb(h_idx)\n",
    "        r = self.rel_emb(r_idx)\n",
    "        t = self.ent_emb(t_idx)\n",
    "\n",
    "        # (h, r, t) transfer vector\n",
    "        h_transfer = self.time_transfer(start_time_idx)\n",
    "        t_transfer = self.time_transfer(start_time_idx)\n",
    "        r_transfer = self.rel_transfer(r_idx)\n",
    "\n",
    "        h1 = self._transfer(h, h_transfer, r_transfer)\n",
    "        t1 = self._transfer(t, t_transfer, r_transfer)\n",
    "\n",
    "        # multiplication as QuatE\n",
    "        hr = self._calc(h1, r)\n",
    "        # Inner product as QuatE\n",
    "        score = torch.sum(hr * t1, -1)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        self.normalize_parameters()\n",
    "        return self.ent_emb.weight.data, self.rel_emb.weight.data\n",
    "\n",
    "    def inference_scoring_function(self, h, t, r, start_time, end_time):\n",
    "        b_size = h.shape[0]\n",
    "        emb_dim_ = 4*self.emb_dim\n",
    "        if len(t.shape) == 3:\n",
    "            assert (len(h.shape) == 2) & (len(r.shape) == 2)\n",
    "            r, h_transfer, t_transfer, r_transfer, time_transfer = torch.chunk(r, 5, dim=-1)\n",
    "#             h, h_transfer = torch.chunk(h, 2, dim=-1)\n",
    "            t, t_transfer = torch.chunk(t, 2, dim=-1)\n",
    "            h_transfer = h_transfer + time_transfer\n",
    "            t_transfer = t_transfer + time_transfer.view(time_transfer.shape[0],1,time_transfer.shape[1])\n",
    "            h1 = self.eval_transfer(h, h_transfer, r_transfer)\n",
    "            # print('t.shape: ',t.shape)\n",
    "            t1 = self.eval_transfer(t, t_transfer, r_transfer)\n",
    "            hr = self.eval_calc(h1, r)\n",
    "            hr = hr.view(hr.shape[0],1,hr.shape[1])\n",
    "            # print('t1.shape: ',t1.shape, ' hr.shape: ',hr.shape)\n",
    "            score = torch.sum(hr * t1, -1)\n",
    "            return score\n",
    "\n",
    "        elif len(h.shape) == 3:\n",
    "            assert (len(t.shape) == 2) & (len(r.shape) == 2)\n",
    "            # this is the head completion case in link prediction\n",
    "            r, h_transfer, t_transfer, r_transfer, time_transfer = torch.chunk(r, 5, dim=-1)\n",
    "            h, h_transfer = torch.chunk(h, 2, dim=-1)\n",
    "            t_transfer = t_transfer + time_transfer\n",
    "            h_transfer = h_transfer + time_transfer.view(time_transfer.shape[0],1,time_transfer.shape[1])\n",
    "#             t, t_transfer = torch.chunk(t, 2, dim=-1)\n",
    "            # this is the head completion case in link prediction\n",
    "            h1 = self.eval_transfer(h, h_transfer, r_transfer)\n",
    "            t1 = self.eval_transfer(t, t_transfer, r_transfer)\n",
    "            hr = self.eval_calc(h1, r)\n",
    "            # print('t1.shape: ',t1.shape, ' hr.shape: ',hr.shape)\n",
    "            t1 = t1.view(t1.shape[0],1,t1.shape[1])\n",
    "            score = torch.sum(hr * t1, -1)\n",
    "            return score\n",
    "        elif len(r.shape) == 3:\n",
    "            assert (len(h.shape) == 2) & (len(t.shape) == 2)\n",
    "            # this is the relation prediction case\n",
    "            r, h_transfer, t_transfer, r_transfer, time_transfer = torch.chunk(r, 5, dim=-1)\n",
    "#             h, h_transfer = torch.chunk(h, 2, dim=-1)\n",
    "#             t, t_transfer = torch.chunk(t, 2, dim=-1)\n",
    "            h1 = self.eval_transfer(h, h_transfer, r_transfer)\n",
    "            t1 = self.eval_transfer(t, t_transfer, r_transfer)\n",
    "            hr = self.eval_calc(h1, r)\n",
    "            t1 = t1.view(t1.shape[0],1,hr.shape[1])# needs to be addapted\n",
    "            score = torch.sum(hr * t1, -1)# needs to be addapted\n",
    "\n",
    "    def inference_prepare_candidates(self, h_idx, t_idx, r_idx,start_time_idx, end_time_idx, entities=True):\n",
    "        b_size = h_idx.shape[0]\n",
    "\n",
    "        h = self.ent_emb(h_idx)\n",
    "        t = self.ent_emb(t_idx)\n",
    "        r = self.rel_emb(r_idx)\n",
    "        \n",
    "        time_transfer = self.time_transfer(start_time_idx)\n",
    "        t_transfer = self.ent_transfer(t_idx)\n",
    "        h_transfer = self.ent_transfer(h_idx)\n",
    "        r_transfer = self.rel_transfer(r_idx)\n",
    "\n",
    "        if entities:\n",
    "            all_ents = torch.tensor(range(self.n_ent)).to(device)\n",
    "            candidates = self.ent_emb(all_ents)\n",
    "            candidates = candidates.expand(b_size, self.n_ent, candidates.shape[-1])\n",
    "            all_ents = torch.tensor(range(self.n_ent)).to(device)\n",
    "            candidates_trans = self.ent_transfer(all_ents)\n",
    "            candidates_trans = candidates_trans.expand(b_size, self.n_ent, candidates_trans.shape[-1])\n",
    "        else:\n",
    "            all_rels = torch.tensor(range(self.n_rel)).to(device)\n",
    "            candidates = self.rel_emb(all_rels)\n",
    "            candidates = candidates.expand(b_size, self.n_rel, candidates.shape[-1])\n",
    "        return h, t, torch.cat([r, h_transfer, t_transfer, r_transfer, time_transfer], dim = -1),\\\n",
    "    torch.cat([candidates, candidates_trans], dim = -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d97a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, args):\n",
    "    if model_name == 'TimeQuatDEEnt':\n",
    "        return TimeQuatDEEnt(args)\n",
    "    if model_name == 'TimeQuatDEEnt_Mini':\n",
    "        return TimeQuatDEEnt_Mini(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6687e",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0937dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class temporal_train_loop():\n",
    "    def __init__(self, kg_train, kg_val, kg_test, args):\n",
    "        self.args = args\n",
    "\n",
    "        self.kg_train = kg_train\n",
    "        self.kg_val = kg_val\n",
    "        self.kg_test = kg_test\n",
    "        self.n_epochs = args.n_epochs\n",
    "        self.model_name = args.model_name\n",
    "        self.emb_dim = args.emb_dim\n",
    "        self.lr = args.lr\n",
    "        self.n_epochs = args.n_epochs\n",
    "        self.b_size = args.b_size\n",
    "        self.margin = args.margin\n",
    "        self.model_save_name = args.model_save_name\n",
    "        self.device = device\n",
    "        self.model_path = args.model_path\n",
    "        self.n_neg = args.n_neg\n",
    "\n",
    "#         self.model = TransEModel(self.emb_dim, self.kg_train.n_ent, self.kg_train.n_rel, dissimilarity_type='L2')\n",
    "        self.model = get_model(args.model_name, args)\n",
    "     \n",
    "        self.criterion = get_loss(args.loss_name, args)\n",
    "\n",
    "        # Move everything to CUDA if available\n",
    "        if cuda.is_available():\n",
    "            cuda.empty_cache()\n",
    "            self.model = self.model.to(self.device)\n",
    "            if hasattr(self.criterion, 'to'):\n",
    "                self.criterion = self.criterion.to(self.device)\n",
    "\n",
    "        # Define the torch optimizer to be used\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
    "        # self.sampler = BernoulliNegativeSampler(self.kg_train, n_neg = self.n_neg)\n",
    "        self.sampler = UniformNegativeSampler(self.kg_train, n_neg = self.n_neg)\n",
    "        self.dataloader = TempDataLoader(self.kg_train, batch_size=self.b_size, use_cuda=None)\n",
    "\n",
    "    def save_model(self,):\n",
    "        torch.save(self.model.state_dict(), join(self.model_path,self.model_save_name))\n",
    "\n",
    "    def evaluation(self, ):\n",
    "        with torch.no_grad():\n",
    "            evaluator = TemporalLinkPredictionEvaluator(self.model, self.kg_val)\n",
    "            evaluator.evaluate(b_size=100, verbose=False)\n",
    "            val_res = evaluator.get_results()\n",
    "            val_filt_mrr = val_res['filt_mrr']\n",
    "            # print(val_filt_mrr)\n",
    "            return val_filt_mrr, val_res\n",
    "    \n",
    "    def test(self,):\n",
    "        with torch.no_grad():\n",
    "            self.model = self.model.to('cpu')\n",
    "            del self.model\n",
    "            torch.cuda.empty_cache()\n",
    "            self.model = get_model(model_name = self.model_name, args = self.args)\n",
    "            self.model.load_state_dict(torch.load(join(self.model_path,self.model_save_name)))\n",
    "            os.remove(join(self.model_path,self.model_save_name))\n",
    "            self.model = self.model.to(device=device)\n",
    "            evaluator = TemporalLinkPredictionEvaluator(self.model, self.kg_test)\n",
    "            evaluator.evaluate(b_size=32)\n",
    "            test_res = evaluator.get_results()\n",
    "            return test_res\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        iterator = tqdm(range(self.n_epochs), unit='epoch')\n",
    "        eval_epoch = 1\n",
    "        best_val_mrr = -100\n",
    "        best_epoch = 0\n",
    "        patience_max = 9\n",
    "        patience_counter = 0\n",
    "        train_evolution = []\n",
    "\n",
    "        for epoch in iterator:\n",
    "            running_loss = 0.0\n",
    "            epoch_log = {}\n",
    "            for i, batch in enumerate(self.dataloader):\n",
    "                h, t, r, start_time, end_time = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n",
    "                # print('h.shape ooooo ',h.shape)\n",
    "                n_h, n_t = self.sampler.corrupt_batch(h, t, r)                \n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                pos, neg = self.model(h, t, r, start_time, end_time, n_h, n_t)\n",
    "                loss = self.criterion(pos, neg)\n",
    "#                 loss = self.criterion(pos, neg) + self.model.get_reg_loss(h, t, r, start_time, end_time)\n",
    "                \n",
    "                if hasattr(self.args, 'add_sin_constaint'):\n",
    "                    if self.args.add_sin_constaint:\n",
    "                        loss =loss + 20*self.model.get_constraints(h, start_time, end_time)\n",
    "                        loss =loss + 20*self.model.get_constraints(t, start_time, end_time)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "            epoch_log['avg_train_loss'] = running_loss / len(self.dataloader)\n",
    "            if epoch % eval_epoch == 0:\n",
    "                val_mrr,  val_res = self.evaluation()\n",
    "                epoch_log['val_res'] = val_res\n",
    "                if (best_val_mrr < val_mrr) & (val_mrr<float('inf')):\n",
    "                    best_val_mrr = val_mrr\n",
    "                    best_epoch = epoch + 1\n",
    "                    # save the model\n",
    "                    self.save_model()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter == patience_max:\n",
    "                        break\n",
    "            train_evolution.append(epoch_log)\n",
    "            iterator.set_description(\n",
    "                'Epoch {} | mean loss: {:.5f}| best mrr: {:.5f} | Best Epoch {} '.format(epoch + 1,\n",
    "                                                      running_loss / len(self.dataloader), best_val_mrr, best_epoch))\n",
    "\n",
    "        # self.model.normalize_parameters()\n",
    "        return train_evolution\n",
    "\n",
    "class bt_temporal_train_loop():\n",
    "    def __init__(self, kg_train, kg_val, kg_test, args):\n",
    "        self.args = args\n",
    "\n",
    "        self.kg_train = kg_train\n",
    "        self.kg_val = kg_val\n",
    "        self.kg_test = kg_test\n",
    "        self.n_epochs = args.n_epochs\n",
    "        self.model_name = args.model_name\n",
    "        self.emb_dim = args.emb_dim\n",
    "        self.lr = args.lr\n",
    "        self.n_epochs = args.n_epochs\n",
    "        self.b_size = args.b_size\n",
    "        self.margin = args.margin\n",
    "        self.model_save_name = args.model_save_name\n",
    "        self.device = args.device\n",
    "        self.model_path = args.model_path\n",
    "#         self.model = TransEModel(self.emb_dim, self.kg_train.n_ent, self.kg_train.n_rel, dissimilarity_type='L2')\n",
    "        self.model = bt_get_model(args.model_name, args)\n",
    "     \n",
    "        self.criterion = get_loss(args.loss_name, args)\n",
    "\n",
    "        # Move everything to CUDA if available\n",
    "        if cuda.is_available():\n",
    "            cuda.empty_cache()\n",
    "            self.model = self.model.to(self.device)\n",
    "            if hasattr(self.criterion, 'to'):\n",
    "                self.criterion = self.criterion.to(self.device)\n",
    "\n",
    "        # Define the torch optimizer to be used\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
    "        self.dataloader = TempDataLoader(self.kg_train, batch_size=self.b_size, use_cuda=None)\n",
    "\n",
    "    def save_model(self,):\n",
    "        torch.save(self.model.state_dict(), join(self.model_path,self.model_save_name))\n",
    "\n",
    "    def evaluation(self, ):\n",
    "        with torch.no_grad():\n",
    "            evaluator = TemporalLinkPredictionEvaluator(self.model, self.kg_val)\n",
    "            evaluator.evaluate(b_size=20, verbose=False)\n",
    "            val_res = evaluator.get_results()\n",
    "            # print(val_res)\n",
    "            val_filt_mrr = val_res['filt_mrr']\n",
    "            return val_filt_mrr, val_res\n",
    "    \n",
    "    def test(self,):\n",
    "        with torch.no_grad():\n",
    "            self.model = self.model.to('cpu')\n",
    "            del self.model\n",
    "            torch.cuda.empty_cache()\n",
    "            self.model = bt_get_model(model_name = self.model_name, args = self.args)\n",
    "            self.model.load_state_dict(torch.load(join(self.model_path,self.model_save_name)))\n",
    "            os.remove(join(self.model_path,self.model_save_name))\n",
    "            self.model = self.model.to(device=device)\n",
    "            evaluator = TemporalLinkPredictionEvaluator(self.model, self.kg_test)\n",
    "            evaluator.evaluate(b_size=32)\n",
    "            test_res = evaluator.get_results()\n",
    "            return test_res\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        iterator = tqdm(range(self.n_epochs), unit='epoch')\n",
    "        eval_epoch = 1\n",
    "        best_val_mrr = -100\n",
    "        best_epoch = 0\n",
    "        patience_max = 5\n",
    "        patience_counter = 0\n",
    "        train_evolution = []\n",
    "        for epoch in iterator:\n",
    "            running_loss = 0.0\n",
    "            epoch_log = {}\n",
    "            for i, batch in enumerate(self.dataloader):\n",
    "                h, t, r, start_time, end_time = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                loss = self.model(h, t, r, start_time, end_time)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "            epoch_log['avg_train_loss'] = running_loss / len(self.dataloader)\n",
    "            if epoch % eval_epoch == 0:\n",
    "                val_mrr, val_res = self.evaluation()\n",
    "                epoch_log['val_res'] = val_res\n",
    "                if best_val_mrr < val_mrr:\n",
    "                    best_val_mrr = val_mrr\n",
    "                    best_epoch = epoch + 1\n",
    "                    # save the model\n",
    "                    self.save_model()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter == patience_max:\n",
    "                        break\n",
    "            train_evolution.append(epoch_log)\n",
    "\n",
    "            iterator.set_description(\n",
    "                'Epoch {} | mean loss: {:.5f}| best mrr: {:.5f} | Best Epoch {} '.format(epoch + 1,\n",
    "                                                      running_loss / len(self.dataloader), best_val_mrr, best_epoch))\n",
    "\n",
    "        # self.model.normalize_parameters()\n",
    "        return train_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c200f68",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8600e443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b38c5762eb45f5a12f744f3520f644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m args\u001b[38;5;241m.\u001b[39mdissimilarity_type \u001b[38;5;241m=\u001b[39m dissimilarity_type\n\u001b[1;32m     72\u001b[0m tmp_trainlp \u001b[38;5;241m=\u001b[39m temporal_train_loop(kg_train, kg_val, kg_test, args)\n\u001b[0;32m---> 73\u001b[0m train_evolution \u001b[38;5;241m=\u001b[39m \u001b[43mtmp_trainlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m test_res \u001b[38;5;241m=\u001b[39m tmp_trainlp\u001b[38;5;241m.\u001b[39mtest()\n\u001b[1;32m     75\u001b[0m train_log \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[24], line 98\u001b[0m, in \u001b[0;36mtemporal_train_loop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m epoch_log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_train_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m eval_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 98\u001b[0m     val_mrr,  val_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     epoch_log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_res\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_res\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (best_val_mrr \u001b[38;5;241m<\u001b[39m val_mrr) \u001b[38;5;241m&\u001b[39m (val_mrr\u001b[38;5;241m<\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n",
      "Cell \u001b[0;32mIn[24], line 44\u001b[0m, in \u001b[0;36mtemporal_train_loop.evaluation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     43\u001b[0m     evaluator \u001b[38;5;241m=\u001b[39m TemporalLinkPredictionEvaluator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkg_val)\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     val_res \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mget_results()\n\u001b[1;32m     46\u001b[0m     val_filt_mrr \u001b[38;5;241m=\u001b[39m val_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilt_mrr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[15], line 39\u001b[0m, in \u001b[0;36mTemporalLinkPredictionEvaluator.evaluate\u001b[0;34m(self, b_size, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilt_rank_true_tails[i \u001b[38;5;241m*\u001b[39m b_size: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m b_size] \u001b[38;5;241m=\u001b[39m get_rank(filt_scores, t_idx)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     38\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minference_scoring_function(candidates, t_emb, r_emb, start_time, end_time)\n\u001b[0;32m---> 39\u001b[0m filt_scores \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_dict_of_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank_true_heads[i \u001b[38;5;241m*\u001b[39m b_size: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m b_size] \u001b[38;5;241m=\u001b[39m get_rank(scores, h_idx)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilt_rank_true_heads[i \u001b[38;5;241m*\u001b[39m b_size: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m b_size] \u001b[38;5;241m=\u001b[39m get_rank(filt_scores, h_idx)\u001b[38;5;241m.\u001b[39mdetach()\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mfilter_scores\u001b[0;34m(scores, dictionary, key1, key2, key3, key4, true_idx)\u001b[0m\n\u001b[1;32m      5\u001b[0m filt_scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(b_size):\n\u001b[0;32m----> 8\u001b[0m     true_targets \u001b[38;5;241m=\u001b[39m \u001b[43mget_true_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m true_targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m, in \u001b[0;36mget_true_targets\u001b[0;34m(dictionary, key1, key2, key3, key4, true_idx, i)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key4 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m         true_targets \u001b[38;5;241m=\u001b[39m dictionary[\u001b[43mkey1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, key2[i]\u001b[38;5;241m.\u001b[39mitem(), key3[i]\u001b[38;5;241m.\u001b[39mitem(), key4[i]\u001b[38;5;241m.\u001b[39mitem()]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         true_targets \u001b[38;5;241m=\u001b[39m dictionary[key1[i]\u001b[38;5;241m.\u001b[39mitem(), key2[i]\u001b[38;5;241m.\u001b[39mitem(), key3[i]\u001b[38;5;241m.\u001b[39mitem()]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res_save_file = 'TDYCE_experiments_log.jsonl'\n",
    "if os.path.exists(res_save_file):\n",
    "    all_exp_df = check_experiment(res_save_file)\n",
    "else:\n",
    "    all_exp_df = None\n",
    "args = Params()\n",
    "\n",
    "args.n_epochs = 1000\n",
    "# args.margin = 200\n",
    "args.model_save_name = 'best_de_model_.bt'\n",
    "args.loss_name = 'MarginLoss'\n",
    "# args.loss_name = 'BinaryCrossEntropyLoss'\n",
    "args.model_path = 'best_models'\n",
    "args.time_mode = 'simple'\n",
    "# args.n_neg = 10\n",
    "# args.flag_hamilton_mul_norm = False\n",
    "# args.input_dropout = .3\n",
    "# args.hidden_dropout = .3\n",
    "# 0.36\n",
    "# args.time_proj_method = 'tero'\n",
    "def get_model_name(dataset,batch_size , lr , emb_dim , model_name , dissimilarity_type):\n",
    "    vals = [dataset,batch_size , lr , emb_dim , model_name , dissimilarity_type]\n",
    "    keys = ['dataset','batch_size' , 'lr' , 'emb_dim' , 'model_name' , 'dissimilarity_type']\n",
    "    kv = []\n",
    "    for k,  v in zip(keys,vals):\n",
    "        kv.append(':'.join([str(k),  str(v)]))\n",
    "    kv = '$'.join(kv)\n",
    "    kv = kv+'.pt'\n",
    "    return kv\n",
    "\n",
    "args.time_proj_method = 'ent'#ent_rel \n",
    "for model_name in ['TimeQuatDEEnt_Mini']:\n",
    "    args.model_name = model_name\n",
    "    for dataset in ['icews14']:# icews14 gdelt\n",
    "        args.dataset = dataset\n",
    "        kg_train, kg_val, kg_test = get_data(args.dataset, args.time_mode)\n",
    "        args.n_entities = kg_train.n_ent\n",
    "        args.n_relations = kg_train.n_rel\n",
    "        args.tem_total = kg_train.n_time\n",
    "        for n_neg in [5,2,10,1]:\n",
    "            args.n_neg = n_neg\n",
    "            for margin in [200,100,10]:\n",
    "                args.margin = margin\n",
    "                for b_size in [1000, 500]:\n",
    "                    # 500, 1000, 2000\n",
    "                    args.b_size = b_size\n",
    "                    for lr in [0.001, 0.01]:\n",
    "                        args.lr = lr\n",
    "                        for emb_dim in [100]:\n",
    "                            args.emb_dim = emb_dim\n",
    "                            args.num_features = args.emb_dim\n",
    "\n",
    "                            if args.model_name in ['TeRoTransEModel']:\n",
    "                                dissimilarity_type_list = ['L1']\n",
    "                                # 'L2', 'L1'\n",
    "                            elif args.model_name in ['toruse']:\n",
    "                                dissimilarity_type_list = ['L1', 'torus_L1', 'torus_L2', 'torus_eL2']\n",
    "                            else:\n",
    "                                dissimilarity_type_list = ['']\n",
    "\n",
    "                            for dissimilarity_type in dissimilarity_type_list:\n",
    "#                                 if all_exp_df is not None:\n",
    "#                                     exp_already_done = experiment_exists(all_exp_data_df = all_exp_df, exp_data_dict = args.__dict__)\n",
    "#                                 else:\n",
    "#                                     exp_already_done = False\n",
    "#                                 model_save_name = get_model_name(dataset, args.b_size , lr , emb_dim , args.model_name , dissimilarity_type)\n",
    "#                                 print(exp_already_done)\n",
    "#                                 print(model_save_name)\n",
    "                                exp_already_done = False\n",
    "                                if not exp_already_done:\n",
    "                                    args.dissimilarity_type = dissimilarity_type\n",
    "                                    tmp_trainlp = temporal_train_loop(kg_train, kg_val, kg_test, args)\n",
    "                                    train_evolution = tmp_trainlp.run()\n",
    "                                    test_res = tmp_trainlp.test()\n",
    "                                    train_log = {}\n",
    "                                    print(test_res)\n",
    "                                    train_log['train_params'] = args.__dict__\n",
    "                                    train_log['train_evolution'] = train_evolution\n",
    "                                    train_log['test_results'] = test_res\n",
    "#                                     write_json_lines(file_name =res_save_file, dict_data = train_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf12c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.39 filt mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfbdf08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c455938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a9c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c14faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3010b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
